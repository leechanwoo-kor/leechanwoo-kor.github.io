---
title: "[LLM] sLLM(small Large Language Model)"
categories:
  - LLM
tags:
  - LLM
  - sLLM
toc: true
toc_sticky: true
toc_label: "sLLM(small Large Language Model)"
toc_icon: "sticky-note"
---

<br>![image](https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/d1e32689-d62b-4650-b751-5ae353c001cb){: .align-center}<br>

# sLLM (small Large Language Model)

## 서론

ChatGPT 등장 이후 생성형 AI의 근간인 LLM(Large Language Model, 거대언어모델)의 비즈니스 활용이 주목을 받는 와중에 작년 메타의 라마(LLaMA)가 공개되면서 LLM의 한계를 보완한 sLLM(small Large Language Model, 소형언어모델)가 새로운 대안으로 떠오르고 있다.

### LLM(Large Language Model)

LLM은 인간의 언어를 이해하고 생성할 수 있도록 훈련된 언어 모델이다. 생성형 AI의 대표주자인 챗GPT가 이 LLM을 기반으로 만들어졌다. 언어 모델의 크기는 통상 매개변수(시스템상 작동에 영향을 미치는 데이터) 개수에 따라 정해진다. 1000억 개 이상이면 LLM으로 분류된다. 챗GPT에 적용된 'GPT-3'의 매개변수는 1750억 개이며, 구글이 개발한 '팜(PaLM)'은 5400억 개에 달한다.

### sLLM(small Large Language Model)

sLLM은 최소 수십억 개의 매개변수만으로 연산 작업을 단축시켜 보다 효율적으로 운영될 수 있는 언어 모델에 속한다. sLLM이 처음 주목받기 시작한 건 23년 초부터이다. 당시 메타는 LLM인 '라마(LLaMA)'를 공개한 데 이어 미국 스탠퍼드대와 함께 라마를 기반으로 한 sLLM '알파카(Alpaca)'를 개발했다. 매개변수가 적은 만큼 LLM에 비해 더 적은 컴퓨팅(처리 과정) 자원으로 최대한의 효율을 낼 수 있다. 그만큼 **훈련 시간, 비용, 용량, 전력 소모량**이 훨씬 절감된다.

<p align="center">
  <img src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/57ae4139-9a83-404e-a18b-6cbd0d192ba5"><br>
  <em>스탠퍼드대학교가 메타의 라마 중 매개변수가 가장 적은 버전(7B)을 기반으로 한<br>소형 언어 모델인‘알파카 7B를 만들었다. /Stanford CRFM</em>
</p>

<br>

<p align="center">
  <img src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/3f875596-a021-4f41-a669-fb2a3086b0da"><br>
  <em>메타가 2023년 7월 18일에 공개한 'Llama2'는 매개변수 규모에 따라<br>세 가지 모델(70억 개, 130억 개, 700억 개)로 제공된다. /Meta</em>
</p>

<br>

## 현재 한국어 언어 모델의 한계

### 성능 문제

<br>![image](https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/ef62d03b-33cc-41b5-9120-4e430f94c335){: .align-center width="75%" height="75%"}<br>

네이버와 카카오는 해외 빅테크 기업들이 주력하는 인공지능(AI) 분야에서 아직 경쟁력을 갖추지 못하고 있다. 챗GPT 등장 이후 불과 1년 만에 구글, 마이크로소프트(MS)를 중심으로 AI 패권 경쟁이 격화되자 네이버는 24년 8월 대규모 언어 모델(LLM)인 ‘하이버클로바X’를 내놓았다. 네이버는 한국어와 한국 문화에 최적화된 초거대 AI 모델이란 점을 내세웠지만 챗GPT나 바드 등 빅테크가 내놓은 모델도 한국어 학습 수준을 높이며 네이버의 장점이 희석됐다.

업스테이지의 솔라(SOLAR)는 23년 12월 AI 모델 성능 순위 매기는 허깅페이스의 '오픈 LLM 리더보드' 소형모델 부문서 1위를 기록했다. 알리바바의 '큐원', 메타의 '라마 2', 미스트랄AI의 '미스트랄' 사전학습 모델보다 높은 점수를 받았다. 23년 8월에는 오픈AI의 GPT-3.5 벤치마크 점수도 넘겼다.

### 서비스 연동 사례 부족

<p align="center">
  <img src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/f796ad30-0406-42c3-b300-ba671e7e133e"><br>
  <em>1월 5주 오픈 Ko-LLM 리더보드 /업스테이지, NIA</em>
</p>

<br>

실제로 Ko-LLM 리더보드를 통해 좋은 성적을 낸 모델이 기업활동에 본격 활용하게 된 사례도 있다.

1월 11일에 업스테이지의 글로벌 1위 대형언어모델(LLM) '솔라(SOLAR)'를 카카오톡 ‘아숙업(AskUp)’에 적용했다. 아숙업의 대화 중 10% 정도만 솔라를 적용했다.

롯데정보통신의 'LDCC/LDCC-SOLAR-10.7B' 모델은 1월 3주에 이어 2주째 정상을 지켰다. 23년 11월 1주 차에 '라마 2' 기반 모델로 1위를 차지한 이후 '솔라'에 이어 야놀자의 솔라 미세조정 버전을 도입하는 등 다양한 모델로 계속 정상을 차지하고 있다.

이처럼 정상급 LLM 기술력을 선보인 롯데정보통신은 1월 24일 AI 플랫폼 ‘아이멤버(Aimember)’를 롯데그룹 전 계열사에 도입한다고 발표했다. 아이엠버에 적용한 언어모델은 '롯데GPT'로, 바로 리더보드 1위의 기술력을 바탕으로 했다.

한국어 언어 모델이 일반 성능에서는 아직 부족하나, 특정 분야에 특화한 언어 모델은 충분한 성능을 보이고 있다. 향후에 사내 다양한 프로젝트에 직접 구축한 언어 모델을 활용하려면 자체 파운데이션 모델 구축, 각 비즈니스에 맞게 미세조정을 진행해 특화 언어 모델을 구축해야 한다.

<br>

## sLLM의 이점

sLLM은 LLM에 비해 매개변수를 줄여 학습을 위한 비용 및 시간 절감이 가능한 것이 특징이다. 보다 빠르게 미세조정(파인튜닝)을 해 정확도를 높일 수 있는 데다 특정 분야 및 기업에서 기존에 보유한 데이터를 활용해 할루시네이션(환각) 현상을 줄일 수 있다. [6]

## LLM의 문제와 해결 방안

LLM과 sLLM은 모르는 부분도 그럴싸하게 포장해 오답을 내놓는 ‘할루시네이션(환각)’이라는 고질적인 문제를 갖고 있다.

업계에서 현재 할루시네이션을 줄이기 위해 사용되는 기술은 크게 ‘파인튜닝’과 ‘검색증강생성(RAG)’이 있다. 파인튜닝은 특정 도메인에 LLM이 접목될 경우 높은 답변 적합성을 위해 기존 LLM에 특정 데이터셋을 활용해 미세조정하는 작업을 의미한다. 다만 파인튜닝은 최신성과 적합성을 제고할 수는 있으나, 데이터 용량 증가에 따른 높은 비용과 지속적인 업데이트의 어려움이 수반된다.

반면 RAG는 파인튜닝에 이어 전 세계적으로 주목받는 기술군으로 파인튜닝에 활용한 특정 데이터셋보다 보안성이 높고 최신의 데이터를 답변 생성에 사용한다. 답변 생성 시 자체 데이터를 활용해 답변이 적합한지 확인하고 사용자에게 보여주는 피드백 과정이 추가돼 할루시네이션 방지에 더욱 유리하다. 모델이 사람의 언어를 이해하고 답변을 위한 검색을 하는 과정에서 기존 키워드 검색 방식과 문맥과 의미를 중요하게 생각하는 시맨틱 검색 방식의 혼합형인 하이브리드 검색 방식이 가능하기 때문이다.

💡 **파인 튜닝**이란, 사전 학습 모델(pre-trained model)에 도메인 특화 데이터를 추가 학습시켜 맞춤형 모델로 업데이트 하는 것을 의미한다.
{: .notice--info}

💡 **RAG(Retrieval Augmented Generation)**은 베이스 모델의 외부에서 데이터를 가져와 임베딩을 얻어 Vector DB에 저장하고, 사용자가 질문을 하면 관련도가 가장 높은 임베딩을 찾아내서 모델에 이를 제공하는 방식이다.
{: .notice--info}

## References

- [https://www.joongang.co.kr/article/25225624#home](https://www.joongang.co.kr/article/25225624#home)
- [https://www.chosun.com/economy/tech_it/2024/01/25/HOHGI24D5JAP7HXHDRGDCSXG6U/](https://www.chosun.com/economy/tech_it/2024/01/25/HOHGI24D5JAP7HXHDRGDCSXG6U/)
- [https://zdnet.co.kr/view/?no=20240110140542](https://zdnet.co.kr/view/?no=20240110140542)
- [https://www.aitimes.com/news/articleView.html?idxno=156455](https://www.aitimes.com/news/articleView.html?idxno=156455)
- [https://www.aitimes.com/news/articleView.html?idxno=156763](https://www.aitimes.com/news/articleView.html?idxno=156763)
- [https://m.ddaily.co.kr/page/view/2024011807435856569](https://m.ddaily.co.kr/page/view/2024011807435856569)
- [https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)
