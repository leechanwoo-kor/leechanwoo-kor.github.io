---
title: "[LLM] sLLM(small Large Language Model)"
categories:
  - LLM
tags:
  - LLM
  - sLLM
toc: true
toc_sticky: true
toc_label: "sLLM(small Large Language Model)"
toc_icon: "sticky-note"
---

<br>![image](https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/d1e32689-d62b-4650-b751-5ae353c001cb){: .align-center}<br>

# sLLM (small Large Language Model)

매개변수(Paramgeter)가 기존 대형언어모델(Large language models, LLM)에 비해 변수의 수가 50억(5B) 내지 100억(10B) 개로 줄여 학습을 위한 소요 비용이나 시간을 대폭 줄이고 미세조정(Fine-Tuning)으로 정확도를 높여 다른 애플리케이션과 통합하기도 쉬운 모델을 ‘경량화 대형언어모델(smaller Large Language Model, sLLM)’이라고 한다.
{: .notice}

## 서론

ChatGPT 등장 이후 생성형 AI의 근간인 LLM(Large Language Model, 거대언어모델)의 비즈니스 활용이 주목을 받는 와중에 작년 메타의 라마(LLaMA)가 공개되면서 LLM의 한계를 보완한 sLLM(small Large Language Model, 소형언어모델)가 새로운 대안으로 떠오르고 있다.

### LLM(Large Language Model)

LLM은 인간의 언어를 이해하고 생성할 수 있도록 훈련된 언어 모델이다. 생성형 AI의 대표주자인 챗GPT가 이 LLM을 기반으로 만들어졌다. 언어 모델의 크기는 통상 매개변수(시스템상 작동에 영향을 미치는 데이터) 개수에 따라 정해진다. 1000억 개 이상이면 LLM으로 분류된다. 챗GPT에 적용된 'GPT-3'의 매개변수는 1750억 개이며, 구글이 개발한 '팜(PaLM)'은 5400억 개에 달한다.

### sLLM(small Large Language Model)

sLLM은 최소 수십억 개의 매개변수만으로 연산 작업을 단축시켜 보다 효율적으로 운영될 수 있는 언어 모델에 속한다. sLLM이 처음 주목받기 시작한 건 23년 초부터이다. 당시 메타는 LLM인 '라마(LLaMA)'를 공개한 데 이어 미국 스탠퍼드대와 함께 라마를 기반으로 한 sLLM '알파카(Alpaca)'를 개발했다. 매개변수가 적은 만큼 LLM에 비해 더 적은 컴퓨팅(처리 과정) 자원으로 최대한의 효율을 낼 수 있다. 그만큼 **훈련 시간, 비용, 용량, 전력 소모량**이 훨씬 절감된다.

<p align="center">
  <img src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/57ae4139-9a83-404e-a18b-6cbd0d192ba5"><br>
  <em>스탠퍼드대학교가 메타의 라마 중 매개변수가 가장 적은 버전(7B)을 기반으로 한<br>소형 언어 모델인‘알파카 7B를 만들었다. /Stanford CRFM</em>
</p>

<br>

<p align="center">
  <img src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/3f875596-a021-4f41-a669-fb2a3086b0da"><br>
  <em>메타가 2023년 7월 18일에 공개한 'Llama2'는 매개변수 규모에 따라<br>세 가지 모델(70억 개, 130억 개, 700억 개)로 제공된다. /Meta</em>
</p>

<br>

## 현재 한국어 언어 모델의 한계

### 성능 문제

<br>![image](https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/ef62d03b-33cc-41b5-9120-4e430f94c335){: .align-center width="75%" height="75%"}<br>

네이버와 카카오는 해외 빅테크 기업들이 주력하는 인공지능(AI) 분야에서 아직 경쟁력을 갖추지 못하고 있다. 챗GPT 등장 이후 불과 1년 만에 구글, 마이크로소프트(MS)를 중심으로 AI 패권 경쟁이 격화되자 네이버는 24년 8월 대규모 언어 모델(LLM)인 ‘하이버클로바X’를 내놓았다. 네이버는 한국어와 한국 문화에 최적화된 초거대 AI 모델이란 점을 내세웠지만 챗GPT나 바드 등 빅테크가 내놓은 모델도 한국어 학습 수준을 높이며 네이버의 장점이 희석됐다.

업스테이지의 솔라(SOLAR)는 23년 12월 AI 모델 성능 순위 매기는 허깅페이스의 '오픈 LLM 리더보드' 소형모델 부문서 1위를 기록했다. 알리바바의 '큐원', 메타의 '라마 2', 미스트랄AI의 '미스트랄' 사전학습 모델보다 높은 점수를 받았다. 23년 8월에는 오픈AI의 GPT-3.5 벤치마크 점수도 넘겼다.

### 서비스 연동 사례 부족

<p align="center">
  <img src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/f796ad30-0406-42c3-b300-ba671e7e133e"><br>
  <em>1월 5주 오픈 Ko-LLM 리더보드 /업스테이지, NIA</em>
</p>

<br>

실제로 Ko-LLM 리더보드를 통해 좋은 성적을 낸 모델이 기업활동에 본격 활용하게 된 사례도 있다.

1월 11일에 업스테이지의 글로벌 1위 대형언어모델(LLM) '솔라(SOLAR)'를 카카오톡 ‘아숙업(AskUp)’에 적용했다. 아숙업의 대화 중 10% 정도만 솔라를 적용했다.

롯데정보통신의 'LDCC/LDCC-SOLAR-10.7B' 모델은 1월 3주에 이어 2주째 정상을 지켰다. 23년 11월 1주 차에 '라마 2' 기반 모델로 1위를 차지한 이후 '솔라'에 이어 야놀자의 솔라 미세조정 버전을 도입하는 등 다양한 모델로 계속 정상을 차지하고 있다.

이처럼 정상급 LLM 기술력을 선보인 롯데정보통신은 1월 24일 AI 플랫폼 ‘아이멤버(Aimember)’를 롯데그룹 전 계열사에 도입한다고 발표했다. 아이엠버에 적용한 언어모델은 '롯데GPT'로, 바로 리더보드 1위의 기술력을 바탕으로 했다.

한국어 언어 모델이 일반 성능에서는 아직 부족하나, 특정 분야에 특화한 언어 모델은 충분한 성능을 보이고 있다. 향후에 사내 다양한 프로젝트에 직접 구축한 언어 모델을 활용하려면 자체 파운데이션 모델 구축, 각 비즈니스에 맞게 미세조정을 진행해 특화 언어 모델을 구축해야 한다.

<br>

## References

- [https://www.aitimes.kr/news/articleView.html?idxno=30000](https://www.aitimes.kr/news/articleView.html?idxno=30000)
- [https://www.joongang.co.kr/article/25225624#home](https://www.joongang.co.kr/article/25225624#home)
- [https://www.chosun.com/economy/tech_it/2024/01/25/HOHGI24D5JAP7HXHDRGDCSXG6U/](https://www.chosun.com/economy/tech_it/2024/01/25/HOHGI24D5JAP7HXHDRGDCSXG6U/)
- [https://zdnet.co.kr/view/?no=20240110140542](https://zdnet.co.kr/view/?no=20240110140542)
- [https://www.aitimes.com/news/articleView.html?idxno=156455](https://www.aitimes.com/news/articleView.html?idxno=156455)
- [https://www.aitimes.com/news/articleView.html?idxno=156763](https://www.aitimes.com/news/articleView.html?idxno=156763)
- [https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)
