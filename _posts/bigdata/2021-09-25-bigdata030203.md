---
title: "[빅데이터분석기사 필기] Ⅲ.빅데이터 모델링 - 02. 분석기법 적용 (3) 인공신경망"
categories:
    - 빅데이터분석기사
tags:
    - 빅데이터분석기사
toc: true
toc_sticky: true
toc_label: "02. 분석기법 적용 (3) 인공신경망"
toc_icon: "sticky-note"
---

**키워드🔑**<br>
인공신경망, 퍼셉트론, 활성함수, XOR문제, 다층퍼셉트론, 역전파알고리즘, 기울기소실,
<br>활성화함수, 계단함수, 부호함수, 시그모이드, tanh함수, ReLU, Softmax 함수
{: .notice--warning}

# 02. 분석기법 적용

## 1. 분석 기법

### 4) 인공신경망

<br>
(1) 인공신경망<br>
📌 뉴런의 전기신호 전달을 모방한 기계학습 모델

- 인공신경망(ANN; Artificial Neural Network)
	- 입력값 받아서
	- 출력값 만들기 위해
	- 활성화 함수 사용함
- 활성화 함수/활성함수(Activation Function): 입력신호의 총합을 출력신호로 변환하는 함수
	- 입력받은 신호를 얼마나 출력할지 결정
	- 출력된 신호의 활성화 여부 결정
- 신경망 모형의 특징
	- 변수가 많은 경우/입출력 변수간 복잡한 비선형 관계일 때 유용함
	- 잡음에 민감하지 않음
	- 은닉층 너무 많으면, 과대적합 위험
	- 은닉층 너무 적으면. 충분한 데이터 표현 X

<br>
(2) 인공신경망의 역사<br>
📌 퍼셉트론과 XOR 선형 분리 불가 문제 → 다층퍼셉트론과 기울기 소실 문제 → 인공지능과 딥러닝

|1세대<br>1943 ~ 1986|2세대<br>1986 ~ 2006|3세대<br>2006 ~ now|
| :-----: | :-----: | :-----: |
|퍼셉트론|다층 퍼셉트론|인공지능 부각|
|-구성:입력층/출력층<br>-최초의 인공신경망|-구성:입력층/하나 이상의 은닉층/출력층<br>-비선형적 분리 데이터에 대한 학습 가능<br>은닉층을 통해 XOR 문제를 해결함|-알파고 등에 의해 인공지능이 부각됨<br>-빅데이터 수집, 분석이 가능해지면서 발전<br>-CNN, RNN 등의 딥러닝 기술 발전|
|순방향 신경망|역전파 알고리즘|tanh, ReLU, Leaky ReLU, Softmax, ...|
|-데이터 전파:입력 → 은닉 → 출력<br>-입력데이터가 판별함수값으로 변환<br>-선형 분류 가능한 신경망|-역방향 가중치 갱신(출력 → 은닉 → 입력)<br>-오차를 최소화시키도록 학습<br>-Backpropagation Algorithm|-tanh,ReLU를 활성함수로 사용하여<br>기울기소실문제를 해결함<br>-Leaky ReLU, Softmax 등으로 발전|
|XOR 선형 분리 불가 문제|기울기 소실/사라지는 경사|딥러닝 기술 발전|
|선형분류만 가능한 퍼셉트론으로는<br>XOR연산을 할 수 없다는 문제가 있음|역전파에서 활성함수인<br>시그모이드 함수에 대해 편미분하는데,<br>1보다 작으므로 계속 곱하다 보면<br>0에 가까워지면서 기울기가 사라진다|CNN: 합성곱 신경망<br>(Convolutional Neural Network)<br>RNN: 순환 신경망<br>(Recurrent Neural Network)|

---

<br>
(3) 인공신경망의 구조<br>
📌 퍼셉트론 / 다층 퍼셉트론

- 퍼셉트론(Perceptron) 구성: 입력값 / 가중치 / 순 입력함수 / 활성함수 / 출력값(입력값)
	- 입력값: 훈련 데이터(Training Data)
	- 순 입력함수: 함수에서 모든 입력값과 가중치를 곱하고 Sum
	- 활성함수
		- 순입력함수에서 나온 결과 임계값 비교 → 출력값(예측값)으로 1or -1
		- 예측값 ≠ 실젯값 → 가중치 업데이트 → 이 과정을 반복하면서 학습하는 것
	- 퍼셉트론 문제점: XOR 선형 분리 불가 문제 → 해결 위해 다층 퍼셉트론 등장
		- AND 연산: 입력값(X, Y)이 모두 1이면 1출력 / 나머지는 0 → 선형분리 가능
		- OR 연산: 입력값(X, Y)이 모두 0이면 0출력 / 나머지는 1 → 선형분리 가능
		- XOR 연산: 입력값(X, Y)이 같으면 0 출력 / 다르면 1 출력 → 선형분리 불가능

--- 

- 다층 퍼셉트론(MLP; Multi-Layer Perceptrons): 비선형적으로 분리되는 데이터에 대한 학습이 가능한 퍼셉트론
	- 구성: 입력층과 출력층 사이에 1개 이상의 은닉층
	- 활성화 함수: 시그모이드 함수(Sigmoid Function)
		- 시그모이드: 유한한 영역을 가짐 / 미분 가능 / 모든 점에서 음이 아닌 미분값 / 하나의 변곡점
	- 역전파 알고리즘을 통해 다층에서 학습 가능
		- 예측값과 실젯값의 차이인 에리(Error)를 통해 가중치 조정 → 연걸 강도 갱신 → 목적함수 최적화
- 다층 퍼셉트론의 문제점
	- 과대 적합: 학습 데이터가 부족하면 실제 데이터에서 성능 떨어짐 → 빅데이터 확보 가능해지면서 해결
	- 기울기 소실: 시그모이드 함수의 편미분을 진행하면 기울기가 0에 근시 → ReLU, tanh함수 사용하여 해결

---

<br>
(4) 뉴런의 활성화 함수<br>
📌 순 입력함수에서 전달받은 값을 출력값으로 반환하는 함수

- 계단 / 부호 / 시그모이드 / tanh / ReLU / Leaky ReLU / Softmax 함수
- Dying ReLU: ReLU 함수에서 마이너스(-) 값 → 전부 0을 출력 → 일부 가중치들이 업데이트 되지 않음

|계단함수<br>Step<br>Function|부호함수<br>Sign<br>Function|시그모이드 함수<br>Sigmoid<br>Function|하이퍼볼릭<br>탄젠트 함수<br>tanh Function|ReLU 함수|Leaky ReLU|Softmax 함수|
| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |
|임계값 기준<br>활성화 or<br>비활성화<br>(Y = 1 or 0)|임계값 기준<br>양 or 음 출력<br>(Y = +1 or -1)|-하나의 변곡점<br>-로지스틱 함수<br>-기울기 소실의 원인<br>( 0 ≤  y ≤ +1 )|시그모이드의<br>기울기 소실을<br>해결함|X > 0 → Y = X<br>기울기소실해결<br>X ≤ 0 → Y = 0<br>뉴런이 죽음|ReLU의<br>Dying ReLU<br>현상을 해결|출력값 여러개<br>목표치 다범주<br>각 범주에 속할<br>사후확률 제공|

---
