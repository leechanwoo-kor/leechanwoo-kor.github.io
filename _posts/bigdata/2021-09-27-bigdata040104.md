---
title: "[빅데이터분석기사 필기] Ⅳ.빅데이터 결과 해석 - 01. 분석 모형 평가 및 개선 (4)"
categories:
    - 빅데이터분석기사
tags:
    - 빅데이터분석기사
toc: true
toc_sticky: true
toc_label: "01. 분석 모형 평가 및 개선 (4)"
toc_icon: "sticky-note"
---

**키워드🔑**<br>
과대적합방지, 데이터 증강, 모델복잡도감소, 가중치규체, L1규제, L2규제, 드롭아웃,
<br>매개변수최적화, 확률적 경사 하강법, 모멘텀, AdaGrad, Adam, 
<br>취합방법론, 다수결, 배깅, 페이스팅, 랜덤서브스페이스, 랜덤패치, 랜덤포레스트, 
<br>부스팅방법론, 에이다부스트, 그래디안트부스트
{: .notice--warning}

# 01. 분석 모형 평가 및 개선

## 2. 분석모형 개선

### 1) 과대 적합 방지

<br>
(1) 과대 적합(Over-fitting)<br>
📌 지나친 학습 → 일반화↓

- 과대 적합
	- 제한된 학습데이터셋에 지나치게 특화되어 새로운 데이터에 대한 오차가 매우 커지는 현상
- 과대 적합이 발생하는 경우
	- 모델 파라미터 개수 많음
	- 학습데이터셋 부족
- 일반화(Generalization): 테스트데이터에 대해 높은 성능을 갖춤 / 정상추정함 / 과소&과대적합 X
	- 과소 적합
		- 지나치게 단순한 모델
		- 데이터에 내재된 구조를 학습하지 못함
	- 과대 적함
		- 지나치게 학습데이터에 적합
		- 일반화 떨어짐

---

<br>
(2) 과대 적합 방지<br>
📌 데이터 증강 / 모델 복잡도 감소 / 가중치 규체 / 드롭아웃

- 데이터 증강(Data Augmentation)
	- 데이터 양이 적을 경우, 데이터를 변형하여 양을 늘림
- 모델 복잡도 감소
	- 은닉층 개수 감소/모델 수용력 낮춤 → 모델 복잡도 줄일 수 있음
- 가중치 규제 적용: 개별 가중치 값을 제한 → 복잡한 모델을 간단하게
	- 비용함수(Cost Function): 관측값과 연산값의 차이를 도출
		- 비용함수 최소화를 위해서, 가중치들이 작아져야 함
	- λ = 규제 강도를 정하는 하이퍼 파라미터
		- λ 값이 크면, 가중치 규제를 위해 추가한 항들을 작게 유지하는 것을 우선함
	- L1 규제: 모든 가중치들의 절댓값 합계를 비용함수에 추가	→ λ |w|
	- L2 규제: 모든 가중치들의 제곱합을 비용함수에 추가		→ (1/2) λw²
- 드롭아웃(Dropout): 학습 과정에서 신경망 일부를 사용하지 않음
	- 특정 뉴런/조합에 너무 의존적인 인공신경망이 되는 것을 방지
	- 매번 랜덤으로 뉴런 선택 → 서로 다른 신경망들을 앙상블하는 것과 같은 효과
	- 신경만 학습 과정에서만 사용하는 기법
	- 예측 과정에서는 드롭아웃을 사용하지 않음
- 드롭아웃 유형: 초기(DNN) / 공간적(CNN) / 시간적(RNN) 드롭아웃
	- 초기 드롭아웃 - DNN에서 사용
		- p의 확률로 노드들을 생략하여 학습함
		- 일반적으로 p = 0.5
	- 공간적 드롭아웃 - CNN에서 사용
		- 피처맵 내 노드 전체에 대해 드롭아웃 적용 여부를 결정함
	- 시간적 드롭아웃 - RNN에서 사용
		- 노드가 아닌, 연결선 일부를 생략하여 학습함(Drop Connection)
	
---


### 2) 매개변수 최적화

<br>
(1) 매개변수(Parameter)<br>
📌 데이터 학습을 통해 모델 내부에서 결정되는 변수

<br>
(2) 매개변수 최적화(Parameter Optimization)<br>

- 손실함수(Loss Function): 학습모델의 출력값과 레이블 실제값의 차이(오차)
- 모델 학습의 목적 = 매개변수 최적화
	- 손실함수의 값을 최소화하는 매개변수를 찾는 것
	- 오차를 최소화하는 가중치와 편향을 찾는 것

<br>
(3) 매개변수 종류<br>
📌 가중치 & 편향

- 가중치(Weight)
	- 곱
	- 입력값마다 각기 다르게 곱해지는 수치
	- y = ax+b에서 절편 a 해당
- 편향(Bias)
	- 합
	- 가중합에 더해주는 상수
	- y = ax+b에서 기울기 b에 해당

---

<br>
(4) 매개변수 최적화 기법<br>
📌 확률적 경사 하강법 / 모멘텀 / AdaGard / Adam

- 2차원 손실함수 그래프를 이용하여 매개변수 최적화를 수행
	- X축 = 가중치(Wi)
	- Y축 = 손실값(=오차)
	- 그래프에서 기울기 = 0인 지점 = 손실값이 최소화되는 지점 = 최적의 매개변수를 찾을 수 있음
- 매개변수 최저고하 과정은 학습률에 따라서 달라짐
	- 학습률 적음 → 매우 느린 학습 → 최적화에 많은 시간 소요
	- 학습률 높음 → 기울기 = 0 지점을 지나침 → 최적화 실패
	- 학습률 적당 → 기울기 = 0 지점 찾음 → 최적화 성공

|확률적 경사 하강법|모멘텀|AdaGrad|Adam|
|:-----:|:-----:|:-----:|:-----:|
|SGD; Stochastic<br>GradientDescent|Momentum|Adaptive Gradient<br>Algorithm|Adaptive Moment<br>Estimation|
|![image](https://user-images.githubusercontent.com/55765292/135569847-65ccdc70-5755-49e1-bc85-b984ea6d6db3.png)|![image](https://user-images.githubusercontent.com/55765292/135568327-c9034cce-bdb1-48af-936d-2b060038fb28.png)|![image](https://user-images.githubusercontent.com/55765292/135568340-5382d296-029a-41c2-aeba-5a8cfbf59518.png)|![image](https://user-images.githubusercontent.com/55765292/135568356-25076fe2-a4cd-4b2f-ac5d-b479958f80ea.png)|
|- 먼저 손실함수 기울기 구함<br>→ 기울기따라 조금씩 아래로<br>→손실함수 최소 지점 도달<br>- 최적점 근처에서 느리게 진행|-모멘텀 = SGD + 속도<br>- 기울기 방향으로 가속됨|-학습률 감소 기법을 적용<br>- 기울기 큰 부분에서 크게 학습<br>-최적점에 가까워질수록<br>학습률 줄임 → 조금씩적게 학습|- Adam = 모멘텀 + AdaGrad<br>- 탐색경로 또한 모멘텀과<br>AdaGrad를 합친 양상|
|경로: 지그재그로 크게 변함| 공이 그릇 바닥을 구르듯 움직임<br>지그재그 정도 덜함|처음에는 큰폭이었다가<br>→갱신 움직임 크게 줄어듦|공이 그릇 바닥을 구르듯 움직임<br>모멘텀보다 좌우 흔들림 적음|

- 확률적 경사 하강법(SGD)
	- 기울기를 구할 떄 1개의 데이터를 무작위로 선택함(확률적)
	- 문제점: 지역극소점에 갇히는 문제 자주 발생
		- 손실함수 그래프에서 지역극소점(Local)에 갇혀서, 전역극소점(Global)을 찾지 못하는 경우가 많음
		- 손실함수가 방향에 따라 기울기가 달라지는 비등방성 함수일 경우 매우 비효율적
	- SGD의 단점 개선을 위해 고안됨 방법론들이 모멘텀/AdaGrad/Adam
	- 탐색경로: 지그재그로 크게 변함
- 모멘텀(Momentum)
	- SGD + 속도
	- 기울기가 줄어도 누적된 기울기 값에 의해 → 탐색경로의 변위가 줄어들어서 → 빠르게 최적점으로 수렴
	- X축의 한 방향으로 일정한 가속 / Y축 방향 속도는 일정하지 않음
	- 관성의 방향을 고려하여, 진동과 폭을 줄이는 효과
	- 모멘텀 갱신경로
		- 공이 그릇 바닥을 구르듯 움직임
		- SGD보다 지그재그 덜함
- AdaGrad(Adaptive Gradient Algorithm)
	- 학습 진행할수록 학습률 감소시킴
	- 학습률 감소 기법 적용
		- 손실함수 처음 부분: 기울기 큼 → 학슴률 큼
		- 최적점에 가까워짐: 기울기 감소 → 학습률 줄여서 조금씩 작게 학습
	- 최적점 탐색경로
		- 손실함수 처음 부분: y축 방향으로 기울기 큼 → 큰 폭으로 움직임
		- 최적점에 가까워짐: y축 방향으로 갱신 강도 빠르게 감소 → 큰 폭으로 작아짐
	- 각각의 매개변수에 맞는 학습률 값을 만들어줌
	- 탐색경로: 지그재그 움직임이 빠르게 줄어든다
- Adam(Adaptive Moment Estimation)
	- 모멘텀 + AdaGrad
	- Adam 갱신경로
		- 모멘텀처럼 공이 그릇 바닥을 구르듯 움직임
		- 모멘텀보다 좌우 흔들림 적음

---

### 3) 분석 모형 융합

<br>
(1) 취합 방법론(Aggregation)<br>
📌 다수결 / 배깅 / 페이스팅 / 랜덤 서브스페이스 / 랜덤 패치 / 랜덤 포레스트

|다수결<br>(Majority Voting)|배깅<br>(Bagging)|페이스팅<br>(Pasting)|랜덤<br>서브스페이스|랜덤<br>패치|랜덤<br>포레스트|
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|-여러모형 결과 종합<br>-다수결로 최종 설정<br>-직접투표: 단순투표<br>-간접투표: 가중치|-복원추출로<br>학습데이터 나눔<br>-중복 허용하므로<br>편향가능성 있음|-비복원추출로<br>학습데이터 나눔<br>-중복사용X|-다차원 독립변수 중<br>일부 차원만 선택<br>-즉, 특성 샘플링<br>-학습데이터는<br>모두 사용함|-종속&독립변수<br>일부만 랜덤사용<br>-학습데이터, 특성<br>모두 샘플링|-의사결정나무결합<br>-독립변수 차원을<br>랜덤하게 감소시킴<br>그 중에서 선택<br>-모형성능 변동감소|

<br>
(2) 부스팅 방법론(Boosting)<br>
📌 에이다 / 그래디언트 부스트

- 에이다 부스트(AdaBoost) = 적응 부스트(Adaptive Boost)
	- 약한 모형 각각을 순차적으로 적용하는 과정에서 잘 분류된 샘플 사중치 낮추고 오 분류된 샘플 가중치 높여서 샘플 분포를 변화시키는 기법
- 그레디언트 부스트(Gradient Boost)
	- 약한 모형 각각을 순차적으로 적용하는 과정에서 오 분류된 샘플 에러를 최적화하는 기법
	
---

### 4) 최종 모형 선정

<br>
(1) 최종 모형 선정 절차<br>
📌 최종 모형 평가 기준 선정 → 최종 모형 분석 결과 검토 → 알고리즘별로 결과 비교

- 평가 기준 선정: 정확도/재현율/징밀도 등의 평가지표 이용
- 분석 결과 검토: 평가 기준, 실질적인 활용 가능성에 대한 검토
- 알고리즘별 결과 비교: 알고리즘별로 파라미터를 변경하며 수행 → 변경 전후의 차이점 비교, 결과 기록

---
