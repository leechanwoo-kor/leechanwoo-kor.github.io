---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (7)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (7)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Setting Up your Optimization Problem

### Weight initialization for Deep Networks

이전에 심층 신경망에서는 기울기 소실과 폭주 문제가 있을 수 있다는 것을 알았습니다. 이것에 대한 부분적 해결법으로 완전히 해결해 주지는 않지만 신경망의 무작위 초기화 선택을 더 괜찮거나 조심스럽게 하는 것입니다.

이를 이해하기 위해, 하나의 신경 세포를 초기화하는 방법을 예시로 시작해 봅시다. 이것을 심층 네트워크로 일반화할 것입니다.

![image](https://user-images.githubusercontent.com/55765292/177688457-c77280b3-0caf-477b-8473-07f3ccfd239b.png)

단 하나의 뉴런으로 된 예를 살펴보고 그리고 나서 나중에 심층 네트워크에 대해 얘기하겠습니다. 하나의 뉴런으로 4가지 특성을 입력할 수 있는데요. $x_1$에서 $x_4$까지 그리고 $a = g(z)$가 있고 
$\hat{y}$가 출력됩니다.

그러면 z는 위와 같은 식으로 나타나고 b는 일단 0으로 설정하고 무시하겠습니다. 그러면 z가 폭주하지 않으며 너무 작지 않게 하기 위해서 n이 클수록 $w_i$가 더 작기를 원할 것입니다. 왜냐하면 z는 $x_iw_i$의 합이기 때문에 그렇죠. 그리고 이런 항을 더하는 경우 각각의 항이 작은 값을 가져야 좋습니다.

한가지 합리적인 방법은 $w_i$의 분산을 $\cfrac{1}{n}$과 같도록 하는 하는 것으로 여기서 $n$은 뉴런으로 들어가는 입력 특성의 개수입니다. 실제로는 특정 레이어에 대한 가중치 행렬 $W$을 설정하는 것으로 이것은 np.random.randn 다음에 이것이 행렬의 모양이 어떻던 간에 여기 넣고 레이어 l의 각 단일 세포에 제공한 특징의 수 분의 1의 제곱근을 곱합니다. 따라서 이것은 n(l-1)으로 그것이 레이어 l에 들어가는 각각의 유닛별 숫자입니다.

만약에 여러분이 1/n을 쓰는 것 대신에 ReLU 활성화 함수를 쓰는 경우 분산을 2/n으로 설정하는 것이 조금 더 잘 작동합니다. 따라서 ReLU 활성화 함수를 쓸 때 초기화에서 종종 볼 수 있습니다. $g^{[l]}(z) = ReLU(z)$인 경우죠. 

그리고 그것은 확률변수에 얼마나 익순한지에 달려 있습니다. 알고 보니 가우스 확률 변수에 이것의 제곱근을 곱하면 분산이 2/n로 인용되고요. 그리고 n에서 n위첨자 l-1로 간 이유는 이 예에서는 n개의 입력 특성에 있는 로지스틱 회귀를 사용하지만 보다 일반적인 경우 레이어 l에는 해당 레이어의 각 단위가 n(l-1)개의 입력이 있습니다.

따라서 활성화의 입력 특성이 대략적으로 평균 0이고 분산이 1이면 z도 유사한 척도를 가집니다. 그리고 이것은 해결되지 않지만 그것은 기울기 소실 및 폭주 문제를 줄이는 데 확실히 도움이 되는데요. 각각의 가중치 행렬 w를 설정하려하기 때문에 1보다 너무 크지 않고 너무 작지 않아서 빠르게 폭주하거나 소실되지 않도록 합니다.

몇 가지 다른 분산을 언급했습니다. 방금 설명한 버전은 ReLU 활성화 함수로 가정하고 있고 이것은 논문에서 나왔습니다. 또 다른 변수 몇 가지가 있는데요.

tanh 활성화 함수를 이용하는 경우 상수 2를 쓰는 것보다 상수 1을 쓰는 것이 낫다는 논문이있습니다. 따라서 위의 분산으로 항을 대체합니다. 이것을 Xavier 초기화라고 합니다. 또 다른 버전은 Yoshua Bengio와 그의 동료들이 쓴 논문에서 있습니다. 하지만 이 공식을 이용하고 여러 이론적인 정의가 있습니다.

하지만 ReLu 활성화 함수를 이용하는 경우가 가장 흔한 활성화 함수입니다. 그래서 기존의 항을 사용할 것입니다. tanh를 사용한다면 대신 Xavier 초기화를 사용할 수 있습니다.

하지만 실제로는 이 공식들은 모두 시작점을 제시합니다. 가중치 행렬 초기화의 분산에 사용할 기본값을 줍니다. 여기 있는 분산을 원한다면 분산 매개변수가 튜닝할 수 있는 하이퍼 파라미터 중 하나일 수 있습니다. 그래서 이 공식에 곱해지는 또 다른 매개변수를 가지고 이 공식에 곱해지는 또 다른 매개변수를 가지고 하이퍼 파라미터 급증의 일부로 해당 거듭제곱 횟수를 튜닝할 수 있습니다.

때때로 하이퍼 파라미터 튜닝은 적당한 크기 효과가 있습니다. 이것은 튜닝을 시도할 만한 첫 번째 하이퍼 파라미터는 아니지만 이것을 통해 튜닝한 몇 개의 문제들을 접했었는데요. 비교적 도움이 됩니다.

하지만 선호도로만 따지면 밑에 있는 편입니다. 튜닝할 수 있는 다른 하이퍼 파라미터와 비교했을 때 말이죠. 따라서 그것이 가중치를 초기화하는 방법에 대한 합리적인 크기를 선택하는 것뿐만 아니라 기울기 소실 및 폭주 문제에 대한 약간의 직관을 제공하기를 바랍니다.

그것이 가중치를 너무 빠르게 폭주하게 하지 않고 너무 빨리 0으로 줄어들지 않도록 할 것이며 따라서 가중치나 기울기가 폭발적으로 늘어나거나 소실하지 않게 합리적으로 심층 네트워크를 훈련시킬 수 있습니다.

심층 네트워크를 훈련할 때 신경망을 훨씬 더 빠르게 훈련시키는 데 도움이 되는 또 다른 기술입니다.


### Numerical Approximation of Gradients

역전파를 도입하는 경우 gradient checking이라는 테스트를 찾으실텐데요. 역전파의 도입이 올바르도록 도와줄 것입니다. 가끔은 공식들을 다 적어놓고 100퍼센트 맞는지 확실하지 않은 경우가 있는데요. 상세한 부분까지 역전파에서 맞는지 말이에요.

gradient checking까지 이야기하기 위해서 일단 먼저 숫자적으로 기울기의 산출값을 구하는 방법을 다루겠습니다. 다음에는 gradient checking을 도입하는 방법과 backdrop의 도입이 맞도록 하는 방법에 대해 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/177693782-92b6dcc2-26a5-4027-b299-5128cb58f578.png)

자, 그럼 f 함수를 가지고 보겠습니다. $f(\theta) = \theta^3$입니다. 쎄타의 일정한 값으로 시작해보겠습니다. 쎄타가 1이라고 해봅시다. 쎄타를 단순히 약간 우측으로 이동시키는 대신에, 쎄타 덩하기 앱실론 만큼 말이죠. 오른쪽으로 약간 이동시키고 또 왼쪽으로 쎄타 빼기 앱실론만큼 또 이동시키겠습니다. 그럼 각각 1, 1.01, 0.99가 됩닏나. 앱실론은 이전과 동일합니다.
$\epsilon = 0.01$ 입니다.

여기 조그마한 삼각형을 갖고 높이와 너비로 계산하는 것보다 더 나은 기울기의 추정치를 구할 수 있는데요. 여기 지금 $f(\theta+\epsilon)$과 $f(\theta-\epsilon)$ 입니다. 그렇게 해서 높이 나누기 너비를 더 큰 삼각형에서 구합니다.

기술적인 이유에서, 상세히 이야기하진 않겠지만, 높이 나누기 너비를 여기 이 큰 삼각형에서 하면 훨씬 더 정확한 쎼타 derivative의 근사치가 나옵니다. 위의 삼각형만 갖는 대신에 마치 2개의 삼각형을 갖는 것과 같습니다.

그렇게 해서 2개를 모두 반영시키는 것입니다. 이 큰 삼각형을 사용함으로써 말이죠. 그렇기 때문에 한쪽의 차이를 갖추는 것이 아니라 양쪽의 차이를 갖추는 것입니다.

계산을 하게 되면 위의 그림처럼 3.0001이라는 것을 알 수 있습니다. 이전에 $g(\theta)$가 쎄타가 1이었을 때, 근사치 오류는 0.0001입니다. 이전과는 다르게 one sided difference을 가졌는데요. 그렇게 되면 근사치가 3.0301으로 오류값이 더 큽니다.

이러한 two sided difference는 derivative를 추정하는데 이 값이 3과 매우 유사하다는 것을 알 수 있습니다. 이것은 더 큰 confidence로 $g(\theta)$가 f의 derivative의 올바른 도입이라고 할 수 있습니다.

이 방법을 gradient checking과 역전파에 쓰는 경우, one-sided difference를 쓰는 것보다 2배는 더 느리게 작동합니다. 실제로는 이 다른 방법을 쓰는 것이 값어치가 있다고 봅니다. 훨씬 더 정확도가 높기 때문이죠.

다음은 선택적인 이론 내용인데요. 미적분학에 조금 익숙하신 분들을 위한 내용입니다. 공식적은 derivative의 정의는 아주 작은 값의 앱실론의 경우, limit을 사용하는 아래 공식입니다. 앱실론이 0으로 가는 경우입니다. limit의 정의는 미적분학 수업을 통해 배우기 되는 내용인데요. 여기서 다루지 않겠습니다.

0이 아닌 앱실론 값의 경우, 여기 approximation의 오류는 앱실론 제곱의 오더입니다. 그리고 앱실론은 굉장히 작은 숫자입니다. 만약 앱실론이 0.01인 경우, 여기서와 같이 말이죠, 그럼 앱실론 제곱은 0.0001입니다. 여기 큰 O표기는 오류가 어떤 상수값 고하기 이것이라는 건데요. 이것은 사실 정확히 approximation 오류입니다. 그럼 큰 O 상수는 1이 되겠죠.

반면에 만약 이 공식을 쓰면, 오류는 앱실론의 오더입니다. 그리고 앱실론이 1보다 작은 경우, 앱실론은 앱실론 제곱보다 훨씬 더 큽니다. 그렇기 때문에 여기 이 공식이 여기 이 왼쪽 공식보다 훨씬 더 덜 정확한 approximation입니다. 그렇기 때문에 저희는 gradient checking을 할 때, two sided difference를 선호합니다. $\cfrac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$을 계산할 때 말이죠.

이는 numerical approximations와 관련된 내용입니다. 기억해야 할 부분은 two-sided difference 공식이 더 정확하다는 점입니다. 그러므로 다음엔 gradient checking을 할 때는 그 방법을 이용할 것입니다. 이제 two-sided difference를 통해, 숫자적으로 다른 사람이 주는 g함수가, $g(\theta)$가 f 함수 derivative의 올바른 도입인지 입증할 수 있습니다.
