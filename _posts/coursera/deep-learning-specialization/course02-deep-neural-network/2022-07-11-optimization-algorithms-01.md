---
title: "[Ⅱ. Deep Neural Network] Optimization Algorithms (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Optimization Algorithms (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Optimization Algorithms

### Mini-batch Gradient Descent

이번에는 신경망 네트워크를 훨씬 더 빨리 트레이닝 시킬 수 있도록 도와주는 최적화 알고리즘에 대해 살펴보도록 하겠습니다. 머신 러닝을 적용한다는 것은 매우 반복적인 업무를 동반한다고 했습니다. 여러 모델들을 그냥 무조건 트레이닝시키고 가장 잘 작동하는 것을 찾아야합니다. 그렇기 때문에 재빨리 모델들을 트레이닝 시키는 것이 매우 중요합니다.

한가지 더 어렵게 만들 수 있는 부분은 딥러닝이 빅데이터가 준비되었을 때 가장 잘 작동하는 경향이 있다는 것입니다. 신경망을 큰 데이터세트에서 트레이닝 시킬 수 있는데요. 이런 경우 속도가 매우 느리겠죠. 이런 경우 빠른 최적화 알고리즘과 그리고 양질의 최적화 알고리즘을 갖는 것이 팀 효율성의 속도를 높힐 수 있다는 것입니다.

그러면 이제 미니 배치 기울기 하강에 대한 이야기로 시작해보록 하겠습니다.

![image](https://user-images.githubusercontent.com/55765292/178169569-b41138c0-7e63-44ff-9f55-7133efe52e8b.png)

이전에 봤듯이 벡터화는 모든 m 예시에 대해서 효율적으로 산출을 할 수 있게 해줍니다. 특정한 공식 없이도 모든 전체 트레이닝세트를 처리할 수 있게 말이죠. 그렇기 때문에 트레이닝 예시를 가지고 방대한 매트릭스 X들에 쌓아올리는 것입니다.

$x^{(1)}, x^{(2)}, x^{(3)}, \dots, x^{(m)}$이 올라갑니다. 여기 Y값에 대해서도 비슷하게 말이죠. 따라서 X의 차원은 ($n_x, m$) Y의 차원은 ($m \times 1$)이었습니다. 벡터화는 모든 m 예시들을 꽤 빨리 처리할 수 있게 해주는데요. 만약 m이 아주 큰 값일 경우에 말이죠.

아직 느릴 수는 있습니다. 예를 들어 m이 500만이나 5000만 또는 그보다 더 크다면 어떻게 될까요?

전체 훈련 세트에 경사하강법을 구현하면, 전체 훈련 세트를 약간의 기울기 하강에 대한 첫 단계를 시작하기 전에 모든 것을 처리해야 합니다. 그런 다음 경사 하강의 또 다른 작은 단계를 수행하기 전에 5백만 개의 훈련 샘플의 전체 훈련 세트를 다시 수행해야합니다.

따라서 5백만개의 예제로 구성된 거대한 훈련 세트 전체의 처리를 마치기도 전에 경사 하강법이 약간의 진전을 이루도록 하면 더 따른 알고리즘을 얻을 수 있다는 것이 밝혀졌습니다. 특히 할 수 있는 것은 다음과 같습니다.

훈련 세트를 더 작게 나눈다고 가정해 보겠는데요. 이런 아주 작은 훈련 세트로 나눠서 아기 트레이닝 세트를 미니 배치들이라고 부르겠습니다. 그러고 난 뒤 이런 아기 트레이닝 세트가 각각 천 개의 예시를 가지고 있다고 해보겠습니다.

그러면 $x^{(1)}$에서 $x^{(1000)}$까지 있고 이것을 첫번째 아기 트레이닝 세트라고 하겠는데요. 또는 미니 배치라고도 하죠. 그 다음에 또 추가로 1000개의 예시를 갖습니다. $x^{(1001)}$에서 $x^{(2000)}$까지 1000개의 예시를 갖고 그렇게 다음도 이어서 올 수 있겠죠.

이제 새로운 표기법을 보여드릴 텐데요. 이것을 중괄호가 있는 $X^{ \{ 1 \} }$라고 부를 것이고 그 다음은 $X^{ \{ 2 \} }$라고 부를 것입니다.

자 그럼 여기서 총 5백만개의 트레이닝 샘플이 있고 여기 각각의 미니 배치들이 1000개의 예시가 있으면 여기 이것은 5000개가 있다는 것인데요. 왜냐하면 5000 곱하기 1000이 5백만개이기 때문입니다. 이 미니 배치를 모두 합치면 5000개가 됩니다.

따라서 $X{ \{5000\} }$으로 끝난 다음 마찬가지로 Y에 대해서도 동일한 작업을 수행합니다. Y에 대한 트레이닝 데이터도 적합하게 나눠줍니다. 이제 미니 배치 번호 t는 $X^{ \{t\} }$로 구성될 예정이며 Y는 $Y^{ \{t\} }$입니다. 그리고 이것은 입력 값과 결과값 쌍으로 이루어진 1000개의 트레이닝 샘플입니다.

다음으로 넘어가기 전에 여기 표기법이 명백하도로 우리는 이전에 위 첨자 둥근 괄호 i를 사용하여 훈련 세트에서 색인을 생성하므로 $x^{(i)}$의 경우 i번째 훈련 예시입니다. 그리고 위첨자 대괄호 L를 이용해서 신경망의 여러 레이어에 대한 색인을 생성합니다. 그러므로 $z^{[l]}$은 z값에서 신경망의 L레이어에 대해 여기에서 중괄호 t를 도입하여 다양한 미니 배치에 색인을 생성합니다.

그러면 $X^t$와 $Y^t$가 있는데요. 여러분이 이것들에 대한 이해도를 체크하기 위해서 $X^t$와 $Y^t$의 치수는 무엇인가요?

X는 $n_x \times m$입니다. 그러므로 만약 $X^1$이 1000개의 트레이닝 예시이거나 1000개의 예시에 대한 값인 경우 여기 차원은 $n_x \times 1000$이어야 하며 치수는 $1 \times 1000$이어야 합니다.

여기 알고리즘의 이름을 설명드리자면, 배치 기울기 하강은 전체 트레이닝 세트를 한번에 처리하는 경우를 뜻합니다. 그리고 그이름은 훈련 샘플의 전체 배치를 동시에 처리하는 것으로 보아서 유래되었습니다. 훌륭한 이름이라고 보긴 힘들지만 단순히 그냥 이렇게 불립니다.

반대로 미니 배치 기울기 하강은 다음 그림에서 이야기할 알고리즘을 뜻하는데요. 그리고 처리하는 것은 단일 미니 $X^t, Y^t$인데요. 전체 훈련 세트 X, Y 동시에 처리하는 대신말이죠.

그럼 미니 배치 경사하강법이 어떻게 작동하는지 봅시다.

![image](https://user-images.githubusercontent.com/55765292/178169583-b78d6f99-e07d-41db-9f11-e4cab0c12921.png)

훈련 세트에서 미니 배치 경사 하강법을 실행하려면 각각 1,000개만큼 5,000개의 미니 배치가 있기 때문에 t는 1부터 5000까지 실행합니다. for loop 내부에서는 무엇을 할 것이냐면, $X^t, Y^t$를 이용해서 기울기 하강의 한 단계를 도입할 것입니다.

이것은 마치 트레이닝 세트의 크기가 1000개의 예시가 있는 것과 비슷합니다. 그리고 이미 익숙한 알고리즘을 도입하는 것과 비슷하며 여기 작은 트레이닝 세트인 m값이 1000인 경우에서 말이죠. 별개의 for loop를 1000개의 예시에 갖게 하는 것이 아닌 벡터화를 통해 1000개의 예시를 한번에 처리하는 것과 같습니다.

우선 첫째로 입력 값에 대한 순전파를 도입합니다. $X^t$의 값에 말이죠. 그럼 $Z^{[1]} = W^{(1)}X^t + b^{[1]}$이 되게하여 도입합니다. 이전에는 X였지만 이제 전체 훈련 세트를 처리하는 것이 아니라 첫번째 미니 배치만 처리하는 것입니다. 그렇기 때문에 $X^t$가 되죠. 미니 배치 t를 처리하는 경우에 말이죠.

그럼 $A^{[1]} = g^{[1]}(Z^{[1]})$부터 $A^{[L]} = g^{[L]}(Z^{[L]})$까지 진행됩니다. 그리고 이 값이 예측 값이 되는 것입니다. 여기서 아시겠지만 벡터화 도입을 이용하셔야합니다. 여기서 벡터화 도입이 5백만개의 예시 대신에 한번에 1000개의 예시를 처리하기 때문입니다.

다음으로는 J 비용함수를 산출합니다. 여기서는 이 함수를 위의 그림처럼 나탑니다. m은 1000이기 때문에 1/1000을 앞에 곱해줍니다. 정규화를 사용하는 경우 정규화 항을 가질 수도 있습니다. 모든 X, Y에서 수행하는 것을 $X^t, Y^t$에서 수행한다는 것을 제외하면 이전에 경사하강법을 구현할 때와 정확히 동일합니다.

다음으로 역전파를 구현합니다. $J^t$에 대한 기울기 산출하기 위해서 말이죠. 여전히 $X^t, Y^t$만을 사용하고 있으며 가중치 W와 b에 대해서 업데이트합니다. 이것은 미니 배치 경사 하강법을 사용하여 훈련 세트를 한 번 통과하는 것입니다.

여기 그림에 나타난 코드는 1 epoch 훈련을 한다고 표현하기도 합니다. epoch이라는 용어는 훈련 세트로 1번 통과하다는 뜻입니다. 반면 배치 경사 하강법에서는 한번 훈련을 통과하는 것이 오로지 한번의 기울기 하강 절차를 밟게 해줍니다. 미니 배치 기울기 하강을 통해 훈련 세트로 한번의 통과를 통해 즉 1 epoch이죠. 5000 단계의 기울기 하강을 진행하게 해줍니다.

지금은 물론 평소에 원하는 훈련 세트를 여러번 통과하고 싶고 또 하나의 for loop를 갖거나 while loop를 갖어야 할 것입니다. 그리하여 바라건대 수렴하거나 적어도 대략적으로 수렴될 때까지 훈련 세트를 계속 통과합니다. 큰 사이즈의 훈련 세트가 있는 경우 미니 배치 경사 하강법은 배치 경사 하강법보다 훨씬 더 빠르게 실행되며, 이는 딥 러닝의 모든 사람들이 대규모 데이터 세트를 훈련할 때 거의 사용하게 될 것입니다.

다음에는 미니 배치 경사 하강법에 대해 더 깊이 파고들어 그것이 하는 일과 왜 그렇게 잘 작동하는지 더 잘 이해할 수 있도록합시다.
