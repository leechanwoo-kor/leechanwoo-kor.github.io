---
title: "[Ⅴ. Sequence Models] Sequence Models & Attention Mechanism (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Sequence Models & Attention Mechanism (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Sequence Models & Attention Mechanism

## Sequence to Sequence Models

### Attention Model Intuition

대부분 기계 번역을 위해 인코더-디코더 아키텍처를 사용하고 있습니다. 하나의 RNN은 문장을 읽고 다른 하나는 문장을 출력합니다. 어텐션 모델이라 불리는 수정사항이 있습니다. 모든 것을 훨씬 더 잘 수행할 수 있게 만들죠. 어텐션 알고리즘, 어텐션 아이디어는 딥 러닝에서 가장 영향력 있는 아이디어 중 하나입니다. 어떻게 작동하는지 보겠습니다.

#### The Problem of Long Sequences

![image](https://user-images.githubusercontent.com/55765292/193438602-cbe4b1c3-34ec-40b8-99fe-fa99c5604685.png)

이렇게 긴 프랑스어 문장이 있습니다. 네트워크 안의 초록색 인코더에게 전체 문장을 읽고 저장한 다음에 여기에 전달된 활성화에 저장하라고 요청합니다. 그러면 보라색 네트워크인 디코더 네트워크는 영어 번역을 생성합니다.

> Jane went to Africa last September, and enjoyed the culture and met many wonderful people; she came back raving about
how wonderful her trip was, and is tempting me to go too.

인간 번역가가 이 문장을 번역하는 방법은 먼저 프랑스어 문장 전체를 읽고 외워서 아무 준비없이 영어 문장을 뱉어내는 것이 아닙니다. 인간 번역자가 하는 일은 첫 부분을 읽고 번역의 일부를 만들어 내는 겁니다. 두 번째 부분을 보고 몇 단어를 더 생성하고, 몇 단어를 더 살펴보고, 몇 단어를 더 생성하죠. 문장 전체를 부분적으로 작업하는 겁니다. 이렇게 긴 문장을 외우기는 정말 어렵기 때문입니다.

위의 인코더-디코더 아키텍처에서 볼 수 있는 것은 짧은 문장에서 꽤 잘 작동하기 때문에 상대적으로 BLEU 점수가 높게 나올 수도 있지만 아주 긴 문장인 경우, 30~40단어 이상이라면 성능이 저하됩니다. 그래서 BLEU 점수는 문장 길이에 따라 이렇게 됩니다. 짧은 문장들은 번역하기 어렵고 모든 단어를 이해하기 힘듭니다. 그렇죠? 긴 문장의 경우 신경망이 외우기 어렵기 때문에 점수가 좋지 않습니다.

이제 사람과 약간 더 비슷하게 번역하는 **어텐션 모델attention model**을 보실 겁니다. 한 번에 문장의 일부만 보는 어텐션 모델을 사용하면 기계 번역 시스템 성능이 이렇게 보일 수 있습니다. 한 번에 문장의 한 부분만 작업하면 신경망이 긴 문장을 기억하는 능력 측정의 이러한 커다란 하락을 볼 수 없습니다. 이런 능력은 신경망에게 가장 필요한 것이 아닐 수도 있습니다.

#### Attention Model Intuition

![image](https://user-images.githubusercontent.com/55765292/193438654-3a5575f4-ad0d-4787-b309-7833dae1f7c8.png)

**어텐션 모델**은 기계 번역 용도로 개발되었지만 이것은 다른 여러 애플리케이션 분야에도 적용되었습니다. 이건 정말 영향력이 큽니다. 이것을 딥러닝 논문 중 가장 중요한 논문이라고 생각합니다. 짧은 문장으로 이것을 설명해겠습니다. 비록 이 아이디어들이 더 긴 문장을 위해 발전된 것이라도, 더 간단한 예로 이 아이디어들을 설명하는 것이 더 쉬울 것입니다. 우리가 늘 쓰는 문장이 있습니다.

> Jane visite l'Afrique en Septembre.

RNN을 사용한다고 가정합시다. 이 경우 양방향 RNN을 사용하여 각각의 입력 단어에 대한 몇 가지 특징을 계산하기 위해서 $\hat{y}^{<1>}, \hat{y}^{<2>}, \hat{y}^{<3>}$에 이어 $\hat{y}^{<5>}$까지 출력하는 양방향 RNN을 알아야 하며 직역을 하지는 않기에 위에 있는 $y$를 없애겠습니다.

하지만 양방향 RNN을 사용해서 우리가 한 일은 각각의 단어에 대해서, 실제 문장 내 각각의 다섯 개의 위치에서 문장의 단어와 모든 위치의 주변 단어에 대한 매우 풍부한 기능 집합을 계산할 수 있습니다.

이제 계속해서 영어 번역을 만들어 봅시다. 다른 RNN을 사용해서 영어 번역을 만들겠습니다. 여기 RNN 노드를 평소처럼 그리고 아래 활성화와 혼동을 피하기 위해 a를 사용하여 활성화를 나타내는 대신 다른 표기법을 사용할 겁니다. s를 사용하여 여기 위쪽 RNN의 은닉 상태를 표시하려고 합니다. $a^{<1>}$을 쓰는 대신 $s^{<1>}$을 쓰고 이 모델이 만드는 첫 단어가 "Jane"이길 바랍니다.

> Jane visits Africa in September

를 만들기 위해서죠. 여기서 질문은 이 첫 단어를 만들려고 할 때, 프랑스어 문장의 어떤 부분을 봐야 하는지입니다. 이 첫 단어를 가장 먼저 살펴봐야 할 것 같습니다. 아니면 가까운 단어 몇 개를 더 찾아봐야 할 것 같지만 문장 끝을 볼 필요는 없습니다.

어텐션 모델은 일련의 어텐션 가중치의 계산이며 첫 단어를 만들 때 표시할 $\alpha^{<1,1>}$을 사용하며 첫 번째 단어를 생성할 때 여기에서 이 첫 번째 정보에 얼마나 주의를 기울여야 하는지를 나타냅니다.

그 다음 두 번째는 어텐션 가중치라고 불리는 것으로, $\alpha^{<1,2>}$는 첫 단어를 계산하고 있다는 것과 입력에 대한 두 번째 단어에 얼마나 주의를 기울여야 하는지 알려주며 $\alpha^{<1,3>}$로 이어지고 이것들이 모여서 주의해야할 정확한 맥락이 무엇인지 알려주며 c로 표기합니다. 그리고 RNN 유닛에에 입력되어 첫 번째 단어 생성을 시도합니다. 이것이 RNN의 한 단계이고, 다음에 이 모든 세부사항을 구체화하겠습니다.

이 RNN의 두 번째 단계에서는 새로운 은닉 상태 $s^{<2>}$와 새로운 어텐션 가중치들도 갖게 될 것입니다. $\alpha^{<2,1>}$은 두 번째 단어를 생성할 때, 제 생각에는 "visits"가 될 것 같습니다. 프랑스어 입력의 첫 번째 단어에 얼마나 주의를 기울여야 할지 말해줍니다.

이어서 $\alpha^{<2,2>}$ 등도 마찬가지죠. 이들은 "visite"라는 단어에는 얼마나, "l'Afrique"에는 얼마나 주의를 기울여야 할지를 알려줍니다 물론 생성한 첫 단어 "Jane"도 여기에 입력하며, 그런 다음 우리가 주목하고 있는 두 번째 단계의 몇 가지 맥락이 있고, 함께 입력하여 두 번째 단어를 생성합니다. 그리고 세 번째 단계인 $s^{<3>}$로 이어지게 됩니다.

이것을 입력하고 새로운 맥락 c도 있습니다. 이는 $\alpha^{<3,t>}$에 따라 결정되며 다른 프랑스어 입력 단어에 대해 얼마나 주목해야 하는지 말해줍니다. 아직 지정하지 않은 것들도 있지만 이 문맥들이 정확히 어떻게 정의되는지와 문맥의 목표가 세 번째 단어에 대한 것인지 포착하려면 문장의 이 부분을 살펴봐야 하는지, 이 내용은 다음에 좀 더 자세히 설명하겠습니다.

이러한 공식은 이러한 주의 가중치를 계산하는 방법과 함께 다음에 이어집니다. 다음에 $\alpha^{<3,t>}$를 보실 수 있으며 이것은 세 번째 단어를 만들려고 할 때 올바른 결과인 "Africa"가 될 거라고 생각합니다. 이 RNN 단계가 시간 $t$에서 프랑스 단어에 주목해야할 양은 시간 $t$에서 양방향 RNN의 활성화에 따라 달라지며, 저는 이를 시간 $t$의 순방향 활성화와 역방향 활성화에 따른 것이라 생각하며 이전 단계의 상태인 $s^{<2>}$에 따라 달라지고 이런 것들이 함께 프랑스어 문장의 특정 단어에 얼마나 주의를기울이는지에 영향을 미칩니다.

이어서 모든 세부 사항을 구체화하겠습니다. 하지만 가장 중요한 직관은 결국 EOS를 만들어 낼 때까지 한번에 한 단어를 만드는 RNN의 순방향 생성이며 매 단계마다 이런 어텐션 가중치가 있습니다. $\alpha^{< t,t'>}$은 영어 단어인 $t$를 만들려고 할 때 얼마나 많은 $t'$ 프랑스어 단어를 사용해야 하는지 알려줍니다.

그리고 이것은 특정 영어 단어를 만들 때 주의를 기울일 프랑스어 문장의 로컬 창 내에서만 볼 수 있도록 모든 시간 단계에서 허용합니다. 어텐션 모델에 대한 직관적인 느낌을 전달했길 바라며 이제 우리는 알고리즘이 어떻게 작동하는지 대충 알게 되었습니다. 이어서 어텐션 모델의 자세한 내용을 살펴보겠습니다.


### Attention Model

이전에 어텐션 모델이 번역을 생성하는 동안 신경망이 인간 번역가와 상당히 유사하게 오직 입력 문장의 일부에만 집중할 수 있도록 만들 수 있는 방법을 알아보았습니다. 이제 그 직관력을 어텐션 모델을 구현하는 정확한 세부 사항으로 형식화해보겠습니다.

#### Attention Model

![image](https://user-images.githubusercontent.com/55765292/193439220-f9124c6d-8457-4abd-a90a-577ca1478c06.png)

이전과 마찬가지로 입력 문장이 있다고 가정하고 양방향 RNN, 양방향 GRU, 또는 양방향 LSTM을 사용하여 모든 단어의 특징을 계산한다고 해봅시다. 실제로 GRU와 LSTM은 종종 이것을 위해 사용되는데, LSTM이 좀 더 일반적일 수도 있습니다.

따라서 순방향 순환의 경우, 첫 시간 단계의 순방향 순환 $a$가 있고 첫 시간 단계의 역방향 순환 활성화입니다. 두 번째 시간 단계의 순방향 순환 활성화이고 역방향 활성화 등으로 이어집니다. 전부 합하면 순방향 다섯 시간 단계, 역방향 다섯 시간 단계를 밟게 됩니다.

여기 $a^{<0>}$이 있고 엄밀히 말해서 추측하기로는 영벡터인 여섯 번째 시간 단계의 역방향 $a$가 있을겁니다. 그런 다음 모든 시간 단계의 순방향 표기를 단순화하기 위해 양방향 RNN에서 순방향 순환과 역방향 순환을 계산한 특징들이 있지만 이 두 가지를 모두 표현하기 위해 $a^{< t>}$를 사용하겠습니다. 따라서 $a^{< t>}$는 시간 $t$의 특징 벡터가 될 것입니다.

표기법과 일치하지만, 두 번째를 사용하고 있으므로 저는 이것을 $t'$라고 부르겠습니다 $t'$을 사용하여 프랑스 문장의 단어들을 나타낼 겁니다. 다음으로 이는 순방향만 있는, 번역을 생성하기 위한 s 상태의 단방향 RNN입니다.

첫 번째 시간 단계에서 $y^{<1>}$을 생성하고 맥락 c를 입력하게 됩니다. 시간을 나타내고 싶으면 $c^{<1>}$을 쓸 수 있지만 가끔은 첨자 없이 c만 씁니다. 그리고 이는 어텐션 매개 변수 $\alpha^{<1,1>}, \alpha^{<1,2>}$ 등에 따라 결정되며 어느 정도의 어텐션을 갖는지 알려줍니다.

이러한 $\alpha$ 매개 변수는 각각 다른 시간 단계에서 얻는 특징이나 활성화에 따라 맥락이 얼마나 달라지는지 알려줍니다. 따라서 우리가 이 맥락을 정의하는 방법은 각각 다른 시간의 어텐션 가중치로 계산된 특징을 합하는 것입니다.

따라서 보다 형식적으로 어텐션 가중치는 음수가 아니기에 양수가 되고 총합은 1입니다. 나중에 이것이 사실인지 확인하는 방법을 알게 될 겁니다. 그리고 시간 1에서의 맥락은 종종 위첨자를 생략하고, 모든 시간 $t'$에서 대해서 이러한 활성화의 총합으로 표현됩니다.

따라서 이 용어는 어텐션 가중치이며 이 용어는 여기서 왔습니다. 따라서 $\alpha^{< t,t'>}$은 $y^{< t>}$가 $a^{< t'>}$에 대해서 기울이는 어텐션 양입니다.

즉, 다시 말해서 시간 $t$의 출력 단어를 생성할 때 시간 $t'$의 입력 단어에 얼마나 주의를 기울여야 하는지입니다. 이것이 출력을 생성하는 한 단계이고 그 다음 단계에서 두 번째 출력을 생성하고 다시 한 번 새로운 어텐션 가중치의 세트를 모두 합하여 새로운 맥락이 생성됩니다.

이것도 역시 입력으로 두 번째 단어를 생성할 수 있습니다. 이제 이 가중치 총합이 두 번째 시간 단계의 맥락이 됩니다. 시간 $t'$에 대한 $\alpha^{<2,t'>} * $a^{< t'>}$의 총합이죠.

그럼 이 맥락 벡터를 사용해 봅시다. $c^{<1>}$이 바로 이전에 있고, 이어서 $c^{<2>}$ 등으로 이어집니다. 여기 위의 이 네트워크는 맥락 벡터가 출력되는 표준 RNN 시퀀스로 보이며 한 번에 한 단어씩 번역을 생성할 수 있습니다.

우리는 이러한 어텐션 가중치와 입력 문장의 특징들을 사용해 맥락 벡터를 계산하는 방법도 정의했습니다. 따라서 남은 유일한 것은 이 어텐션 가중치를 어떻게 계산할지 정의하는 것입니다.

#### Computing Attention $\alpha^{< t,t'>}$

![image](https://user-images.githubusercontent.com/55765292/193439231-955963b1-905f-4a8f-8103-f0530f7128e0.png)

요약하면 $\alpha^{< t,t'>}$은 출력 번역의 $t$번째 단어를 생성하려고 할 때 $\alpha^{< t'>}|$에 기울여야 하는 어텐션의 양입니다. 그럼 공식을 한번 작성해서 어떻게 작동하는지 보겠습니다.

이는 $\alpha^{< t,t'>}$을 계산하는 공식으로 $e^{< t,t'>}$ 항으로 계산하게 되며 $t'$에 대한 이러한 가중치의 총합이 1이 되기 위해서 기본적으로 소프트맥스를 사용합니다. 따라서 고정된 모든 $t$값에 대해 $t'$에 대한 이 값의 총합은 1이 됩니다. 그리고 이 소프트맥스 우선 순위를 사용하여 이 총합이 1이 되도록 할 수 있습니다.

이제 어떻게 이런 $e$를 계산할까요? 한 가지 방법은 다음과 같은 소규모의 신경망을 사용하는 것입니다. 즉, $s^{< t-1>$}은 이전 시간 단계의 신경망 상태였습니다. 이것이 우리가 가진 네트워크입니다. 만일 $y^{< t>}$를 생성하려 한다면 $s^{< t-1>}$은 이전 단계의 은닉 상태로 $s^{< t>}$로 넘어가며 소규모 신경망에 대한 하나의 입력이 됩니다. 보통은 은닉층 하나의 신경망으로 이것들을 많이 계산해야 하기 때문입니다.

그 다음은 시간 $t'$의 특징인 $α^{< t'>}$이 다른 입력입니다. 그리고 직관은, 만약 $t'$의 활성화에 얼마나 주의를 기울일 것인지 결정하고 싶다면 가장 많이 의존해야 할 것처럼 보이는 것은 이전 시간 단계의 은닉 상태의 활성화입니다. 여기로 입력되는 맥락 피드 때문에 아직 현재 상태의 활성화가 없어서 계산하지 못합니다.

그러나 상위 번역을 생성하는 이 RNN의 숨겨진 단계를 살펴본 다음 각 위치에 대해, 각 단어에 대해서 특징을 봅니다. 따라서 $\alpha^{< t,t'>}$와 $e^{< t,t'>}$가 이 두 가지 수량에 의존하는 것은 매우 자연스러워 보입니다.

하지만 우리는 그 함수가 무엇인지 모릅니다 이러한 경우 소규모의 신경망을 훈련해서 이 함수가 무엇이든지 학습하도록 만들 수 있습니다. 그리고 올바른 함수를 학습하기 위해 역전파와 경사 하강법을 믿어야 합니다. 이 모든 모델을 구현하고 경사 하강법을 통해 훈련한다면, 모든 것이 실제로 작동합니다.

이 작은 신경망은 $y^{< t>}$가 $\alpha^{< t'>}$에 얼마나 많은 주의를 기울여야 하는지를 알려주는 꽤 괜찮은 일을 하며 이 공식은 어텐션 가중치 합이 1이 되도록 한 다음 느리지만 꾸준하게 한 번에 한 단어를 생성하도록 합니다. 이 신경망은 경사 하강법을 사용하여 이 모든 것을 자동으로 학습하는 입력 문장의 오른쪽 부분에 주의를 기울입니다.

이 알고리즘의 한 가지 단점은 이 알고리즘을 작동시키려면 2차의 시간이나 비용이 필요하다는 겁니다. 입력에 단어 $T_x$와 출력에 단어 $T_y$가 있다면 이러한 어텐션 매개 변수의 총 개수는 $T_x * T_y$가 됩니다.

따라서 이 알고리즘은 2차 비용으로 실행됩니다. 입력이나 출력 문장이 없는 기계 번역 애플리케이션이더라도 일반적으로 긴 2차 비용을 사용할 수 있습니다. 하지만, 비용을 절감하기 위한 연구도 있습니다.

지금까지 기계 번역의 맥락에서 어텐션의 아이디어를 설명했습니다. 자세히 다루지 않아도 이 아이디어는 다른 문제에도 적용되었습니다. 그럼 이미지 캡처를 볼까요. 이미지 캡처 문제에서 할 일은 그림을 보고 그림 캡션을 작성해야 합니다.

매우 유사한 아키텍처 또한 가질 수 있음을 보여주었습니다. 사진을 보고 사진 캡션을 작성하는 동안 한 번에 그림의 한 부분에만 주의를 기울입니다. 관심이 있으신 분은 그 논문을 보시길 권합니다.

#### Attention Example

![image](https://user-images.githubusercontent.com/55765292/193439238-d6ea5ec0-2b3e-4ba5-9c26-4c79f5a315dd.png)

기계 번역은 매우 복잡한 문제였지만 날짜 정규화 문제에 대해서는 스스로 직접 설정하고 주의를 기울여야 합니다. 그래서 날짜를 입력하는 문제는 이와 같습니다.

> July 20th 1969 → 1969-07-20

이것은 실제 아폴로호 달 착륙 날짜이며 이렇게 표준화된 형식이나 날짜 같은 것으로 바꾸고 신경망으로 이를 이 형식으로 정규화합니다.

> 23 April, 1564 → 1564-04-23

여담이지만 이건 윌리엄 셰익스피어의 생일입니다. 그렇다고 여겨지는 날짜죠 이전의 연습 문제에서 볼 수 있는 것은 신경망으로 이러한 형식의 날짜를 입력하고 주의 모델을 사용하여 이 날짜에 대해 정규화된 형식을 만들도록 하는 것입니다.

또 다른 할 재미있는 일은 어텐션 가중치의 시각화를 보는 것입니다. 여기 기계 번역 예가 있으며 여러 색상으로 그려졌습니다. 서로 다른 어텐션 가중치의 크기입니다. 이것에 너무 많은 시간을 쓰고 싶진 않지만 어텐션 가중치가 높은 경향이 있는 적절한 입력과 출력 단어들을 발견할 수 있습니다.

따라서 출력에서 특정 단어를 생성할 때 일반적으로 입력의 올바른 단어에 주의를 기울이고 어텐션 모델과 함께 역전파를 사용하여 학습했을 때 주의를 기울여야 할 위치를 배웁니다. 이것이 바로 어텐션 모델로 딥 러닝의 가장 강력한 아이디어 중 하나입니다.
