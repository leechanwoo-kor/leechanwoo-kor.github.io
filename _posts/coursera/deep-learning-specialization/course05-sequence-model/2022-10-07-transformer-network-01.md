---
title: "[Ⅴ. Sequence Models] Transformer Network (1)"
categories:
  - Deep Learning Specialization
  - Transformer Network
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Attention Models
  - Transformer Network
  - Transformers
  - Self Attention
  - Multi Head Attention
toc: true
toc_sticky: true
toc_label: "Transformer Network (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Transformer Network

## Transformers

### Transformer Network Intuition

딥 러닝에서 가장 흥미로운 발전 중 하나는 **트랜스포머 네트워크transformer network**입니다. 이는 NLP 세계를 완전히 장악한 아키텍쳐이죠. 그리고 오늘날 NLP에 대한 가장 효과적인 많은 알고리즘들은 트랜스포머 아키텍처를 기반으로 하죠.

이것은 상대적으로 복잡한 신경망 아키텍처이지만 조금씩 살펴볼 것입니다. 그래서 끝날 무렵 트랜스포머 네트워크 작동 방법에 대해 더 잘 알게 되고 문제없이 적용하게 될 것입니다.

<br/>

![image](https://user-images.githubusercontent.com/55765292/194481705-56cccaed-b737-4cbb-b142-4239d37ae40a.png)

시퀀스 작업의 복잡도가 증가함에 따라 모델의 복잡성도 증가합니다. **RNN**에서 **기울기 소멸vanishing gradients** 문제가 있어 **장거리 종속성long range dependencies**과 **시퀀스sequence**를 캡처하는데 어려움이 있다는 것을 발견했습니다.

그런 다음 **GRU**를, 그리고 **LSTM** 모델을 정보 플로우를 제어하기 위해 게이트를 사용하는 많은 문제들을 해결하는 방법으로 살펴보았습니다. 그래서 이들 각각의 단위는 몇 가지 더 많은 계산을 했죠.

이러한 직관들은 정보 플로우에 대한 제어를 개선시켰지만 복잡도 또한 증가시켰습니다. 그래서 RNN에서 GRU 그리고 LSTM으로 이동하면서 모델들은 더 복잡해 졌습니다.

그리고 이 모든 모델들은 여전히 **순차적sequential** 모델입니다. input을, input 문장을 한 번에 한 단어나 토큰을 소화했다는 점에서 말이죠. 그리고 각 단위가 마치 정보 플로우에 병목인 것 같았습니다. 왜냐하면, 예를 들어서 이 마지막 단위의 output을 계산하기 위해 먼저 이전에 오는 모든 단위의 output을 계산해야 하기 때문이죠.

트랜스포머 아키텍처를 배우고나면 전체 시퀀스에 대해 더 많은 계산을 병렬로 실행할 수 있게 합니다. 그래서 왼쪽에서 오른쪽으로 한 번에 한 단어씩 처리하는 대신 전체 문장을 동시에 소화할 수 있죠.

<br/>

트랜스포머 네트워크는 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser 그리고 Illia Polosukhin가 작성한 논문 Attention Is All You Need로 발표되었습니다. 

- Attention + CNN

트랜스포머 아키텍처의 주요 혁신은 표현representation를 기반으로 한 어텐션attention의 사용과 CNN, 컨볼루션 신경망 스타일의 결합입니다.

![image](https://user-images.githubusercontent.com/55765292/194481771-85d5f09e-7464-489e-abd1-224e5feba169.png)

그래서 RNN은 한 번에 하나의 output을 처리하는데요. 그래서 $y^{<0>}$가 $y^{<1>}$을 계산하는데 제공되고 그리고 이것은 $y^{<2>}$를 계산하는데 사용되죠. 이는 토큰을 처리하는데 매우 순차적인 방법입니다. 그리고 이것을 CNN이나 많은 픽셀 단어들을 input 할 수 있는 신뢰도와 대조할 수 있습니다. 그리고 그것들에 대한 표현들을 병렬로 계산할 수 있습니다.

그래서 어텐션 네트워크에서 볼 수 있는 것은 매우 풍부하고 유용한 단어 표현을 계산하는 방법입니다. 하지만 CNN의 병렬 처리 스타일과 더 유사한 것으로 말이죠. 어텐션 네트워크를 이해하기 위해서 다음 두 개의 주요 아이디어가 있을 것입니다.

첫 번째는 **셀프 어텐션self attention**입니다. 셀프 어텐션의 목적은, 예를 들어, 만약 $A^{<1>}, A^{<2>}, A^{<3>}, A^{<4>}$ 그리고 $A^{<5>}$ 라는 5개의 단어로 된 문장이 있다면, 이 5개의 단어에 대한 5개의 표현을 계산하게 되는 것입니다. 그리고 이는 문장의 모든 단어들에 대한 표현을 병렬로 계산하는 방법을 기반으로 하는 어텐션이 될 것입니다.

그런 다음 **멀티 헤드 어텐션multi head attention**은 셀프 어텐션 처리의 루프에 대한 기본 A입니다. 따라서 이러한 표현들의 여러 버전이 나오게 되죠. 그리고 이러한 표현들은 매우 풍부한 표현들이 될 것이며 유효성을 위한 기계 번역이나 다른 NLP에도 사용될 수 있습니다.

이어서 이 풍부한 표현들을 계산하기 위해 셀프 어텐션을 보겠습니다. 그 후에, 멀티 헤드 어텐션에 대해 이야기 할 것입니다. 그리고 트랜스포머 네트워크에 대한 모든 것을 종합하여 전체 트랜스포머 아키텍처가 최종적으로 어떻게 작동하는지 이해할 수 있을 것입니다

<br/>

### Self-Attention

트랜스포머의 **셀프 어텐션self-attention** 메커니즘에 대해 얘기해 보죠. 만약 이 주요 아이디어를 얻을 수 있다면 무엇이 트랜스포머 네트워크를 작동시키는가에 대한 가장 중요한 핵심 아이디어를 이해하시게 될 겁니다. 도움이 되는 도구입니다. 함께 시작해봅시다.

<br/>

![image](https://user-images.githubusercontent.com/55765292/194708489-57a0c44b-d834-44ed-abd9-73274a8a8f95.png)

어텐션이 RNN과 같은 순차적인 신경망에서 어떻게 사용되는지 살펴보았습니다. CNN과 같은 스타일로 어텐션을 사용하기 위해 input 문장의 각 단어에 대한 어텐션 기반 표현을 생성하는 셀프 어텐션을 계산해야 합니다. 그럼 예제를 실행해 보죠.

Jane, visite, l'Afrique, en, septembre 우리의 목표는 각 단어가 이렇게 생긴 어텐션 기반 묘사를 계산하는 것입니다. 그래서 결국엔 문장에 단어가 5 개이기 때문에 이것이 5개가 있을 것입니다. 그리고 이것들을 계산할 때 이 다섯 단어가 있는 다섯 표현을 $A^{<1>}$ ~ $A^{<5>}$로 부르겠습니다.

$q$, $K$, 그리고 $V$와 같은 많은 심볼이 보이는데요. 이에 대한 의미는 나중에 설명할 것이므로 지금은 너무 걱정하지 않아도 됩니다. 사용하려는 실행 예제로 이 문장에서 l'Afrique라는 단어를 가지고 트랜스포머 셀프 어텐션 메커니즘이 이 단어에 대한 $A^{<3>}$를 계산할 수 있게 해주는지 살펴보겠습니다. 그리고 문장의 다른 단어들도 똑같이 합니다.

📌 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 합니다. 그리고 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 합니다.{: .notice--info}

이전에 **워드 임베딩word embedding**이라는 것을 배웠는데요. 그래서 l'Afrique을 나타내는 하나의 방법은 l'Afrique에 대한 워드 임베딩을 찾아 보는 겁니다. 하지만 맥락에 따라, 우리는 l'Afrique나 Africa를 어떻게 생각하고 있나요? 역사적 장소로? 아니면 휴일의 목적지? 혹은 세계에서 두 번째로 큰 대륙? 여러분이 l'Afrique를 어떻게 생각하는가에 따라 다르게 표시하도록 선택할 수 있습니다.

그리고 그것이 이 표현 $A^{<3>}$가 할 일이죠. 주변의 단어를 보고 무슨 일이 일어나고 알아내려고 합니다. 이 문장에서 우리가 Africa에 대해 어떻게 얘기하고 있는지 말이죠. 그리고 이것에 대한 가장 적절한 표현을 찾습니다.

실제 계산의 관해서는 이전에 RNN의 맥락에서 적용했던 어텐션 메커니즘과 많이 다르지 않을 것입니다. 문장의 모든 다섯 단어에 대해 이 표현들을 병렬로 계산하는 것만 빼고는 말이죠. RNN 초기에 어텐션을 빌드할 때 우리는 이 방정식을 사용했었는데요.

- RNN Attention
> $a^{< t,t'>} = \dfrac{exp(e^{< t,t'>})}{\sum\nolimits^{T_x}_{t'=1}{exp(e^{< t,t'>})}}$

셀프 어텐션 메커니즘에서는 어텐션 방정식은 이렇게 생겼습니다.

- Transformers Attention
> $A(q,K,V)$ = $\sum_i{\dfrac{exp(e^{< q \cdot k^{< i>}>})}{\sum_j{exp(e^{< q \cdot k^{< j>}>})}}v^{< i>}}$

방정식의 유사성이 있는 것이 보입니다. 또한 여기 내부 용어는 소프트맥스를 포함합니다. 여기 왼쪽에 있는 용어처럼 말이죠. 그리고 지수 용어는 어텐션 값과 유사하다고 생각할 수 있습니다. 정확히 이런 용어들이 어떻게 작동되는지는 다음에 볼 수 있습니다. 그러니, 아직은 세부 사항에 대해 너무 걱정 마세요.

하지만 주요 차이점은 모든 단어가, l'Afrique를 예를 들어 쿼리, 키, 그리고 값이라는 세 개의 값이 있습니다. 이 벡터들은 각 단어에 대한 어텐션 값을 계산하기 위한 핵심 input 입니다. 이제, $A^{<3>}$를 계산하기 위한 필요한 단계로 바로 넘어가 보죠.

<br/>

![image](https://user-images.githubusercontent.com/55765292/194710506-ec6b5ce5-f27f-49a4-a02f-df99cfba20bf.png)

이 그림에서 단어 l'Afrique에서 셀프 어텐션 표현 $A^{<3>}$로 가기 위한 계산을 살펴보겠습니다. 참고로, 여기 오른쪽 맨 위에 소프트맥스 같은 이전 그림의 방정식을 표시했습니다.

먼저, 각 단어를 쿼리 키, 그리고 값이라 세 개의 값 쌍과 연관시킵니다. 만약 $x^{<3>}$가 l'Afrique에 대한 워드 임베딩이면 $q^{<3>}$가 계산되는 방법은 학습된 매트릭스입니다. 그래서 이는 $W^{Q}$ 곱하기 $x^{<3>}$이 됩니다. 그리고 키와 값 쌍도 유사하게 합니다.

그래서 $k^{<3>} = W^K \times x^{<3>}$, 그리고 $v^{<3>} = W^V \times x^{<3>}$. 그래서 이 매트릭스들, $W^Q, W^K$ 및 $W^V$는 이 러닝 알고리즘의 매개변수이며 이 각 단어에 대한 쿼리, 키, 그리고 값 벡터를 완성할 수 있게 해줍니다.

그래서 이 쿼리, 키, 그리고 값 벡터는 무엇을 해야 할까요? 그들은 개념과 쿼리 및 키-값 쌍을 가질 수 있는 데이터 베이스에 느슨한 비유를 사용하고 있었습니다. 만약 이러한 데이터베이스 유형에 익숙하다면 비유를 이해할 수 있겠지만 하지만 그 데이터베이스 개념에 익숙하지 않다면, 걱정하지 마세요. 이 벡터의 쿼리, 키, 그리고 값의 의도에 숨겨진 직관을 드려보도록 하죠.

q<3>는 l'Afrique에 대해 물어볼 수 있는 질문입니다. 그래서 이와 같은 질문을 나타내죠. "거기서 무슨 일이 일어나고 있는 거야?" Africa, l'Afrique는 목적지입니다. $A^{<3>}$를 계산할 때를 알고 싶은데요. 그럼 저기서는 무슨 일이 일어나고 있나요? 우리가 할 일은 $q^{<3>$ 및 $k^{<1>}$ 사이의, 즉 쿼리 3과 키 1의 내적을 계산하는 것입니다. 그리고 이는 Africa에서 무슨 일이 일어나고 있는가에 대한 질문에 답이 얼마나 좋은지 말해줍니다.

그러면 $q^{<3>}$과 $k^{<2>}$ 사이의 내적을 계산합니다. 그리고 이는 우리에게 Africa에서 무슨 일이 일어나는지에 대한 질문에 visite이 얼마나 좋은 답인지 말하고자 의도된 것입니다. 이 작업의 목적은 가장 유용한 표현 $A^{<3>}$의 계산을 도울 가장 중요한 정보를 위로 올리는 것입니다.

다시, 직관적인 구조를 위해 만약 $k^{<1>}$ 이 이 단어를, 사람을 나타낸다면 Jane은 사람이니까요. 그리고 $k^{<2>}$는 두 번째 단어인 visite을, 바로 행동이죠. 그러면 $q^{<2>}$와 $k^{<2>}$의 내적은 가장 큰 값을 가진 것을 볼 수 있고 이는 Africa에 무슨 일이 일어나고 있는지에 대한 질문에 visite이 가장 적절한 맥락을 제공한다고 하는 직관적인 예일 수 있습니다. 즉, 방문지로 보고 있다는 것이죠.

그래서 우리가 할 일은 이 다섯 값을 이 행으로 가져다 소프트맥스를 계산합니다. 그래서 여기 실제로 소프트맥스가 있습니다. 그리고 우리가 얘기하고 있는 예제에서 $q^{<3>} \times k^{<2>}$에 상응하는 단어 visite는 가장 큰 값이 있을 수 있습니다.

그러면 마지막으로, 이 소프트맥스 값을 가져다 $v^{<1>}$을 곱합니다. $v^{<1>}$는 단어 1, 2 등등을 말하죠. 그래서 이 값들은 이쪽 위의 값과 상응합니다. 마지막으로, 전부 더합니다. 이 합계는 합계 연산자와 상응하며 이들 모두를 더하면 $A^{<3>}$가 나옵니다. 바로 여기 이 값과 동일하죠. $A^{<3>}$을 쓰는 또 다른 방법은 여기 위에 있는 $A(q<3>, K, V)$, 하지만 때때로 $A^{<3>}$을 쓰는 것이 더 편리합니다.

이 표현의 핵심 장점은 l'Afrique라는 단어는 고정된 워드 임베딩이 아니라는 겁니다. 대신, 셀프 어텐션 메커니즘이 l'Afrique가 방문 목적지라는 것을 깨닫게 해주어 이 단어에 대한 더 풍부한 더 유용한 표현을 계산합니다.

지금까지 세 번째 단어 l'Afrique를 실행 예제로 써왔는데요 이 과정을 문장의 모든 다섯 단어들에 사용해 Jane, visite, l'Afrique en, septembre에 대한 유사하게 풍부한 표현을 얻을 수 있습니다. 이 모든 다섯 개의 계산을 합치면 문헌에 사용된 의미는 다음과 같습니다.

우리가 방금 어텐션(q, K, V)를, 이러한 모든 값들로 q, K, V 메트릭스를 쓰며 시퀀스의 모든 단어에 대해 얘기했던 이러한 계산을 요약할 수 있습니다. 그리고 이것은 그냥 여기 위의 이 방적식의 압축 또는 벡터화된 표현이죠. 분모의 용어는 내적의 크기를 조정해 폭주하지 않게 합니다. 그래서 걱정할 필요가 없도록 말이죠.

하지만 이런 유형의 어텐션에 대한 또 다른 이름은 크기가 조정된 내적 어텐션입니다. 그리고 이것은 Attention Is All You Need라는 오리지널 트랜스포머 아키텍처 논문에도 있는 표현입니다. 그래서 이것이 트랜스포머 네트워크의 셀프 어텐션 메커니즘입니다.

요약하면, 각 다섯 단어를 쿼리, 키, 그리고 값과 연결하는 것이죠. 쿼리는 Africa에서 무슨 일이 일어나는지와 같은 그 단어에 대한 질문을 받을 것입니다. 키는 다른 모든 단어들을 보고 쿼리와 유사하게 어느 단어가 가장 그 질문에 답변이 되는지 알아내도록 돕습니다.

이 경우에는, visite이 Africa서 일어나는 일이죠. 누군가가 Africa를 방문합니다. 

그리고 마지막으로, 값은 표현을 어떻게 visite이 $A^{<3>}$ 내에서 표현되어야 하는지, Africa의 표현 내에 연결할 수 있게 해줍니다. 그래서 이것으로 여기는 Africa이며 누군가가 Africa를 방문하고 있다라고 말을 하고 있는 Africa라는 단어에 대한 표현을 할 수 있죠.

이것은 단어에 대한 훨씬 더 미묘하고, 훨씬 더 풍부한 표현입니다. 어떤 단어가 그 단어의 왼쪽에 있는지 오른쪽에 있는지를 기반으로 적응을 하지 못하고 각 단어마다 동일한 고정된 워드 임베딩을 끌어 올리는 것 보다 말이죠. 우리는 모든 맥락을 고려해야 합니다.

이제, 셀프 어텐션 메커니즘을 배웠습니다. 이 모든 것에 큰 4 개의 루프를 넣으면 그것은 멀티 헤드 어텐션 메커니즘이 됩니다. 그것의 자세한 사항을 이어서 살펴보죠.

<br/>

### Multi-Head Attention

