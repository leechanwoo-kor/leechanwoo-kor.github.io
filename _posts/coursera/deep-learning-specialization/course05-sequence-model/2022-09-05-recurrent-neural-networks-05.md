---
title: "[Ⅴ. Sequence Models] Recurrent Neural Networks (5)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Recurrent Neural Networks (5)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Recurrent Neural Networks

## Recurrent Neural Networks

### Gated Recurrent Unit (GRU)

기본 RNN이 어떻게 작동하는지 보셨을 겁니다. 이번에는 **Gated Recurrent Unit, GRU**에 대해서 알아보겠습니다. 장거리 의존성을 더 잘 캡처하기 위해 그리고 기울기 소멸 문제를 해결하기 위해 RNN의 숨겨진 레이어를 수정합니다. 한 번 보죠.

#### RNN unit

![image](https://user-images.githubusercontent.com/55765292/188364179-fb2c659d-f4b2-4d86-ba4f-a96960ebb50e.png)

우리는 이미 RNN의 t시에서 활성화를 계산하는 공식 살펴보았습니다. 이는 매개변수 $W_a$에 적용되는 활성화 함수입니다. 이전 시간에 대한 활성화는 현재 input을 곱한 다음 바이어스를 더합니다.

그래서 이것을 그림으로 그리려고 합니다. 그림으로 그리려는 RNN 유닛을 박스로 그려보겠습니다.

input $a^{< t-1>}$과, 마지막 시간 단계에 대한 비활성화이죠, 그리고 input $x^{< t>}$ 이 둘이 만나서 몇몇 가중치 후에 그리고 이런 종류의 선형 계산 후에 g가 tanh 활성화 함수인 경우 tanh를 실행한 후에 활성화 $a$의 output을 계산합니다.

활성화 $a^< t>$의 output은 소프트맥스 유닛이나 $\hat{y}^{< t>}$를 출력하는 데 쓰이는 것으로 파싱될 수도 있습니다. 이것이 아마 RNN의 숨겨진 레이어의 RNN 유닛의 그림으로 시각화한 것일 겁니다. 이 그림을 보여드린 이유는 GRU를 설명할 때 비슷한 그림을 사용할 것이기 때문입니다.

#### GRU (simplified)

![image](https://user-images.githubusercontent.com/55765292/188365517-a32db67b-9330-48e6-8bef-422da6cec6ce.png)

우리가 이전에 봤던 이 문장을 참고해서 동기부여를 하려 하는데요.

> The cat, which already ate ..., was full.

이런 문장을 고려할 때 cat은 단수이기 때문에 were 대신 was가 들어간다는 것, 즉 cat was full 과 cats were full임을 확실히 이해하기 위해서 말이죠.

우리가 이 문장을 왼쪽에서 오른쪽으로 읽듯이 GRU 유닛은 $c$라는 새로운 변수를 갖게 될 것입니다. 이 변수는 셀에서 메모리 셀을 나타냅니다. 메모리 셀이 하는 일은 메모리를 제공하는 것입니다.

예를 들어, 메모리 셀은 기억할 수 있는 작은 기억을 제공합니다. 그래서 이 문장이 단수인지 복수인지 기억을 해 문장 끝에 가도 여전히 문장의 주어가 단수인지 복수인지를 고려할 수 있죠.

시간 T에서 메모리 셀은 t의 값 c를 가질 것입니다. 우리가 보게 될 것은 GRU 유닛이 실제로 t의 c와 동등한 t의 활성화 값 a를 출력하는 것입니다. 지금은 메모리 셀 값과 output 활성화 값을 나타내기 위해 다른 기호 c와 a를 사용하려 합니다. 비록 그 둘이 같기는 하지만요.

이 표기를 사용하는 이유는 조금 나중에 LSTM에 대해서 이야기 할 것이고 이것들은 두 개의 다른 값이 될 것이기 때문입니다. 하지만 지금으로써 GRU $c^{< t>}$는 output 활성화 $a^{< t>}$와 동일합니다.

그래서 이것들이 GRU 유닛의 계산을 좌우하는 방정식입니다. 매 시간 단계마다 메모리 셀을 값 $\tilde{c}$로 덮어쓰는 것을 고려하겠습니다. 이것은 c를 대체하기 위한 후보가 될 것입니다. 그리고 활성화 함수를 사용해 이것을 계산하겠습니다.

> $\tilde{c}=tanh(W_c[c^{< t-1>},x^{< t>}]+b_c)$

그래서 저게 매개변수 매트릭스 $W_c$이고요 그런 다음 매개변수 매트릭스 현재 입력값 $x^{< t>}$ 뿐만 아니라 메모리 셀, 활성화 값의 이전값을 전달한 다음 바이어스를 더합니다. $\tilde{c}^{< t>}$는 $c^{< t>}$의 대체 후보자가 됩니다.

GRU의 중요한 아이디어는 우리가 게이트를 가지게 될 것이라는 겁니다. 게이트를 $\Gamma_u$라고 부르겠습니다. 이것은 감마gamma를 뜻하는 그리스 알파벳 대문자입니다. u는 업데이트를 의미합니다. 이는 0과 1 사이의 값이 됩니다.

GRU 작동 방법에 대해 여러분의 직관을 위해서 $\Gamma_u$, 이 게이트 값이 언제나 0 혹은 1인 것으로 생각하세요. 실제로는 sigmoid 함수가 있는 여러분의 컴퓨터는 여기에 적용됩니다. 그래서 sigmoid 함수는 이렇게 생겼음을 값은 언제나 0과 1 사이라는 것을 기억하세요.

대부분의 가능한 input의 범위의 경우 Sigmoid 함수는 0과 매우 가깝거나 1과 매우 가깝습니다. 직관을 위해 대부분의 경우 감마를 0이나 1이라고 생각하세요.

제가 이것을 위해 알파벳 $\Gamma$를 선택한 이유는 게이트 펜스를 보면 이렇게 생겼기 때문입니다. 이 펜스에는 많은 $\Gamma$들이 있습니다. 그래서 $\Gamma_u$를 게이트를 나타낼 때 사용할 겁니다.

그러고 나서 GRU의 다음 핵심 부분은 이 방정식으로 우리는 $\tilde{c}$를 사용해 $c$를 업데이트하려고 하는 후보를 찾았죠. 그러면 게이트가 실제로 업데이트를 할지 여부를 결정합니다.

그래서 생각해본다면 이 메모리 셀 $c$는 단어의 보존 여부에 따라 0이나 1로 설정이 될 것입니다. 정말로 이 문장의 주제는 단수나 복수입니다.

왜냐하면 예를 들어 단수이면 이것을 1로 설정하고 복수이면 이것을 0으로 설정한다면 GRU 단위는 $c< t>$값을 여기 끝까지 암기할 것입니다. 여전히 1과 동일한 부분까지 말이죠. 그래서 저것으로 선택되었던 것은 단수임을 알 수 있습니다.

$\Gamma_u$ 게이트의 임무는 이 값을 언제 갱신할 것인지 결정하는 것입니다. 특히 The cat이라는 문장을 볼 때 여러분은 새로운 개념에 대해 얘기하고 있다는 것을 압니다. 문장의 주어는 cat이라는 것을 말이죠.

그래서 이때가 이 비트를 업데이트하기 좋은 시기입니다. 그리고 사용이 끝나면 이제 암기할 필요가 없으니 잊어버려도 되죠. GRU를 위해 우리가 사용할 특정한 방정식은 다음과 같습니다.

> $c^{< t>}=\Gamma_u \times \tilde{c}^{< t>} + (1-\Gamma_u) \times c^{< t-1>}$

$c^{< t>}$의 실제 값은 $\Gamma_u$ 곱하기 후보 값 1을 뺀 값과 이전 값 $c^{< t-1>}$을 곱합니다. 만약 게이트가 이 z의 업데이트 값이 1이라면 $c^{< t>}$의 새로운 값을 이 후보 값과 동일하게 설정하는 것에 주목하세요.

그래서 여기처럼 게이트를 1로 설정하고 가서 그 비트를 업데이트 합니다. 그러고 나서 이 가운데의 모든 값들의 경우 게이트는 0으로 합니다. 그래서 똑같이 하되 업데이트는 하지 않고 이전 값을 고수합니다.

왜냐하면 $\Gamma_u$가 0인 경우 $\Gamma_u \times \tilde{c}^{< t>}$는 0이 되고 $(1-\Gamma_u) \times c^{< t-1>}$은 1이 되어서 문장을 왼쪽에서 오른쪽으로 스캔해도 이전 값과 동일하게 설정합니다. 게이트가 0일 때는 업데이트 하지 말고 이전 값을 고수하고 그 값을 잊지마라고 하는 것입니다.

그렇게 하면 여기 밑까지 와도 $c^{< t>} = c^{< t-1>}$을 설정하고 있었는데도 cat이 단수였다는 것을 기억합니다.

GRU 유닛을 나타내는 그림도 그려보죠. 참고로 온라인 블로그 게시물 및 교과서와 튜토리얼을 살펴보면 이러한 유형의 그림은 나중에 살펴볼 LSTM 유닛 뿐만 아니라 GRU를 설명하는데 꽤 유명합니다.

개인적으로 그림에서 방정식을 이해하기가 더 쉽다고 생각하는데요. 그래도 그림이 이해가 가지 않아도 너무 걱정하지 마세요.

GRU 유닛의 input은 이전 시간 단계에 대한 $c^{< t-1>}$을 그리고 이는 $a^{< t-1>}$과 같으니 이것도 input으로 합니다. 그리고 $x^{< t>} 또한 input으로 합니다. 그러면 이 두 개가 함께 결합하고 적당한 기다림과 약간의 tanh으로 이것은 $\tilde{c}^{< t>$를 제공합니다. 바로 $c^{< t>}$의 대체 후보자이죠. 그리고 다른 매개변수 세트를 가지고 sigmoid 활성화 기능을 통해 $\Gamma_u$를 받아 게이트를 업데이트 하죠.

그리고 마지막으로 또 다른 작업을 통해 이 모든 것이 결합합니다. 공식을 적지는 않겠습니다만 이 보라색으로 색칠된 박스는 여기 아래에 있던 방정식을 나타냅니다. 그래서 저것이 이 보라색이 나타내는 것이며 게이트 값을 새로운 후보 값을 입력으로 취합니다. 저기는 다시 게이트 값이네요.

그리고 $c^{< t>}$에 대한 이전 값은 이것을 input으로 합니다, 또 다 합쳐서 메모리 셀에 대한 새로운 값을 생성합니다. 그것이 바로 $c^{< t>} = a$ 입니다.

그리고 이 임펄스를 소프트맥스나 $y^{< t>}$에 대한 예측을 만드는 것을 통해 사용할 수 있습니다. 그래서 바로 이것이 GRU 유닛입니다. 약간 단순화된 버전이죠.

정말 굉장한 것은 게이트를 통해 그것을 여러분이 문장을 왼쪽에서 오른쪽으로 스캔할 때 '업데이트하기 좋은 시간'이라며 결정한다는 것입니다. 그리고 들어가서 문장에서 여러분이 한참 전에 설정했던 이 메모리 셀을 정말로 사용해야되기 전까지는 변하지 말라고 하는 것입니다.

이제 게이트가 꽤 0으로 설정하기 쉽기 때문에 이 수량이 큰 음수에서 수적 반올림까지 가면 업데이트 게이트는 기본적으로 0이나 0과 가까워 집니다. 그럴 경우에 이 업데이트 방정식을 가지고 $c^{< t>}=c^{< t-1>}$을 설정합니다.

그래서 이것은 셀을 위한 값을 보존하는데 좋습니다. 감마는 0과 아주 가까울 수 있기 때문에 0.0000001이거나 그보다 더 작을 수 있습니다.

기울기 소멸 문제에는 별로 문제가 없습니다. $\Gamma$는 0과 매우 가깝기 때문에 이는 결국 $c^{< t>}=c^{< t-1>}$이 되고 $c^{< t>}$의 값은 거의 정확하게 유지되기 때문이죠 이것은 기울기 소멸 문제를 상당히 효과적으로 도와줄 수 있으며 여러분의 네트워크가 장거리 의존성까지도 학습하게 해줍니다.

예를 들어, cat과 was는 서로 떨어져 둘 사이에 수 많은 단어가 있어도 관련이 있죠.

이제, 이것의 구현 방법에 대해 자세히 살펴보려 하는데요. 쓰여진 방정식에서 $c^{< t>}$는 벡터가 될 수 있습니다. 만약 100 차원 숨겨진 활성화 값이 있다면 $c^{< t>}$는 100 차원이 될 수 있고 $c^{< t>}$ 또한 같은 차원일 것이고 $\Gamma_u$ 또한 제가 박스에 그리는 다른 것들과 똑같은 차원일 것입니다.

이런 경우에는 곱셉이 사실 요소별element-wise 곱셈입니다. 만약 $\Gamma_u$가 100 차원의 벡터라면 실제로는 비트의 100 차원 벡터입니다. 값은 대부분 0과 1로 업데이트 하려는 비트인 이 100차원 메모리 셀을 말해줍니다.

물론 실제로는 $\Gamma$는 정확히 0이나 1이 되지 않을 것이며 가끔 제가 중간의 값들을 말할 때도 있을 것입니다. 대부분 거의 정확히 0인 값이나 정확히 1인 값을 가지다고 생각하면 편하실 겁니다.

이러한 요소별 곱셈은 각 단계마다 업데이트할 메모리 셀 벡터의 차원인 GRU를 알려줍니다. 다른 비트를 업데이트 하는 동안 일부 정수 비트를 유지할 수 있죠.

예를 들어서 단수형이나 복수형의 cat을 기억하기 위해서 1 비트를 사용할 것이며 그리고 여러분이 음식을 얘기하고 있다는 것을 깨닫기 위해 몇몇 다른 비트를 사용할 것입니다. 우리가 먹는 거나 음식에
대해서 얘기했었기 때문이죠. 그런 다음 나중에 cat이 배가 부른지에 대해 얘기하죠.

서로 다른 비트를 사용해 모든 시점에서 비트의 하위 세트만 변경할 수 있습니다. 여러분은 이제 GRU의 가장 중요한 대부분의 아이디어를 이해하고 있습니다. 제가 이 그림에서 보여드렸던 것은 사실 약간 단순화된 GRU 유닛입니다. 전체 GRU 유닛을 설명해 보겠습니다.

#### Full GRU

![image](https://user-images.githubusercontent.com/55765292/188367965-17c71827-b541-4597-9544-c50bee96f253.png)

Full GRU 유닛의 경우 여기서 딱 한 가지의 변화를 줄 것인데요. 바로 메모리 셀에 대해 새로운 후보 값을 계산하는 첫 번째 방정식에서 말이죠. 딱 하나의 용어를 추가하겠습니다.

옆으로 조금 밀어 놓고 게이트를 하나 더 추가하겠습니다. 그래서 이것은 또다른 게이트 $\Gamma_r$입니다. r은 적절성을 뜻합니다. 이 게이트 $\Gamma_r$은 $c^{< t-1>}$이 $c^{< t>}$의 다음 후보를 계산하는데 얼마나 적절한지 알려줍니다. 이 게이트 $\Gamma_r$은 여러분이 새로운 매개변수 매트릭스 $\sigma(W_r[c^{< t-1>}, x^{< t>}] + b_r)$로 기대하는 것 만큼 계산되었습니다.

여러분이 상상하신대로 이러한 유형의 신경망을 디자인 하는 여러 방법이 있습니다. 그러면 우리는 왜 $\Gamma_r$을 사용할까요? 이전 슬라이드의 간단한 버전을 사용하는 것은 어떨까요?

수년 동안 연구자들은 더 긴 거리의 연결을 가지기 위한 이러한 유닛을 설계 방법을 가능한 여러 가지 버전으로 실험해 왔습니다. 장거리에 효과적인 모델을 가지기 위해 또 기울기 소멸 문제를 해결하기 위해서 말이죠.

그리고 GRU는 연구자들이 수렴되어 많은 다양한 문제에 유용한 것으로 밝혀진 가장 일반적으로 사용되는 버전 중 하나입니다. 원한다면 이 단위의 새로운 버전을 발명하려고 한다면 시도할 수 있지만 GRU는 가장 일반적으로 사용됩니다.

연구자들이 여기에 적힌 것과 비슷하지만 정확히 같지는 않은 다른 버전들도 시도했을 거라 상상할 수도 있죠. 다른 일반적인 버전은 LSTM인데요 Long Short-term Memory를 의미합니다. 이 내용은 다음에 다룰 것이죠.

하지만 GRU와 LSTM은 가장 일반적으로 사용되는 이 아이디어의 두 가지 구체적인 인스턴스입니다. 표기법에 관한 한 쪽만 저는 이러한 아이디어들을 이해하기 쉽게 하기 위해 일관된 표기법을 정의하려 했습니다.

학문 문헌을 보면 가끔 사람을 대체 표기법을 사용하는 것을 볼 수 있는데요. 예를 들어 $\tiled{h}$나$u, r$ 그리고 이러한 수량들을 나타내기 위한 $h$가 있습니다.

그들은 GRU와 LSTM 사이의 좀 더 일관성이 있는 표기법을 사용하려 했습니다. gain을 뜻하는 더 일관성 있는 표기법인 $\Gamma$를 사용하는 것 뿐만 아니라요. 이 아이디어를 쉽게 이해할 수 있도록 말입니다.
