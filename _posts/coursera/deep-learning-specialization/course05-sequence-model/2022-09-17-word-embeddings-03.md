---
title: "[Ⅴ. Sequence Models] Natural Language Processing & Word Embeddings (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Natural Language Processing & Word Embeddings (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Natural Language Processing & Word Embeddings

## Introduction to Word Embeddings

### Properties of Word Embeddings

이제 단어 임베딩을 위한 구체적인 알고리즘을 배우기 시작할 것입니다. 단어 임베딩에 적용된 깊은 학습의 히스토리에서 사람들은 실제로 비교적 복잡한 알고리즘으로 시작했습니다. 그리고 시간이 흐르면서 연구원들은 더 간단하고 단순한 알고리즘을 사용할 수 있으며 특히 큰 데이터 세트에 매우 좋은 결과를 얻을 수 있다는 것을 발견했습니다.

하지만 오늘날 가장 인기 있는 몇몇 알고리즘들은 너무 단순해서 제가 그것들을 먼저 보여드리면 거의 마법처럼 보일 수도 있고 어떻게 이런 간단한 일이 가능할까요? 그래서 제가 하려는 것은 그래서 제가 하려는 것은 조금 더 복잡한 알고리즘의 일부로부터 시작하려고 합니다.

왜냐하면 저는 그것들이 왜 작용해야 하는지에 대한 직관을 개발하는 것이 실제로 더 쉽다고 생각하기 때문입니다. 그리고 이 알고리즘들을 단순화하고 아주 좋은 결과를 주는 몇 가지를 보여드리도록 하겠습니다. 시작해 봅시다.

#### Neural Language Model

![image](https://user-images.githubusercontent.com/55765292/190834244-09f7b8e4-38f2-45c3-9bb4-eb43926b2409.png)

언어 모델을 만든다고 해 봅시다. 그리고 신경망을 이용할 겁니다. 훈련을 하는 동안 신경망이 입력과 같은 것을 하기를 원할 수도 있습니다.

> I want a glass of orange ___

그리고 그 다음 단어를 순서대로 예측해 보세요. 그리고 이 단어들 아래에 저는 색인을 다른 단어들의 어휘들에 적었습니다. 신경 언어 모델을 구축하는 것은 임베딩되는 단어를 배울 수 있는 작은 방법이라는 것이 밝혀졌습니다.

자, 여기 신경망을 만들어서 연속적인 단어에서 다음 단어를 예측할 수 있습니다. 단어들을 나열해 보겠습니다.

I want a glass of orange 첫번째 단어로 시작해보죠. 그래서 저는 제1이라는 단어에 상응하는 벡터를 하나 더 만들고자 합니다. 벡터가 하나 더 있고 위치가 4343입니다 1만 차원 벡터가 될 것입니다. 그리고 E의 매개 변수 매트릭스를 가지고 $e_{4343}$을 포함한 벡터 $e_{4343}$을 얻도록 $e_{4343}$을 찍겠습니다.

그리고 이 단계는 $e_{4343}$을 E매트릭스에 의해 얻어진 후 벡터 4343을 더한다는 것을 의미합니다. 그리고 다른 모든 단어에도 똑같이 할 것입니다. 즉, 원하는 단어는 9665 하나가 벡터를 추가하고 E와 곱해서 포함 벡터를 얻는 것입니다. 다른 단어들도 마찬가지입니다.

'a' 는 사전에 있는 첫 번째 단어이고 알파벳은 첫 번째입니다. 그래서 $O_1$, $e_1$을 얻고 이와 비슷하게 이 문장의 다른 단어들도 그래서 여기에 삼차원 전부를 담아두고 있습니다. 그래서 이 각각은 300차원 벡터입니다. 그리고 우리가 할 수 있는 것은 그 모든 것을 신경망으로 채우고 있는 것입니다.

여기 신경망 층이 있습니다. 그리고 이 신경망은 소프트맥스에 공급됩니다. 소프트맥스에는 자신만의 매개변수도 있습니다. 그리고 소프트맥스는 우리가 예측하려는 최종 낱말을 위해 10,000개의 가능한 발음에 따라 분류됩니다.

그래서 트레이닝 슬라이드에서 쥬스라는 단어를 봤다면 소프트맥스의 학습 목표는 또 다른 워드 쥬스를 예측해야 한다는 것을 예측해야 합니다. 그래서 여기 이 숨겨진 이름은 자신만의 매개변수를 가지고 있습니다.

자, $W^{[1]}$이라고부르겠습니다. $b^{[1]}$도 있습니다. 소프트맥스 자체에는 $W^{[2]}, b^{[2]}$라는 자신만의 매개변수가 있었습니다. 그리고 그들은 300차원적인 단어를 임베딩시켰습니다. 그리고 여기 6개의 단어가 있습니다. 6 곱하기 300입니다. 그래서 이 층이나 이 입력은 6개의 벡터를 포함한 것을 쳐서 얻은 1,800차원 벡터가 됩니다.

글쎄요, 사실 가장 흔히 하는 일은 고정된 히스토리 창을 갖는 것입니다. 예를 들어, 여러분은 항상 앞의 네 단어를 예측하려고 할 수 있습니다. 이 단어 네 개는 알고리즘의 하이퍼매개 변수입니다. 그래서 이것이 여러분이 아주 긴 문장이나 아주 짧은 문장에 적응하는 방식입니다. 또는 여러분은 항상 앞의 네 단어를 살펴보기로 결심합니다.

그래서 여러분은 이렇게 말하겠죠. 저는 그 네 단어를 계속 사용할 겁니다.

자, 이제 이걸 없애봅시다. 그래서, 만약 여러분이 항상 4개의 단어 기록을 사용하고 있다면 이것은 여러분의 신경 네트워크가 1,200차원의 피쳐 벡터를 입력하고 이 층으로 가서 소프트맥스 하나를 가지고 결과를 예측하려고 한다는 것을 의미합니다.

그리고 다시 다양한 선택입니다. 그리고 고정된 히스토리를 사용하는 것은 입력 크기가 항상 고정되어 있기 때문에 임의로 긴 문장들을 다룰 수 있다는 뜻입니다. 따라서 이 모델의 매개변수는 이 매트릭스 E가 되며 모든 단어의 행렬 E를 사용합니다.

따라서, 다른 위치에 대한 서로 다른 매트릭스가 없습니다. 그리고 경사하강법을 수행하기 위해서 역전파를 사용할 수 있습니다. 연속적으로 4개의 단어를 예측할 수 있도록 한 그래디언트를
전송할 수 있습니다. 텍스트 코퍼에서 다음 단어는 무엇일까요?

그리고 이 알고리즘은 우리가 꽤 괜찮은 단어 임베딩이 된다는 걸 알게 됩니다. 그 이유는, 만약 오렌지 주스 사과 주스 예를 기억하신다면 알고리즘의 도움으로 오렌지와 사과를 배울 수 있습니다. 왜냐하면 그렇게 함으로써 때로는 오렌지 주스를 볼 수도 있고 때로는 사과 주스를 볼 수도 있기 때문에 그 단어들을 표현할 수 있는 300차원적인 요소만이 있다면 알고리즘은 훈련 세트에
딱 맞는 것을 찾을 겁니다.

사과, 오렌지, 포도, 배 그리고 배 등 아주 희귀 과일이자 유사한 특징을 가진 두리안 과일이라면 이것은 단어 임베딩을 위한 아주 성공적이고 빠른 알고리즘 중 하나입니다. 이 행렬 E를 배울수 있는 것이죠. 하지만 이제 이 알고리즘을 일반화해서 어떻게 더 단순한 알고리즘을 도출할 수 있는지 봅시다.

#### Other Context/Target Pairs

![image](https://user-images.githubusercontent.com/55765292/190834210-de3af5a7-c1b1-4dae-90de-ba6817654ee5.png)

자, 다른 알고리즘을 예로 들어보겠습니다. 좀더 복잡한 문장을 사용해서 훈련 세트에서는 이런 긴 문장이 있다고 가정해 봅시다. 저는 시리얼에 오렌지 주스 한 잔을 넣고 싶습니다. 그래서 마지막에 본 것은 알고리즘이 목표단어 '쥬스'를 예측할 수 있다는 것이었습니다. 그리고 마지막 네 단어가 일치하는 문맥도 있었습니다.

그래서 여러분의 목표가 여러 다른 종류의 문맥으로 연구원들을 포함시키는 것을 배우는 것이라면 언어 모델을 작성하는 경우 컨텍스트가 대상 단어 바로 앞에 몇 개의 단어가 되는 것은 당연한 것입니다. 하지만 목표가 언어 모델을 학습하는 것이 아니라면 다른 컨텍스트를 선택할 수 있습니다.

예를 들어, 문맥은 왼쪽, 오른쪽에 있는 네 단어인 학습 문제를 제기할 수 있습니다. 그래서 왼쪽에서 오른쪽으로 네 단어들을 이해할 수 있습니다. 그리고 이것이 의미하는 것은 우리가 학습 문제를 하나 더 생각해 봤다는 것입니다 알고리즘이 왼쪽에 네 단어를 부여하면 그래서 오른쪽에는 오렌지 한 잔과 네개의 단어가 있습니다. 그리고 가운데 단어를 예상해야 합니다.

그리고 이렇게 학습의 문제를 올려놓고 있는 가운데 단어들을 쓰려고 합니다. 그리고 오른쪽 네 단어들은 신경연결망에 쓰입니다. 앞서 보신 바와 비슷하죠. 단어의 중간을 예측하려고 노력하고 그 단어를 가운데 넣으려고 노력합니다.

단어 임베딩을 배우려고 할 때도 쓰입니다. 아니면 좀 더 단순한 상황을 사용하고자 한다면 아마 마지막 한 단어만 사용하시겠죠. 오렌지란 단어를 보면 오렌지 다음에 무엇이 오는가? 이것은 다른 학습 문제입니다.

한 단어, 오렌지, 그리고 잘 말할 것입니다. 다음 단어는 무엇이라고 생각하시나요? 그리고 그 단어에 맞는 신경 네트워크를 만들 수 있습니다. 이전의 단어와 이전의 단어와 하나의 단어를 신경망에 내장시키는 것이죠. 다음 단어에 대한 예측을 해보면서요. 아니면, 놀랍게도 가까운 한 단어로 된 단어를 가져가는 것이 좋습니다.

어떤 이들은 근처에 있는 아마 "glass"를 선택할 겁니다. 어떤 사람들은 "유리를 봤다"고 말하겠죠. 그리고 유리근처에 다른 단어가 있어요. 그 단어가 뭐라고 생각하시나요? 그래서, 이 문맥에 가까운 단어 하나를 사용할 것입니다.

다음에 이 내용을 공식화하겠지만 이건 Skip-Gram 모델의 아이디어입니다. 그리고 단순한 알고리즘의 한 예입니다. 현재 문맥들이 훨씬 더 단순해졌으며 단 하나의 단어로 된 단어가 되는데요. 매우 잘 작동합니다. 그래서 연구원들이 알아낸 것은 만약 여러분이 정말로 언어 모델을 만들고 싶다면 마지막 몇 단어를 하나의 문맥으로 사용하는 것은 자연스럽다는 것입니다.

하지만 여러분의 주요 목표가 단어 포함 방법을 정말로 배우고자 한다면 여러분은 이러한 모든 다른 내용들을 사용할 수 있고 그것들은 매우 의미 있는 작업 포함 속에서 결과를 초래할 것입니다. 다음에 자세한 내용을 설명하겠습니다. 월터 VEC 모델에 대해 이야기하겠습니다.

요약하자면 여러분은 어떻게 언어 모델링 문제가 기계 학습 문제를 유발하는지 보셨을 겁니다. 이 문제로 인해 마지막 네 단어처럼 내용을 입력하고 구체적인 대상 단어를 제시하면 어떻게 입력 단어를 포함할 수 있는지 알 수 있습니다. 이어서 문맥에서 대상 단어로 표시하는 더 단순한 문맥과 심지어 더 단순한 학습 알고리즘을 사용하는 방법을 볼 수 있습니다. 또한 단어를 포함하는 좋은 단어를 배울 수도 있습니다.
