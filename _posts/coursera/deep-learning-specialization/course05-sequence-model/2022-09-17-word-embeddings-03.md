---
title: "[Ⅴ. Sequence Models] Natural Language Processing & Word Embeddings (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Natural Language Processing & Word Embeddings (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Natural Language Processing & Word Embeddings

## Introduction to Word Embeddings

### Properties of Word Embeddings

이제 단어 임베딩을 위한 구체적인 알고리즘을 배우기 시작할 것입니다. 단어 임베딩에 적용된 깊은 학습의 히스토리에서 사람들은 실제로 비교적 복잡한 알고리즘으로 시작했습니다. 그리고 시간이 흐르면서 연구원들은 더 간단하고 단순한 알고리즘을 사용할 수 있으며 특히 큰 데이터 세트에 매우 좋은 결과를 얻을 수 있다는 것을 발견했습니다.

하지만 오늘날 가장 인기 있는 몇몇 알고리즘들은 너무 단순해서 제가 그것들을 먼저 보여드리면 거의 마법처럼 보일 수도 있고 어떻게 이런 간단한 일이 가능할까요? 그래서 제가 하려는 것은 그래서 제가 하려는 것은 조금 더 복잡한 알고리즘의 일부로부터 시작하려고 합니다.

왜냐하면 저는 그것들이 왜 작용해야 하는지에 대한 직관을 개발하는 것이 실제로 더 쉽다고 생각하기 때문입니다. 그리고 이 알고리즘들을 단순화하고 아주 좋은 결과를 주는 몇 가지를 보여드리도록 하겠습니다. 시작해 봅시다.

#### Neural Language Model

![image](https://user-images.githubusercontent.com/55765292/190834244-09f7b8e4-38f2-45c3-9bb4-eb43926b2409.png)

언어 모델을 만든다고 해 봅시다. 그리고 신경망을 이용할 겁니다. 훈련을 하는 동안 신경망이 입력과 같은 것을 하기를 원할 수도 있습니다.

> I want a glass of orange ___

그리고 그 다음 단어를 순서대로 예측해 보세요. 그리고 이 단어들 아래에 저는 색인을 다른 단어들의 어휘들에 적었습니다. 신경 언어 모델을 구축하는 것은 임베딩되는 단어를 배울 수 있는 작은 방법이라는 것이 밝혀졌습니다.

자, 여기 신경망을 만들어서 연속적인 단어에서 다음 단어를 예측할 수 있습니다. 단어들을 나열해 보겠습니다.

I want a glass of orange 첫번째 단어로 시작해보죠. 그래서 저는 제1이라는 단어에 상응하는 벡터를 하나 더 만들고자 합니다. 벡터가 하나 더 있고 위치가 4343입니다 1만 차원 벡터가 될 것입니다. 그리고 E의 매개 변수 매트릭스를 가지고 $e_{4343}$을 포함한 벡터 $e_{4343}$을 얻도록 $e_{4343}$을 찍겠습니다.

그리고 이 단계는 $e_{4343}$을 E매트릭스에 의해 얻어진 후 벡터 4343을 더한다는 것을 의미합니다. 그리고 다른 모든 단어에도 똑같이 할 것입니다. 즉, 원하는 단어는 9665 하나가 벡터를 추가하고 E와 곱해서 포함 벡터를 얻는 것입니다. 다른 단어들도 마찬가지입니다.

'a' 는 사전에 있는 첫 번째 단어이고 알파벳은 첫 번째입니다. 그래서 $O_1$, $e_1$을 얻고 이와 비슷하게 이 문장의 다른 단어들도 그래서 여기에 삼차원 전부를 담아두고 있습니다. 그래서 이 각각은 300차원 벡터입니다. 그리고 우리가 할 수 있는 것은 그 모든 것을 신경망으로 채우고 있는 것입니다.

여기 신경망 층이 있습니다. 그리고 이 신경망은 소프트맥스에 공급됩니다. 소프트맥스에는 자신만의 매개변수도 있습니다. 그리고 소프트맥스는 우리가 예측하려는 최종 낱말을 위해 10,000개의 가능한 발음에 따라 분류됩니다.

그래서 트레이닝 슬라이드에서 쥬스라는 단어를 봤다면 소프트맥스의 학습 목표는 또 다른 워드 쥬스를 예측해야 한다는 것을 예측해야 합니다. 그래서 여기 이 숨겨진 이름은 자신만의 매개변수를 가지고 있습니다.

자, $W^{[1]}$이라고부르겠습니다. $b^{[1]}$도 있습니다. 소프트맥스 자체에는 $W^{[2]}, b^{[2]}$라는 자신만의 매개변수가 있었습니다. 그리고 그들은 300차원적인 단어를 임베딩시켰습니다. 그리고 여기 6개의 단어가 있습니다. 6 곱하기 300입니다. 그래서 이 층이나 이 입력은 6개의 벡터를 포함한 것을 쳐서 얻은 1,800차원 벡터가 됩니다.

글쎄요, 사실 가장 흔히 하는 일은 고정된 히스토리 창을 갖는 것입니다. 예를 들어, 여러분은 항상 앞의 네 단어를 예측하려고 할 수 있습니다. 이 단어 네 개는 알고리즘의 하이퍼매개 변수입니다. 그래서 이것이 여러분이 아주 긴 문장이나 아주 짧은 문장에 적응하는 방식입니다. 또는 여러분은 항상 앞의 네 단어를 살펴보기로 결심합니다.

그래서 여러분은 이렇게 말하겠죠. 저는 그 네 단어를 계속 사용할 겁니다.

자, 이제 이걸 없애봅시다. 그래서, 만약 여러분이 항상 4개의 단어 기록을 사용하고 있다면 이것은 여러분의 신경 네트워크가 1,200차원의 피쳐 벡터를 입력하고 이 층으로 가서 소프트맥스 하나를 가지고 결과를 예측하려고 한다는 것을 의미합니다.

그리고 다시 다양한 선택입니다. 그리고 고정된 히스토리를 사용하는 것은 입력 크기가 항상 고정되어 있기 때문에 임의로 긴 문장들을 다룰 수 있다는 뜻입니다. 따라서 이 모델의 매개변수는 이 매트릭스 E가 되며 모든 단어의 행렬 E를 사용합니다.

따라서, 다른 위치에 대한 서로 다른 매트릭스가 없습니다. 그리고 경사하강법을 수행하기 위해서 역전파를 사용할 수 있습니다. 연속적으로 4개의 단어를 예측할 수 있도록 한 그래디언트를
전송할 수 있습니다. 텍스트 코퍼에서 다음 단어는 무엇일까요?

그리고 이 알고리즘은 우리가 꽤 괜찮은 단어 임베딩이 된다는 걸 알게 됩니다. 그 이유는, 만약 오렌지 주스 사과 주스 예를 기억하신다면 알고리즘의 도움으로 오렌지와 사과를 배울 수 있습니다. 왜냐하면 그렇게 함으로써 때로는 오렌지 주스를 볼 수도 있고 때로는 사과 주스를 볼 수도 있기 때문에 그 단어들을 표현할 수 있는 300차원적인 요소만이 있다면 알고리즘은 훈련 세트에
딱 맞는 것을 찾을 겁니다.

사과, 오렌지, 포도, 배 그리고 배 등 아주 희귀 과일이자 유사한 특징을 가진 두리안 과일이라면 이것은 단어 임베딩을 위한 아주 성공적이고 빠른 알고리즘 중 하나입니다. 이 행렬 E를 배울수 있는 것이죠. 하지만 이제 이 알고리즘을 일반화해서 어떻게 더 단순한 알고리즘을 도출할 수 있는지 봅시다.

#### Other Context/Target Pairs

![image](https://user-images.githubusercontent.com/55765292/190834210-de3af5a7-c1b1-4dae-90de-ba6817654ee5.png)

자, 다른 알고리즘을 예로 들어보겠습니다. 좀더 복잡한 문장을 사용해서 훈련 세트에서는 이런 긴 문장이 있다고 가정해 봅시다. 저는 시리얼에 오렌지 주스 한 잔을 넣고 싶습니다. 그래서 마지막에 본 것은 알고리즘이 목표단어 '쥬스'를 예측할 수 있다는 것이었습니다. 그리고 마지막 네 단어가 일치하는 문맥도 있었습니다.

그래서 여러분의 목표가 여러 다른 종류의 문맥으로 연구원들을 포함시키는 것을 배우는 것이라면 언어 모델을 작성하는 경우 컨텍스트가 대상 단어 바로 앞에 몇 개의 단어가 되는 것은 당연한 것입니다. 하지만 목표가 언어 모델을 학습하는 것이 아니라면 다른 컨텍스트를 선택할 수 있습니다.

예를 들어, 문맥은 왼쪽, 오른쪽에 있는 네 단어인 학습 문제를 제기할 수 있습니다. 그래서 왼쪽에서 오른쪽으로 네 단어들을 이해할 수 있습니다. 그리고 이것이 의미하는 것은 우리가 학습 문제를 하나 더 생각해 봤다는 것입니다 알고리즘이 왼쪽에 네 단어를 부여하면 그래서 오른쪽에는 오렌지 한 잔과 네개의 단어가 있습니다. 그리고 가운데 단어를 예상해야 합니다.

그리고 이렇게 학습의 문제를 올려놓고 있는 가운데 단어들을 쓰려고 합니다. 그리고 오른쪽 네 단어들은 신경연결망에 쓰입니다. 앞서 보신 바와 비슷하죠. 단어의 중간을 예측하려고 노력하고 그 단어를 가운데 넣으려고 노력합니다.

단어 임베딩을 배우려고 할 때도 쓰입니다. 아니면 좀 더 단순한 상황을 사용하고자 한다면 아마 마지막 한 단어만 사용하시겠죠. 오렌지란 단어를 보면 오렌지 다음에 무엇이 오는가? 이것은 다른 학습 문제입니다.

한 단어, 오렌지, 그리고 잘 말할 것입니다. 다음 단어는 무엇이라고 생각하시나요? 그리고 그 단어에 맞는 신경 네트워크를 만들 수 있습니다. 이전의 단어와 이전의 단어와 하나의 단어를 신경망에 내장시키는 것이죠. 다음 단어에 대한 예측을 해보면서요. 아니면, 놀랍게도 가까운 한 단어로 된 단어를 가져가는 것이 좋습니다.

어떤 이들은 근처에 있는 아마 "glass"를 선택할 겁니다. 어떤 사람들은 "유리를 봤다"고 말하겠죠. 그리고 유리근처에 다른 단어가 있어요. 그 단어가 뭐라고 생각하시나요? 그래서, 이 문맥에 가까운 단어 하나를 사용할 것입니다.

다음에 이 내용을 공식화하겠지만 이건 Skip-Gram 모델의 아이디어입니다. 그리고 단순한 알고리즘의 한 예입니다. 현재 문맥들이 훨씬 더 단순해졌으며 단 하나의 단어로 된 단어가 되는데요. 매우 잘 작동합니다. 그래서 연구원들이 알아낸 것은 만약 여러분이 정말로 언어 모델을 만들고 싶다면 마지막 몇 단어를 하나의 문맥으로 사용하는 것은 자연스럽다는 것입니다.

하지만 여러분의 주요 목표가 단어 포함 방법을 정말로 배우고자 한다면 여러분은 이러한 모든 다른 내용들을 사용할 수 있고 그것들은 매우 의미 있는 작업 포함 속에서 결과를 초래할 것입니다. 다음에 자세한 내용을 설명하겠습니다. 월터 VEC 모델에 대해 이야기하겠습니다.

요약하자면 여러분은 어떻게 언어 모델링 문제가 기계 학습 문제를 유발하는지 보셨을 겁니다. 이 문제로 인해 마지막 네 단어처럼 내용을 입력하고 구체적인 대상 단어를 제시하면 어떻게 입력 단어를 포함할 수 있는지 알 수 있습니다. 이어서 문맥에서 대상 단어로 표시하는 더 단순한 문맥과 심지어 더 단순한 학습 알고리즘을 사용하는 방법을 볼 수 있습니다. 또한 단어를 포함하는 좋은 단어를 배울 수도 있습니다.


### Word2Vec

좋은 단어들을 임베딩 시키기 위해 어떻게 신경 언어 모델을 배울 수 있는지 보았습니다. 이번에는 이런 종류의 임베딩을 학습하기에 간단하고 더욱 효율적인 방법인 **Word2Vec** 알고리즘을 볼 수 있습니다. 한번 보시죠.

#### Skip-grams

![image](https://user-images.githubusercontent.com/55765292/190860308-aa5ce0c6-c0e8-4a07-bff8-5ec41df7e299.png)

훈련세트에서 이 문장을 받았다고 가정해 봅시다. 건너뛰기 모델 에서 우리가 하려는 것은 학습 내용을 만드는 데 필요한 몇 가지 사항을 제시합니다. 따라서 문맥은 항상 마지막 네 단어나 마지막 단어들이 되는 것이 아니라 제가 하려는 것은 무작위로 문맥적인 단어가 될 단어들을 골라내는 것입니다.

오렌지라는 단어를 선택했다고 해보죠. 그리고 우리가 할 일은 무작위로 창 안에서 다른 단어를 골라내는 것입니다. 문장의 단어 중 앞뒤 5단어 또는 앞뒤 10단어를 입력하면 대상 단어가 됩니다. 그래서 아마도 여러분이 목표한 단어가 되도록 주스를 고를 수 있을 것입니다.

그것은 단지 한 단어입니다. 아니면 이전에 두 단어를 선택할 수도 있습니다. 다른 한 쌍은 목표물이 유리일 수도 있습니다. 아니면 우연히 제 단어를 목표물로 선택했을 수도 있습니다. 그리고 우리는 지도 하에 학습 문제를 해결하도록 합니다. 상황에 맞는 단어를 입력하면 무작위로 선택된 단어, 즉 앞뒤 10단어 또는 앞뒤 5에서 10단어 사이의 입력 컨텍스트 단어의 창 중에서 물론 이것은 쉬운 문제는 아닙니다.

#### Model

![image](https://user-images.githubusercontent.com/55765292/190860324-7c19cb10-3dd5-45f4-ae07-75b93f321127.png)

여기 모델의 세부사항이 있습니다. 우리가 만 단어로 된 우리의 사전를 계속하겠다고 합시다. 그리고 어떤 것들은 100만단어가 넘는 사전을 가지고 있습니다. 하지만 우리가 해결할 기본적인 지도 학습 문제는 오렌지 같은 단어부터 특정 대상에 이르는 문맥 C의 지도를 배우고 싶다는 겁니다.

이 문장의 예를 들자면 이전 그림의 예를 들자면 주스, 단어 유리, 또는 제 단어일 수도 있습니다. 우리 어휘에서 오렌지은 단어 6257입니다. 주스는 우리 사전의 4834개 중 10,000개의 단어입니다. 그리고 이것이 여러분이 배우고 싶은 입력숫자입니다.

y를 열고 오렌지 같은 입력사항을 나타내려면 먼저 O 아래 첨자 C로 쓸 핫 벡터로 시작해보죠. 그러면 문맥 단어에 대한 하나의 핫 벡터가 있습니다. 그리고 최근에 보신 것과 마찬가지로 포함 매트릭스 E를 벡터 O 서브스크립트 C에 곱하고, 이 경우 입력 컨텍스트 단어의 포함 벡터를 사용할 수 있습니다.

여기서 $e_c$는 대문자 E와 하나의 핫 벡터를 곱한 값과 같습니다. 그리고 우리가 구성한 이 새로운 네트워크에서 이 벡터 $e_c$를 이용해서 소프트맥스 유닛에 제공할 것입니다. 그래서 지금까지 소프트맥스 장치를 신경 네트워크의 한 마디로 그릴 수 있었습니다. 저건 O가 아니라 소프트맥스 장치입니다. 그리고 소프트맥스 유닛에서 $hat{y}$를 출력할 수 있는 감소가 있어요. 그래서 이 모델을 세세하게 쓰기 위해서요.

이 모델은 소프트 모델 서로 다른 단어의 확률과 입력 컨텍스트 단어가 $e^{\theta_t^T e_c}$로 지정된 것입니다. 모든 단어로 나누자면, 식의 분모와 같습니다. 여기 세타 T는 제가 덧붙이고 싶은 매개변수인데요. 하지만 사실 특정한 단어 t가 라벨이 될 가능성이 있습니다.

그래서 이를 풀기 위해 편향을 버렸습니다. 하지만 우리가 원한다면 이것까지 포함할 수 있습니다. 그리고 마지막으로 소프트맥스의 손실 유닛은 평소처럼 우리는 y를 표적으로 사용합니다. 그리고 우리는 여기서 $\hat{y}$과 $y$를 위해 하나의 핫 표현을 사용합니다.

그러면 손실함수는 옆의 식이 됩니다. 이것은 단 1개의 1과 나머지 0을 가진 하나의 핫 벡터입니다. 목표어가 주스라면 여기서 4834번 요소가 됩니다. 1과 같고 나머지는 0과 같습니다. 마찬가지로 $\hat{y}$는 소프트맥스 유닛으로 1만 차원 벡터 출력물이며 가능한 1만 개의 단어를 대상으로 한 확률을 가지고 있습니다.

요약하자면 이것은 전체 작은 모델입니다. 작은 신경 네트워크입니다. 기본적으로 만개를 올려놓고 그리고 아주 작은 최대 단위를 보는거죠. 행렬 E에는 많은 매개 변수가 있어서 매트릭스 E는 이러한 포함 벡터 E에 해당하는 매개 변수를 가집니다. 또한 소프트맥스 유닛에는 세타 T 매개변수를 제공하는 매개변수도 있습니다.

하지만 이 모든 매개변수를 기준으로 이 손실 함수를 최적화하면 실제로 상당량의 벡터 포함 세트를 얻을 수 있습니다. 이것은 **건너뛰기skip-grams** 모델이라고 합니다.

왜냐하면 그것은 오렌지 같은 하나의 단어로 표현된 다음 몇 단어를 건너뛰는 몇 단어를 예측하는 것을 왼쪽이나 오른쪽으로부터 하는 것입니다. 문맥 뒤에 무엇이 오는지 예측합니다. 알고보니 이 알고리즘을 사용하는데 몇가지 문제가 있습니다.

#### Problems with softmax classification

![image](https://user-images.githubusercontent.com/55765292/190860343-ac0dce48-9220-418f-81f7-3ee88b9be85e.png)

주된 문제는 계산 속도입니다. 특히 소프트맥스 모델의 경우 여러분이 이 확률을 평가하기 원할 때마다 여러분의 어휘에서 1만 단어 이상을 계산해야 합니다. 그리고 아마 10,000개는 그다지 나쁘지 않습니다. 하지만 만약 여러분이 100,000 또는 1,000,000개의 사이즈의 어휘를 사용하고 있다면 그것은 매번 이 분모를 합산하는 것이 매우 느립니다.

사실 1만개는 말이 꽤 느린 편이지만 좀더 넓은 어휘로 확장하기가 더 어렵습니다. 이에 대한 몇 가지 해결책이 있습니다. 문헌에서 볼 수 있는 해결책은 계층적 소프트맥스 분류기를 사용하는 것입니다. 이것이 의미하는 바는 어떤 것을 만개의 행으로 분류하는 대신에 한 번에 만개를 분류하는 것입니다.

한 분류기가 있다고 상상해보세요. 단어 첫 5,000개의 단어에서 목표어라고 말하는 것을 상상해보세요. 아니면 두번째 5천개의 단어로 된 단어일까요. 그리고 이 이진 분류기가 첫 5,000개를 말했다고 해 봅시다. 2등급을 생각해 보세요. 첫 2,500단어, 두번째 2,500단어 각각에 대해서 말이죠. 결국 어떤 단어인지 정확히 분류하기 위해 내려갈 때까지 이 트리의 잎과 이와 같은 분류기 트리를 가질 때까지 트리의 리트리버 각각의 리트리버 노드가 단지 바인딩 분류기임을 의미합니다.

그래서 여러분은 만 단어 이상을 합산할 필요가 없습니다. 아니면 하나의 분류를 위해 배가 부를 수도 있습니다. 실제로, 이렇게 계산적으로 분류되는 트리는 보갑 크기의 선형 보다는 보갑 크기의 로그처럼 확장합니다. 이것을 소셜 소프트맥스 분류기라고 합니다. 실제로 소프트맥스 분류기는 분기의 왼쪽과 오른편에 같은 수의 단어를 사용하는 이 완벽한 균형 있는 트리나 이 완벽한 대칭 트리를 사용하지 않습니다.

실상, 계층적 소프트웨어 분류기는 일반어가 상위에 있는 경향이 있는 반면 두리안 같은 덜 흔한 단어들은 트리에 훨씬 더 깊게 묻혀질 수 있습니다. 왜냐하면 일반적인 단어를 더 자주 볼 수 있기 때문에 그리고 의 같은 일반적인 단어들을 얻기 위해서는 몇 개의 노드만 필요할지도 모릅니다. 두리안 같은 말은 덜 자주 보지만 트리 깊은 곳에 묻어도 좋다고 말합니다. 그렇게 깊게 들어갈 필요가 없기 때문이죠.

그래서 트리를 만들기 위한 다양한 경험들이 있습니다. 여러분이 어떻게 계층 구조의 소프트웨어 스파이퍼를 구축했는지 말입니다. 이건 문헌에서 볼 수 있는 하나의 아이디어인데 소프트맥스 분류의 가속도입니다.

제 생각엔 더 간단합니다. 또한 소프트맥스 분류기를 빠르게 만드는 것과 분모의 전체 대문자 크기에 대한 합계를 필요로 하는 문제에 대해서도 잘 작동합니다.

마무리하기전에 컨텍스트 C를 샘플링하는 방법을 알아보려 합니다. 컨텍스트 C를 샘플링하면 타겟 T를 컨텍스트 C의 10단어 창 내에서 샘플링할 수 있습니다. 그러나 컨텍스트 C를 어떻게 선택합니까? 한 가지 할 수 있는 것은 훈련세트에서 무작위로 그렇게 하면 이런 단어, of, a, to 등이 아주 자주 나타납니다.

그래서 그렇게 하면 여러분의 문맥에서 대상 매핑 쌍에 대한 이런 종류의 단어들을 매우 자주 얻는다는 것을 알 수 있습니다. 반면에 오렌지, 사과, 그리고 두리안 같은 단어들은 자주 나타나지 않습니다. 그리고 아마도 여러분은 여러분의 훈련 세트가 이러한 매우 자주 또는 현재 단어들에 의해 지배되는 것을 원하지 않을 것입니다.

이런 빈번한 단어에 대해, $e_c$를 업데이트하며 많은 시간을 쓰기 때문이죠. 하지만 포함 과정을 업데이트 하는데 시간을 쓰길 원하죠 $e_durian$ 같은 덜 흔한 단어들이라도 말이죠.

Word2Vec skip-gram 모델의 경우 두 가지 버전의 Word2Vec 모델이 있고 건너뛰기 그램은 1입니다. 또 다른 하나는 연속적인 후진 모델인 CBow입니다. 이 모델은 중어에서 시작하여 주변 문맥들을 가지고 중어를 예측하려고 주변 단어들을 사용합니다. 그리고 그 알고리즘도 작동합니다. 이것은 몇가지 장점과 단점을 가지고 있습니다. 그러나 지금까지 제시된 건너뛰기-그램 모델과 이 알고리즘의 주요 문제는 소프트맥스 단계의 전체 어휘 크기를 분모로 계산해야 하기 때문에 계산하는 데 매우 비용이 든다는 점입니다.

다음에는 학습 목표를 수정하는 알고리즘을 보여 줍니다. 이 알고리즘은 보다 효율적으로 운영될 수 있도록 해주므로 보다 큰 피팅 세트에 적용할 수 있습니다. 따라서 더욱 좋은 단어 임베딩을 학습시킬 수 있습니다.
