---
title: "[Ⅴ. Sequence Models] Natural Language Processing & Word Embeddings (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Natural Language Processing & Word Embeddings (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Natural Language Processing & Word Embeddings

## Introduction to Word Embeddings

### Properties of Word Embeddings

지금쯤 단어 임베딩이 자연어처리 활용 사례를 구축하는 데 어떻게 도움이 될 수 있는지 이해할 수 있을 것입니다. 단어 임베딩의 가장 매력적인 부분은 유추 문제에도 도움을 줄 수 있다는 것입니다. 그리고 유추문제가 그 자체로는 가장 중요한 자연어처리 사례는 아닐지 모르지만 이러한 문제들은 이러한 단어 임베딩이 하는 것에 대한 감을 전달할 수 있습니다. 단어 임베딩이 할수 있는 것을 말이죠.

#### Analogies

![image](https://user-images.githubusercontent.com/55765292/190176968-cabd035d-9a71-45f7-a8ac-07419afd6bb3.png)

자, 질문을 하나 던져보겠습니다 남자와 여자는 왕과 무엇과 같다? 라고 하면 많은 분들은 '남자와 여자는 왕과 왕비 같다.' 라고 말할거에요. 그러나 이러한 과정을 자동으로 생각해내도록 하는 알고리즘을 가질 수 있을까요? 자, 어떻게 할 수 있는지 보겠습니다.

남자를 표현하기위해 4차원 벡터를 사용한다고 해봅시다 $e_{5391}$이 될 것입니다. 여기에서는 $e_man$ 이라고 표현하겠지만 말이죠. 그리고 여자에대한 임베딩 벡터를 말해보면 저는 이제 $e_{woman}$라는 표현을 사용하고 유사하게 왕과 여왕도 표현합니다.

이 예제에서는 여러분이 4차원 임베딩을 사용한다고 가정할꺼에요. 50 ~1000차원을 사용하는것 대신이요. 그리고 그것은 훨씬 일반적인 표현일거에요 이런 벡터의 한가지 흥미로운 점은 당신이 벡터를 취하면 $e_{man} - e_{man}$은 그러면 근사적으로 -1 빼기 1은 -2이가 되고, 소숫점 자리의 값들은 모두 0이 되어 결국 대략적으로 [-2 0 0 0]을 갖게 됩니다.

그리고 비슷하게 당신이 $e_{king} - e_{queen}$을 하면 근적으로 거의 동일한 결과가 만들어집니다. -1 빼기 0.97은 약 -2가 되죠. 위의 경우와 유사합니다. 왜냐하면 왕과 여왕은 동등하게 왕족이기 때문이지요. 그래서 0이 되고 나이, 음식은 0이되죠.

그래서 이러한 문제에서 알 수 있는 바는 주요한 남여에서의 차이는 성별이라는 것입니다. 그리고 왕과 여왕의 주요 차이는 이 벡터들에서 표현된 바와 같이 마찬가지로 성별입니다. 왜 $e_{man} - e_{woman}$과 $e_{king} - e_{queen}$이 거의 유사한지를 알게합니다. 이러한 유추를 수행하는 한가지 방법은 만약 '남자와 여자는 왕과 무엇과 같은가? 라고 알고리즘에게 물어보면 계산할 수 있는 바는 $e_{man} - e_{woman}$이고 벡터를 찾아내는 일, 단어를 찾아내는 일 입니다.

$e_{man} - e_{woman}$과 $e_{king}$과 $e_{queen}$이 유사한 결과를 말이지요. 그리고 여왕이 포함되면 좌측은 우측과 동일하다는것을 알 수 있게됩니다. 이것은 가장 주목할만한 일 중의 하나이며 놀랍게도 단어 임베딩에서 영향력있는 결과입니다. 그리고 제가 생각하기에는 전체 커뮤니티가 단어 임베딩이 무엇인지에 대한 직관을 이해할 수 있도록 도왔습니다. 자, 이러한 개념을 알고리즘으로 형식화해봅시다.

#### Analogies Using Word Vectors

![image](https://user-images.githubusercontent.com/55765292/190177368-a911608f-dfc3-4fd1-a8b8-169d3a54806a.png)


사진에 있어 아마도 300차원의 공간안에서의 단어 임베딩은 실재하며 그에따라 남자라는 단어는 이 공간에서 점으로 표현되며 그리고 여자라는 단어는 이 공간에서 점으로 표현됩니다. 그리고 왕이라는 단어는 이 공간에서 다른 점으로 표현되고 그리고 여왕이라는 단어는 이 공간에서 다른 점으로 표현됩니다.

그리고 여기에서 짚고자 하는 바는 남자와 여자 사이의 벡터의 차이가 왕과 여왕 사이의 벡터의 차이와 매우 비슷하다는 점입니다. 그리고 제가 그린 이 화살표는 실제로 성별의 차이를 나타내는 벡터입니다. 성별에 그리고 기억해야될 것은 이 점들은 2차원이 아닌 300차원 안에서 그려진 점이라는 것입니다.

남자와 여자는 왕과 무엇과 같다? 라고하는 추론을 이해하기 위해서 여러분이 할 수 있는 일은 w를 찾는 것이고 이 수식은 참이고 단어 w를 찾고 싶다면 $e_{king} - e_{man} + e_{woman}$와 단어 w의 유사성을 최대화하는 단어를 찾으면 됩니다.

제가 한 것은 $e_?$ 두었고 $e_w$로 대체하였으며 $e_w$를 방정식의 한 부분에 두었습니다. 그리고나서 $e_{king} - e_{man} + e_{woman}$을 우측에 두었습니다. 그래서 우리는 적절한 유사성 함수를 갖게 되었습니다. w라는 단어의 임베딩이 우측의 값에 얼마나 유사한지를 나타내는 그런 함수를요. 그리고 유사성을 최대화 하는 단어를 찾는것은 당신이 여왕을 뽑게 해줄것 입니다.

그리고 주목할만한 점은 이러한 문제가 실제로 효과적이라는것입니다. 만일 당신이 단어세트의 임베딩을 배우고 이런 종류의 유사성을 최대화하는 단어 w를 찾는다면 당신은 실제로 정확하게 알맞는 정답을 얻는 것입니다. 그러나 문제의 디테일에 따라 연구논문을 찾아보면 말하자면 여러 연구논문이 30~75%의 정확성을 보고하는것은 드문일이 아닙니다. 우리가 다룬 유추문제와 같은 경우에 말이죠 유추문제를 정답이라고할 수 있는 경우는 완전히 정확한 단어를 추론했을 경우만 가능합니다.

이러한 경우에는 queen이라는 단어를 짚을 경우에만 답이 되는 것입니다. 넘어가기전에 좌측 그래프가 보여주는 의미를 명확하게 하고싶습니다. 이전에 우리는 t-SNE와같은 단어를 시각화하는 알고리즘을 사용하는것에대해 이야기한 바 있습니다.

t-SNE가 하는것은 300차원의 데이터를 취해서 매우 비선형적 방법으로 2차원에 맵핑하는것입니다. 그리고 t-SNE가 배우는 맵핑은 이것은 매우 복잡하며 매우 비선형적 맵핑이라는 것입니다. t-SNE 맵핑 이우에 당신은 이런류의 평행사변형 관계를 예측해서는 안됩니다. 우리가 좌측에서 맞다고 보았던 것 처럼 말이지요.

그리고 이것은 실제로 이 최초의 300차원 공간에서 여러분은 이러한 종류의 평행 사변형 관계를 더욱 신뢰할 수 있습니다. 하지만 대부분의 경우 t-SAE의 비선형적인 매핑 때문에 여러분은 그것을 믿지 말아야 합니다. 그리고 많은 평행사사 유추 관계는 t-SAE에 의해 깨집니다 넘어가기 전에 흔히 사용되는 유사도 함수에 대해서 말해봅시다. 그래서 가장 일반적으로 사용되는 유사성 기능은 코사인 유사성이라고 합니다.

#### Cosine Similarity

![image](https://user-images.githubusercontent.com/55765292/190177618-8fb345e4-19ac-4e71-99aa-06fc30f7e19c.png)

이것이 이전 그림에서 우리가 가졌던 방정식입니다. 그래서 코사인 유사성을 통해 두 벡터 와 v의 유사성을 유클리드 길이의 길이로 나눈 증산 v 사이의 유사성을 정의합니다. 그래서 지금으로서는 분모를 무시하면 이것은 기본적으로 u와 v 사이의 내적 산물입니다. 그래서 만약 u와 v가 매우 비슷하다면 그들의 내제품은 더 큰 경향이 있습니다. 이것은 코사인 유사성이라고 불립니다.

왜냐하면 이것은 사실 두 벡터 U와 v 사이의 각도의 코사인 때문입니다. 이 각은 파이이고, 따라서 사실 이 식은 두 벡터 사잇각의 코사인입니다. 그래서 여러분이 미적분학에서 이 파이의 코사인을 이렇게 생겼다는 것을 기억하실 겁니다. 따라서 그 사이의 각도가 0이면 코사인 유사성이 1과 같습니다.

그리고 각도가 90도라면 코사인 유사성은 0입니다. 그리고 180도이거나 완전히 반대방향으로 가리키는 경우 결국 -1이 됩니다. 여기서 코사인 유사성이라는 용어는 비롯되고 비유 추론 작업에 꽤 효과가 있습니다.

원하신다면 사각 거리 또는 유클리디안 거리 u-v 제곱 거리를 사용할 수도 있습니다. 엄밀히 따지면 이는 유사성보다는 차이점을 측정하는 것입니다. 그래서 우리는 이 부정적인 것을 받아들여야 합니다.

그리고 이것도 역시 효과가 있습니다. 코사인 유사성이 좀 더 자주 사용된다는 걸 알지만 그리고 이것들의 주요한 차이점은 벡터의 길이를 어떻게 정상화하는가입니다. 단어 임베딩에 관한 주목할 만한 결과 중 하나는 그들이 배울 수 있는 유추관계의 일반적인 모습입니다.

예를 들어, 남자란 남자로서 여자에게 여자라는 것을 배울 수 있습니다. 왜냐하면 남자와 여자 왕과 왕비, 남자나 여자처럼 남녀 간의 가장 큰 차이는 단지 성별이기 때문입니다. 캐나다 수도인 오타와와 나이로비로 캐나다에 있는 것은 배울 수 있습니다. 이것이 도시 수도가 국가 이름인 것입니다 큰 것은 키가 클수록 커진다는 것을 알 수 있고 이런 것을 배울 수 있습니다.

반면 엔화가 일본의 화폐이기에 일본은 엔화를 보유하고 있고 러시아는 루블을 보유하고 있습니다. 그리고 이 모든 것들은 큰 텍스트 코퍼에 단어 임베딩 학습 알고리즘을 실행하여 학습시킬 수 있습니다. 이 모든 패턴을 그 자체로 발견할 수 있습니다.

그래서 단어들이 어떻게 유추적인 추론에 사용되는지 보셨을 겁니다. 여러분이 응용 프로그램으로서 유추 추론 시스템을 구축하려 하지는 않겠지만 저는 이 표현들이 배울 수 있는 특징 같은 표현에 대해 몇 가지 직관을 주기를 바랍니다.

또한 여러분은 코사인 유사성이 두 개의 다른 단어 포함 사이의 유사성을 측정하는 방법이기도 합니다. 우리는 이런 매립된 가스의 속성들과 그것들을 어떻게 사용할 수 있는지에 대해 많은 이야기를 했습니다. 이어서 어떻게 단어를 임베딩 시키는 지에 대해 이야기해 봅시다


### Embedding Matrix

괜찮은 단어 임베딩 학습 문제를 공식화해보겠습니다. 알고리즘 실행을 배울 때 마지막으로 만나게 되는 것이 임베딩 행렬인데요. 정확한 의미를 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/190178482-12bed6d8-810d-4ca5-8391-f2e74acf936d.png)

이전과 같이 만 개의 단어를 사용한다고 가정합시다. 이 만 개에는 a, aaron, orange, zulu와 같은 단어, 또한 알 수 없는 단어가 토큰으로 있습니다. 이제 임베딩 행렬 E를 배워보겠습니다.

이 행렬은 300x10,000차원 행렬로 만 개의 어휘가 있거나 만약 알 수 없는 단어 토큰이 하나 더 있다면 10,001개의 어휘가 있는 경우입니다. 이 행렬의 열은 여러분의 어휘 집합에 있는 서로 다른 만 개의 단어에 대해 상이한 임베딩이 될 겁니다. 따라서 orange는 만 개 중 6257번 단어가 되겠죠.

여기서 우리가 사용할 표기법은 $o_{6257}$로 6257번 위치만 값이 1이고 나머지는 0인 원-핫 벡터를 의미합니다. 그러므로 1이 오직 한 군데에만 있는 10,000차원 벡터가 되죠. 상당히 보기 좋은 규모는 아니죠.

좌측의 임베딩 행렬이 넓은 만큼 이 부분도 길어야 합니다. 임베딩 행렬을 대문자 E로 부르고 E를 원-핫 벡터인 $o_{6257}$과 곱하면 300차원 벡터가 됩니다. E는 300 x 10,000이고 0은 10,000입니다. 이 제품은 300X1이 될 것이고 300차원 벡터와 함께 이 벡터의 첫 번째 요소를 계산한다는 것을 알 수 있습니다.

이 300차원 벡터의 첫 번째 요소를 계산하기 위해서 여러분이 하는 일은 행렬 E의 첫 행을 이와 함께 곱하는 것입니다. 하지만 이 모든 요소들은 0 입니다 요소 6257을 제외하고서 0 곱하기 0으로 끝납니다. 이 0의 숫자, 이 0의 숫자, 기타 등등 그리고 이것이 무엇이든지 간에 1배, 그리고 이 0배, 이 0배, 0배, 기타 등등 그래서 첫 번째 요소를 오렌지 색 기둥 아래에 있는 요소처럼 보이게 됩니다.

그리고 우리가 계산 중인 이 300차원 벡터의 두 번째 요소에 대해서는 벡터 0657을 가지고 행렬 E와 함께 두 번째 행에 곱합니다. 다시 말씀드리지만 이 값은 0배, 이 값은 0배, 이 모든 것의 0배 그리고 이 요소의 1배, 그리고 다른 모든 것의 0배, 그리고 그것을 함께 더합니다. 결국 이 칸으로 끝납니다. 그리고 나머지 칸으로 내려가보면 그래서 포함 행렬 E에 이 핫 벡터를 곱하게 합니다. 이 300차원 기둥은 오렌지라는 단어에 해당하는 것입니다. 이 표식은 $e_{6257}$과 같을 것입니다. 이 표기법은 '오렌지'라는 단어를 1차원 벡터로 300을 포함한 벡터를 나타내는 표식이죠.
