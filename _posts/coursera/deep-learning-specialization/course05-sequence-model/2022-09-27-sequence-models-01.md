---
title: "[Ⅴ. Sequence Models] Sequence Models & Attention Mechanism (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Sequence Models & Attention Mechanism (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Sequence Models & Attention Mechanism

## Sequence to Sequence Models

### Basic models

이번에는 Sequence to Sequence에 대해 이야기 하겠습니다. 이는 기계 번역에서 음성인식까지 모든 것에 유용하죠. 기본 모델로 시작해서 빔 서치 어텐션 모델, 그리고 스피치와 같은 오디오 데이터에 대한 모델 논의를 다룰 것입니다. 시작해 보죠.

#### Sequence to Sequence Model

![image](https://user-images.githubusercontent.com/55765292/192558133-1fa3f261-09c3-4cb3-9263-8f4977297071.png)

여러분이 

> Jane visite I'Afrique Septembre

와 같은 프랑스어를 입력해보고 싶다고 합시다. 그리고 그것을 영어 문장으로 바꾸고 싶다면

> Jane is visiting Africa in September

평소대로 $x^{< 1>}$ 에서 $x^{<5>}$를 사용합니다. 이 경우에 5는 단어와 input 시퀀스를 나타냅니다. 그리고 $y^{<1>}$ 에서 $y^{<6>}$을 사용해 output 시퀀스의 단어를 나타냅니다. 그래서 어떻게 신경망을 훈련시켜 시퀀스 X를 input하고 시퀀스 y를 output 할 수 있을까요?

먼저 인코더 네트워크라고 하는 네트워크를 RNN으로 구축합니다. 그리고 이는 GRU 또는 LSTM이 될 수 있는데, 입력 프랑스 단어를 한 번에 한 단어씩 넣습니다. input 시퀀스 RNN을 수집한 후 input 문장을 나타내는 벡터를 output 합니다.

그러고 나서 디코드된 네트워크를 빌드할 수 있는데요. 여기다 그려 보죠. 이는 왼쪽에 검은색으로 표시된 인코딩 네트워크에 의한 인코딩 output을 input을 해서 인코더 네트워크에 의해 한 번에 한 단어씩 번역을 output 하도록 시킬 수 있습니다.

궁극적으로 이는 시퀀스의 끝과 디코더가 멈추는 문장을 말할 수 있게 해줍니다. 그리고 평소처럼 생성기 토큰을 가져다 닉스에게 공급하면 언어 모델을 사용해 기술을 합성할 때 이전과 같은 순서를 유지할 수 있습니다.

딥 러닝에서 가장 주목할 만한 최근 결과들 중 하나가 바로 이 모델이 작동한다는 것입니다. 충분한 프랑스어 문장과 영어 문장들을 주어서 모델을 프랑스어 문장을 input하고 그에 상응하는 영어 번역을 output 하도록 훈련시키면 이것은 실제로 잘 작동됩니다.

이 모델은 단순히 input이 된 프랑스어 문장의 인코딩을 찾는 인코딩 네트워크를 사용합니다. 그리고 디코딩 네트워크를 사용해 상응하는 영어 번역을 생성하죠.

#### Image Captioning

![image](https://user-images.githubusercontent.com/55765292/192558322-80b39e9a-91da-4b95-be99-2ada8640bfa9.png)

이와 매우 유사한 구조 또한 이미지 캡션image captiong에도 작동됩니다. 여기 보이는 것과 같은 이미지를 볼 때 자동적으로

> A cat sitting on a chair

로 자막을 달아지게 하려 합니다. 그러면 어떻게 네트워크에서 이미지를 input하고 위에 있는 저런 자막을 output 하도록 훈련시킬까요?

자신감에 대한 이전 호출에서 여러분은 이미지를 컨볼루션 네트워크로 미리 훈련된 AlexNet에 input 하는 방법을 살펴 보았으며 그리고 input 이미지의 기능에 학습자를 인코딩하는 방법을 배웠습니다. 그래서 이는 실제로 AlexNet 구조입니다. 그리고 만약 우리가 이 마지막 소프트맥스 단위를 제거하면 자유 무역 AlexNet은 고양이 이미지를 나타낼 수 있는 4,096차원 특성벡터를 제공할 수 있습니다.

이 사전 훈련된 네트워크는 이미지에 대한 인코딩된 네트워크가 될 수 있으며 여러분은 이제 이미지를 나타내는 4,096차원 벡터가 있습니다. 그런 다음 여러분은 이것을 가지고 한 번에 한 단어씩 자막을 생성하는 RNN에게 줄 수 있습니다.

우리가 본 프랑스에서 영어로 번역하는 기계 번역과 비슷하게 여러분은 이제 input을 설명하는 특성벡터를 input할 수 있습니다. 그리고 단어의 output 세트를 한 번에 한 단어씩 생성할 수 있죠.

그리고 이것은 이미지 캡션을 하는데 효과적입니다. 특히 여러분이 생성하고자 하는 자막이 너무 길지 않을 때 말이죠. 이제 여러분은 기본 시퀀스 대 시퀀스 모델 작동 방법을 보셨습니다.

기본 이미지에서 시퀀스로 혹은 이미지 캡셔닝 모델 작동 방법을 살펴 보셨죠 하지만 이런 모델을 실행하는 방법에는 몇 가지 차이점이 있는데요. 언어 모델을 사용해 소설 텍스트를 합성하는 방법과 비교해서 시퀀스를 생성하는 것이죠.

주요 차이점 중 하나는 번역에서 무작위로 선택하지 않아도 되는 것입니다. 여러분은 가장 가능성이 높은 번역을 원할 수도 임의로 선택한 캡션을 원하지 않을 수도 있지만 최상의 캡션과 가장 가능성이 높은 캡션을 원합니다. 그래서 그것을 어떻게 생성하는지 살펴보겠습니다.

### Picking the Most Likely Sentence

시퀀스 기계 번역 모델과 이 강좌의 지금까지 다룬 언어 모델은 몇 가지 비슷한 점이 있지만 몇 가지 중요한 차이점도 있습니다. 한번 보시죠.

#### Machine Translation as Building a Conditional Language Model

![image](https://user-images.githubusercontent.com/55765292/192560465-6170eac5-42cb-4ee2-a96b-af1f1122d817.png)

기계 번역을 조건부 언어 모델을 만드는 것이라고 생각할 수 있습니다. 언어 모델링에서 제가 말하고자 하는 것은 이것이 처음 만든 네트워크이고 이 모델은 한 문장의 확률을 추정합니다. 그것이 언어 모델이 하는 일입니다.

그리고 여러분은 이것을 사용하여 새로운 문장을 만들 수도 있고 때로는 여기 $x^{<1>}$과 $x^{<2>}$를 쓸 때, 이 예에서 $x^{<2>}$는 $\hat{y}^{<1>}$과 같거나 피드백일 수도 있습니다. 그러나 $x^{<1>},x^{<2>}$ 등은 중요하지 않았습니다. $x^{<1>}$은 영백터일 수 있으며 $x^{<2>},x^{<3>}$은 생성 중인 이전 출력일 뿐입니다. 이것이 언어 모델이었습니다.

기계 번역 모델은 다음과 같습니다. 저는 두 가지 다른 색깔을 사용하여 각각 코딩된 네트워크를 초록색, 디코딩된 네트워크를 보라색으로 표시하겠습니다. 그리고 디코딩된 네트워크는 저기 위에서 보았던 언어 모델과 상당히 비슷하다는 것을 알 수 있습니다.

기계 변역 모델은 언어 모델과 매우 유사한데 단, 항상 영벡터와 함께 시작하는 대신 입력 문장의 표현을 파악하는 인코딩된 네트워크가 있으며 해당 입력 문장을 사용하여 디코딩된 네트워크를 시작하는데 전부 0인 표현이 아니라 입력 문장의 표현으로 시작합니다.

그래서 저는 이것을 조건부 언어 모델이라고 부르며 어떤 문장의 확률을 모델링하는 대신 입력한 어떤 프랑스어문장에 대한 영어 번역 출력의 확률을 모델링하고 있습니다.

즉, 영어 번역의 확률을 예측하려고 하는 것입니다. 예를 들어, 번역이

> Jane is visiting Africa in September

라면 입력된 프랑스어에 대한 조건은

> Jane visite I"Afrique en septembre

일 가능성이 있습니다. 따라서 이것은 입력된 프랑스어 문장을 조건으로 하는 영어 문장의 확률이며 이것이 조건부 언어 모델인 이유입니다.

#### Finding the Most Likely Translation

![image](https://user-images.githubusercontent.com/55765292/192560356-68b9002a-d42f-4447-97a8-ec6df4a2a94c.png)

자, 만약 여러분이 실제로 이 모델을 적용해서 프랑스어 문장을 영어로 번역하려면 이 프랑스어 문장을 입력했을 때 모델은 여러분에게 해당하는 영어 번역이 다를 확률을 알려줄 것입니다. x는 프랑스어 문장인

> Jane visite l'Afrique en septembre

입니다. 그리고 이것은 프랑스어의 입력에 대한 다른 영어 번역의 확률을 알려줍니다. 여러분은 출력을 무작위로 샘플링하고 싶지 않을 겁니다. 만약 여러분이 이 분포를 통해 단어를 샘플링한다면 주어진 x에 대한 y의 확률인, 아마 한 번은 꽤 괜찮은 번역인

> Jane is visiting Africa in September

를 얻습니다. 하지만 아마 다른 번역인

> Jane is going to be visiting Africa in September

를 얻을 수도 있습니다. 조금 어색하게 들리지만 끔찍한 번역은 아닙니다. 최상의 번역이 아닐 뿐이죠. 그리고 가끔, 아주 우연히 예를 들어 다른 문장인

> In September Jane will visit Africa

를 얻게 될 겁니다. 그리고 아마 우연히, 가끔은 정말 나쁜 번역인

>Her African friend welcomed Jane in September

를 얻을 수도 있습니다. 따라서 기계 번역에 이 모델을 사용할 경우 이 분포에서 무작위로 샘플링하려고 하지 않습니다. 대신 조건부 확률을 극대화하는 영어 문장 y를 찾길 원합니다.

따라서 기계 번역 시스템을 개발하는데 있어서 해야 할 일 중 하나는 여기 이 항을 최대화하는 y값을 실제로 찾을 수 있는 알고리즘을 고안하는 것입니다. 이를 위한 가장 흔한 알고리즘은 빔 탐색이라고 하며 이것은 다음에 다루겠습니다.

#### Why not a Greedy Search

![image](https://user-images.githubusercontent.com/55765292/192560562-acd28c7c-389c-43d0-a3d7-9fb2c1521c0e.png)

빔 탐색을 설명하기 전에, 왜 **탐욕적 탐색greedy search**을 이용하면 안될까요? **탐욕적 탐색**이란 컴퓨터 공학의 알고리즘으로, 조건부 언어 모델에 따라 가장 가능성이 높은 첫 번째 단어를 선택하여 첫 번째 단어를 생성합니다.

기계 번역 모델로 이동한 다음 첫 번째 단어를 선택한 후, 가장 가능성이 있는 두 번째 단어를 고르고 그리고 가장 가능성이 있는 세 번째 단어를 고릅니다. 이 알고리즘을 탐욕적 탐색이라고 하며, 그리고 여러분이 정말로 원하는 것은 $y^{<1>}, y^{<2>}, y^{< T_y>}$까지의 전체 단어 시퀀스를 선택하는 것입니다. 전체 단어의 동시 확률을 극대화시킵니다.

그리고 탐욕적 탐색법은 그저 가장 좋은 첫 단어를 선택한 다음 가장 좋은 첫 단어 후에 가장 좋은 두 번째 단어를 고르고 그리고 그 이후에 가장 좋은 세 번째 단어를 고르는 것으로 사실 효과가 없는 접근법입니다.

이를 증명하기 위해, 다음 두 가지 번역을 고려해 봅시다. 첫 번째 것은 더 나은 번역으로, 우리의 기계 번역 모델에서 첫 번째 문장에서 P(y|x)가 더 높길 바랍니다. 이것은 프랑스어 입력에 대한 좀 더 쉽고 간결한 번역이 됩니다.

두 번째는 나쁜 번역은 아니며 약간 더 장황하고 불필요한 단어가 더 많습니다. 하지만 알고리즘이 첫 두 단어로 "Jane is"를 선택했다면, "going"이 보다 일반적인 영어 단어이기 때문에 주어진 프랑스어 문장에 대한 "Jane is going"의 확률이 "Jane is visiting"의 확률보다 더 높아질 수 있습니다.

따라서 첫 세 단어의 확률을 최대화하는 것을 기반으로 세 번째 단어를 고르면 결국 두 번째 옵션을 고르게 될 가능성이 꽤 높습니다. 결과적으로 덜 최적인 문장을 만들게 됩니다. 이 모델 p(y|x)로 측정된 덜 좋은 문장으로 말이죠.

중요한 부분을 놓치게 되는 논쟁일 수 있겠지만 이것은 보다 넓은 현상의 한 예로써, 만일 확률을 극대화하는 y_1, y_2, 마지막 단어까지의 시퀀스를 찾기를 원한다면 한 번에 하나의 단어를 선택하는 것이 항상 최적인 것은 아닙니다.

물론, 영어 문장에서 단어 조합의 총 수는 기하급수적으로 큽니다. 만약 여러분의 사전에 단어가 1만개밖에 없고 최대 10개 단어 길이까지의 번역을 고려하고 있다고 해도 가능한 10개 단어 길이의 문장은 1만에 10제곱을 한 만큼 있습니다. 어휘 집합 크기를 기준으로 단어를 고르면 사전 크기는 1만 단어입니다.

가능한 문장의 공간이 엄청나게 거대하고 그것들을 전부 다 평가하기는 불가능합니다. 가장 일반적인 방법으로 근사 검색 알고리즘을 사용하는 이유입니다. 근사 검색 알고리즘이 하는 것은 항상 성공하지는 않지만, 조건부 확률을 극대화하는 문장 y를 선택하는 것입니다. 이를 최대화하는 y값을 찾는 것이 보장되지 않더라도 대체로 충분합니다.

요약하자면, 기계 번역이 어떻게 조건부 언어 모델링 문제로 제기되는지 보셨을 겁니다. 하지만 이 문제와 초기 언어 모델링 문제의 주요한 차이점은 문장을 무작위로 생성하기보다는 가장 가능성이 높은 영어 문장, 가장 가능성이 높은 영어 번역을 찾으려고 할 수 있다는 것입니다.

하지만 특정 길이의 모든 영어 문장은 너무 커서 다 열거할 수 없습니다. 따라서 우리는 검색 알고리즘에 의존해야 합니다. 다음은 빔 탐색 알고리즘을 배워 보겠습니다.
