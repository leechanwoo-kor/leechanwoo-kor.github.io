---
title: "[Ⅴ. Sequence Models] Transformer Network (2)"
categories:
  - Deep Learning Specialization
  - Transformer Network
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Attention Models
  - Transformer Network
  - Transformers
  - Self Attention
  - Multi Head Attention
toc: true
toc_sticky: true
toc_label: "Transformer Network (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Transformer Network

## Transformers

### Transformers

셀프 어텐션과 멀티 헤드 어텐션에 대해서 배웠습니다. 이걸 모두 합쳐서 트랜스포머 네트워크를 만들겠습니다.

<br>

![image](https://user-images.githubusercontent.com/55765292/194745779-ca56dad2-bc7a-48bc-b0bf-5b4b432fe2e6.png)

어텐션 메커니즘들을 어떻게 짝지어서 **트랜스포머transformer** 아키텍처를 만드는지 배우게 될 겁니다.

> Jane visite l'Afrique en septembre

이 문장으로 다시 한 번 시작해 봅시다. 이에 상응하는 임베딩도 함께 사용합니다. 이 문장을 프랑스어에서 영어로 바꾸는 방법에 대해 알아보겠습니다.

> \<SOS> $x^{<1>}$ $x^{<2>}$ $\dots$ $x^{< T_x-1>}$ $x^{< T_x>}$ \<EOS>

여기에 문장의 시작과 끝을 나타내는 토큰을 추가했습니다. 지금까지는 단순하게 하기 위해 문장 속에 있는 단어들의 임베딩에 대해서만 이야기했습니다. 시퀀스 번역 작업에서는 많은 시퀀스에 이 예시에 보이는 문장의 시작 부분 **SOS** 문장 끝 부분의 **EOS** 토큰 같은 것들이 유용하게 쓰일 겁니다.

트랜스포머의 첫 번째 단계는 이 임베딩들을 멀티 헤드 어텐션을 보유한 인코더 블럭에 삽입하는 것입니다. 이 부분은 지난 번에 보았던 $Q, K, V$ 값들을 임베딩과 다중 행렬 $W$에서 계산하여 입력한 것입니다. 그러면 이 레이어는 피드 포워드 신경망으로 전달될 수 있는 행렬을 만듭니다. 문장에 어떤 흥미로운 특징이 있는지를 결정하게끔 도와 주는 것입니다.

트랜스포머 논문에서 이 인코딩 블럭은 N번 반복되고 N의 일반적인 값은 6입니다. 그러니까 이 블럭을 6번 정도 돈 후에 인코더의 출력을 디코더 블럭에 넣을 겁니다. 디코더 블럭을 만들어 봅시다.

디코더 블럭은 영어 번역을 출력하는 일을 합니다. 그러니까 처음에는 문장의 시작 토큰을 출력하게 될 겁니다. 디코더 블럭은 단계별로 처음의 몇 단어를 번역을 통해 우리가 만든 것은 무엇이든 출력한 겁니다.

작업을 시작하면서 우리가 알 수 있는 것은 번역이 문장의 시작 토큰부터 시작될 것이라는 것 뿐입니다. 문장의 시작 토큰을 이 멀티 헤드 어텐션 블럭에 입력합니다. 그리고 이 SOS 토큰 하나가 $Q, K, V$를 멀티 헤드 어텐션 블럭에서 계산하여 문장을 시작하는 데에 사용됩니다.

이 첫 번째 블럭이 출력한 것은 다음 멀티 헤드 어텐션 블럭의 $Q$ 행렬을 만드는 데에 사용됩니다. 그리고 인코더가 출력한 것은 $K$와 $V$를 만드는 데에 사용됩니다.

이제 여기에 두 번째 멀티 헤드 어텐션 블럭이 있습니다. 저번처럼 $Q, K, V$를 입력한 것입니다. 왜 이런 식으로 구성되는 것일까요? 이런 식으로 생각하시면 도움이 되실 겁니다.

이 아래쪽의 입력값들은 지금까지 문장에서 번역한 것입니다. 그러니까 이것은 무엇이 문장의 시작인지에 관한 쿼리가 될 것입니다. 그리고 $K$와 $V$로부터 문맥을 가져오게 될 겁니다. 이는 프랑스어 버전의 문장에서 번역된 것입니다. 그리고 나서는 시퀀스 내에서 다음으로 생성할 단어를 결정합니다.

디코더 블럭의 설명을 마치기 위해 멀티 헤드 어텐션은 신경망의 피드에 공급되는 값을 출력합니다 이 디코더 블럭 역시 N번 반복될 것이고 출력값을 받는 동안 6번 정도 반복하면서 그것을 다시 입력하게 되고 약 12번 정도 진행하게 됩니다.

이 새로운 네트워크의 역할은 문장의 다음 단어를 예측하는 것입니다. 여기서 영어 번역문의 첫 번째 단어가 Jane으로 결정하길 바랍니다. 그러면 Jane을 또 입력으로 보내는 겁니다. 이제 다음 쿼리는 SOS와 Jane으로부터 오고 Jane이 있으니 가장 적절한 단어는 무엇일지 보는 겁니다. 가장 적절한 키와 값을 찾아서 가장 알맞은 다음 단어를 만드는데 그것이 Visite가 되게끔 하는 겁니다.

그리고 이 신경망을 다시 반복해서 Africa를 출력하고 그러면 Africa를 다시 입력하고 in을 출력하면, 이 입력값에서 September를 출력하고 문장이 끝났다는 토큰을 출력하고 나면 끝나는 겁니다.

이 인코더 블럭과 디코더 블럭이 어떻게 시퀀스 번역을 위해 결합하는지가 트랜스포머 아키텍처의 중점입니다. 이 경우, 여러분은 입력한 문장을 어떻게 다른 언어의 문장으로 번역하는지를 봄으로써 어텐션과 네트워크가 동시 계산을 할 수 있게끔 결합되는지 본 것입니다.

하지만 이러한 중점을 넘어서 이것들을 변환하기 위한 몇 가지 추가적인 사항이 있습니다. 트랜스포머 네트워크를 더 좋게 만드는 추가 사항들을 간단히 보여드리겠습니다.

첫 번째는 입력값의 위치를 인코딩하는 것입니다. 셀프 어텐션 방정식을 생각해보면 단어의 위치를 나타내는 것이 아무것도 없었습니다. 이 단어가 문장의 첫 번째 단어인지 중간에 있는 것인지 문장의 마지막 단어인지 말이죠. 문장 내에서 단어의 위치는 번역에 있어서 매우 중요할 수 있습니다.

입력값에 있는 요소의 위치를 인코딩하는 방법은 사인 및 코사인 방정식을 결합하는 것입니다. 예를 들어 단어 임베딩이 값을 네 개 갖고 있는 벡터라고 합시다. 이 경우 단어 임베딩의 디멘션은 4가 될 겁니다.

따라서 $x^{<1>}$는 총 4개의 디멘션 벡터라고 합시다. 이 예에서 여러분은 같은 디멘션에서 벡터의 위치 임베딩을 만들 것입니다. 이 역시 4개의 디멘션일 것입니다. 그리고 이 위치 임베딩을 $p^{<1>}$이라고 하겠습니다. 첫 번째 단어인 Jane의 위치 임베딩입니다.

$PE_{(pos,2i)} = sin(\dfrac{pos}{1000^{\frac{2i}{d}}})$

$PE_{(pos,2i+1)} = cos(\dfrac{pos}{1000^{\frac{2i}{d}}})$

이 방정식에서 $pos$, 즉 위치는 숫자로 나타낸 단어의 위치를 말합니다. 그러니까 Jane이라는 단어의 $pos$는 1과 같고 여기에 있는 $i$는 인코딩의 다른 디멘션을 나타냅니다.

$i$에 해당하는 첫 번째 요소는 0입니다. 이 $i$ 요소는 1, 이 $i$는 2, 이 $i$는 3입니다. 이것들이 변수들이고 $pos$와 $i$가 $pos$가 단어의 위치를 나타내는 이 아래쪽 방정식에 들어갑니다. $i$는 0부터 3으로 가고 요소의 디멘션으로 4와 같습니다.

위치 인코딩에서 사인과 코사인은 독특한 위치 인코딩 벡터를 만듭니다. 이러한 벡터들 중 하나가 각각의 단어와 대응합니다. 그러니까 l'afrique라는 세 번째 단어의 위치를 인코딩하는 벡터 $p^{<3>}$는 4개의 값일 겁니다.

이는 첫 번째 단어인 Jane의 인코딩에 쓰인 4개의 값과는 다른 값일 겁니다. 사인과 코사인 커브는 이렇게 생겼습니다. 이 용어들과 분모 이미 알고 있기 때문에 $i$는 0이라는 걸 알 수 있습니다. 이렇게 생긴 사인 곡선이 있겠죠 $i=1$은 코사인과 일치합니다. 9도에서 일치합니다.

$i=2$는 좀 더 낮은 진폭의 이런 그래프가 될 겁니다. $i=3$는 이와 상응하는 코사인 곡선입니다. 그러니까 $p^{<1>}$, 즉 첫 번째 위치에서 여기에 있는 4개의 값을 채우는 위치를 읽습니다. 서로 다른 위치에 있는 각각의 단어에 대해서 가로축이 3일 경우 다른 값을 얻게 됩니다.

여기서 앞의 두 개의 값이 거의 비슷한 높이에 있기 때문에 아주 비슷할 수 있습니다. 하지만 이렇게 여러 개의 사인과 코사인을 이용함으로써 네 개의 값을 모두 보게 되면 $p^{<3>}$는 $p^{<1>}$과 달라질 겁니다.

$p^{<1>}$의 위치 인코딩은 곧장 $x^{<1>}$과 연결되어 이런 식으로 입력됩니다. 각각의 단어의 벡터들이 영항을 받아 문장에서 단어가 어디에 위치하는지 나타냅니다. 인코딩 블럭의 출력값은 문맥상의 의미적 임베딩과 위치 인코딩 정보를 포함합니다.

임베딩 레이어의 출력은 d인데 이 경우 여러분이 얻을 수 있는 시퀀스의 최대 길이인 4입니다. 이 모든 레이어들의 출력값들은 이러한 형태를 갖습니다. 임베딩에 이러한 위치 인코딩을 더하는 것 외에도 이를 잔류 연결이 있는 네트워크로 전달할 수 있습니다.

이러한 잔류 연결은 이전에 보았던 것과 유사한데 이들의 목적은 위치 정보를 전체 아키텍처에 전달하는 것입니다. 위치 인코딩에 더해서 트랜스포머 네트워크는 Bash 표준과 매우 흡사한 레이어를 사용합니다. 이 경우 그 목적은 위치 정보를 위치 인코딩에 전달하는 것입니다.

트랜스포머는 또한 Add & Norm이라는 레이어를 사용합니다. Bash 표준과 거의 비슷한 역할을 하면서 학습 속도를 높이는 데에 도움을 준다고 생각하세요. 이 Bash 표준 같은 레이어 즉 Add & Norm 레이어는 이 아키텍처 전체적으로 반복됩니다.

마지막으로 디코더 블럭의 출력값으로는 선형 및 softmax 레이어가 있는데 한 번에 한 단어씩 다음 단어를 예측합니다. 트랜스포머 네트워크에 관련된 논문을 읽을 경우 마스크 멀티 헤드 어텐션이라는 말을 볼 수도 있습니다. 여기다가 그려보겠습니다.

마스크 멀티 헤드 어텐션은 훈련 과정에서만 중요합니다. 정확한 프랑스어를 영어로 번역하는 데이터 세트를 사용하여 트랜스포머를 훈련시킬 때 말입니다. 앞서 우리는 트랜스포머가 어떻게 한 번에 한 단어씩 예측을 하는지 배웠습니다만 학습은 어떻게 할까요?

여러분의 데이터가 프랑스어를 영어로 정확하게 번역한 것이라고 합시다. Jane visite l'afrique en septembre와 Jane visits Africa in September입니다. 트레이닝 시 여러분은 정확한 영어 번역을 알고 있고 정확한 출력과 입력값을 알고 있습니다. 정확한 전체 출력값을 알고 있기 때문에 트레이닝에는 한 번에 한 단어씩 만들 필요가 없습니다.

대신에 마스킹은 문장의 마지막 문장이 테스트 타임이나 예측 시간에 해야 할 일을 하지 못하게끔 막습니다. 다시 말해, 마스크 멀티 헤드 어텐션은 반복적으로 네트워크가 완벽하게 번역된 척을 하는 것입니다. 초반의 몇 단어만 두고 나머지 단어들은 첫 부분의 완벽한 번역이 주어진다면 신경망이 시퀀스의 다음 단어를 정확하게 예측할 수 있는지 보는 것입니다.

여기까지가 트랜스포머 아키텍처를 요약한 것입니다. 여러분이 원하는 것이 페이퍼 어텐션이므로 이 모델의 여러 가지 새로운 버전이 있을 수 있습니다. 이제 트랜스포머 네트워크의 주요 빌딩 블럭에 대해 이해하게 되셨을 겁니다.
