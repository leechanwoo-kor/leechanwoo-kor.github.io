---
title: "[Ⅴ. Sequence Models] Recurrent Neural Networks (4)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Recurrent Neural Networks (4)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Recurrent Neural Networks

## Recurrent Neural Networks

### Sampling Novel Sequences

시퀀스 모델을 교육한 후, 회화체에서는 학습 내용을 알 수 있는 방법 중 하나는 샘플 소설 시퀀스를 갖는 것입니다 어떻게 하는지 한번 보겠습니다

#### Sampling a Sequence from a Trained RNN

![image](https://user-images.githubusercontent.com/55765292/188296753-f1ea513f-7169-444c-815e-c5eaf09b7627.png)

시퀀스 모델은 다음과 같은 특정 단어의 시퀀스를 모델링한다는 것을 기억하십시오. 그래서 우리는 이 배포에서 훌륭한 단어의 시퀀스를 생성하는 것을 좋아합니다. 이 네트워크는 위에서 보여지는 이 구조를 사용하여 훈련되었습니다. 하지만 샘플링하기 위해서 약간 다른 것을 합니다.

그래서 여러분이 하고자 하는 것은 여러분의 모델이 만들어주기를 바라는 첫 번째 단어의 첫번째 샘플입니다. 따라서 일반적인 $x^{<1>}$을 0과 같고 $a^{<0>}$은 0과 같습니다. 그리고 이제 첫 번째 타임스탬프는 가능한 출력보다 최대 가능성을 갖게 됩니다.

그래서 여러분이 하는 것은 무작위로 이 소프트 맥스 분포에 따라 표본을 매기는 것입니다. 그래서 소프트 맥스 분포에서 알 수 있는 것은 이것이 이것을 "aaron" 이라고 부를 수 있는 확률이 얼마나 되는지 알려준다는 것입니다. "zulu" 를 지칭할 수 있는 확률은 무엇일까요?

첫번째 단어가 알 수 없는 단어 토큰일 가능성이 높습니다. 어쩌면 그것은 문장의 끝이 될 수도 있었습니다. 그런 다음 이 벡터를 사용하여 예를 들어, numpy 명령 `np.random.choice`를 사용하여 이 벡터 확률에 의해 정의된 분포에 따라 샘플링하고 첫 번째 단어를 샘플링할 수 있습니다. 그런 다음 두 번째 단계로 넘어갑니다.

이제 두 번째 단계에서 이 $y^{<1>}$을 입력으로 사용해야 한다는 것을 기억하십시오. 하지만 여러분이 하는 일은 방금 샘플링한 $\hat{y}^{<1>}$를 가져다 다음 시간 단계에서 입력으로 여기에 전달합니다.

어쨌든 첫 번째 단계가 이 입력 내용을 두 번째 위치로 전달하는 것을 선택하면 이 소프트 맥스가 $\hat{y}^{<2>}$에 대해 예측을 합니다.

예를 들어, 첫 번째 단어를 샘플링한 후에 첫 번째 단어는 "the" 였습니다 첫 번째 단어는 "the"로 보통 흔하게 쓰는 단어죠. 그런 다음 "the"는 $x^{<2>}$로 $\hat{y}^{<1>}$과 같습니다. 그리고 이제 여러분은 두 번째 단어가 첫 번째 단어가 "the" 라는 두 번째 단어를 찾아내야 합니다. 그리고 이것은 $\hat{y}^{<2>}$입니다.

그런 다음 이 유형의 샘플링 함수를 사용하여 $\hat{y}^{<2>}$를 샘플링합니다. 그리고 다음 번에 스탬프가 되면 여러분이 말했던 것을 하나의 하드 인코딩으로 바꿉니다. 그리고 다음 시간대로 전달하면 그 다음에 여러분이 무엇을 선택하든 간에 세 번째 단어를 샘플링할 수 있습니다.

그리고 마지막 단계가 될 때까지 계속 진행하세요. 그럼 이 순서가 언제 끝나는지 어떻게 아나요? 자, 한 가지 할 수 있는 것은 문장 토큰의 끝이 여러분의 어휘의 일부라면 EOS 토큰을 생성할 때까지 샘플링을 계속할 수 있습니다.

그리고 그것은 여러분이 문장의 끝을 맞았다는 것을 의미하고 여러분은 멈출 수 있습니다. 또는, 만약 이것을 여러분의 어휘에 포함시키지 않는다면 여러분은 단지 20개의 단어나 100개의 단어나 다른 단어들을 표본하고 그 몇 단계에 도달할 때까지 계속하기로 결정할 수도 있습니다.

그리고 이 특별한 절차는 때때로 알 수 없는 단어 토큰을 생성합니다. 알고리즘이 이 이 토큰을 생성하지 못하도록 하려면 한가지 할 수 있는 것은 알 수 없는 단어 토큰으로 나온 모든 표본을 거절하고 알 수 없는 단어가 아닌 단어를 얻을 때까지 나머지 어휘와 리샘플링을 유지하는 것입니다.

혹은 알 수 없는 단어 출력을 원치 않으면 출력물에 넣을 수도 있습니다. 이것이 RNN 언어 모델에서 무작위로 선택된 문장을 생성하는 방법입니다. 지금까지 저희는 단어 수준의 RNN을 만들어 왔습니다. 제 말은 영어로 된 단어들입니다.

#### Character-level Language Model

![image](https://user-images.githubusercontent.com/55765292/188296761-4a6b9a0c-98a1-4ece-af25-1398171489ad.png)

응용 프로그램에 따라 문자 수준 RNN도 만들 수 있습니다. 이 경우 여러분의 어휘는 그저 알파벳일 뿐입니다. 0에서 9까지의 숫자 구두점을 원하는 경우 0부터 9까지입니다. 그리고 대문자와 소문자를 구분하려면 대문자 알파벳도 포함할 수 있습니다.

한 가지 방법으로는 교육 세트를 보고 여기에 나타나는 문자를 보고 어휘 정의를 위해 사용할 수 있습니다. 그리고 단어 수준 언어 모델이 아닌 문자 수준 언어 모델을 빌드하면 교육 데이터의 개별 단어가 아닌 교육 데이터의 개별 문자가 됩니다.

그래서 이전의 예를 들자면, 문장 고양이는 하루 평균 15시간의 수면을 취합니다. 이 예에서 고양이cat의 c는 $y^{<1>}$, a는 $y^{<2>}$, t는 $y^{<3>}$, 공백은 $y^{<4>}$ 등으로 설정됩니다.

문자 수준 언어 모델을 사용하면 몇 가지 장단점이 있습니다. 하나는 알 수 없는 단어 토큰에 대해 걱정할 필요가 없다는 것입니다. 특히 문자 수준 언어 모델은 0이 아닌 확률인 mau와 같은 시퀀스를 지정할 수 있습니다. 반면 mau가 단어 수준 언어 모델에 대한 어휘에 없는 경우에는 알 수 없는 단어 토큰을 지정해야 합니다.

하지만 문자 수준 언어 모델의 가장 큰 단점은 훨씬 더 긴 배열로 끝난다는 것입니다. 많은 영어 문장들은 10에서 20개의 단어를 가질 수 있지만 많은 수십개의 글자가 있을 수 있습니다. 따라서 문자 언어 모델은 문장의 초기 부분이 문장의 뒷부분에도 어떻게 영향을 주는지 사이의 긴 범위의 종속성을 캡처할 때 단어 수준 언어 모델만큼 좋지 않습니다.

그리고 캐릭터 수준 모델은 훈련하는데 더 많은 계산 비용이 듭니다. 그래서 자연어 처리에서 제가 본 경향은 대부분 워드 수준 언어 모델이 여전히 사용되고 있지만 컴퓨터가 점점 더 빨라지면서 사람들이 있는 응용 프로그램들이 더 많아진다는 것입니다.

적어도 일부 특수한 경우에는 더 많은 문자 레벨 모델을 보기 시작합니다. 하지만, 그것들은 하드웨어이거나 훨씬 더 많은 계산 비용이 드는 경향이 있어서 오늘날 널리 사용되고 있지 않습니다. 알 수 없는 단어나 다른 단어들을 많이 다룰 필요가 있을 수 있는 특별한 응용 프로그램만 빼고 말이죠. 또는 좀 더 전문화된 어휘를 가지고 있는 더 전문적인 응용 프로그램에도 사용됩니다.

그래서 이런 방법들 하에서는 RNN을 만들어 영어 텍스트의 목적을 알아보고 단어 수준을 만들고 언어 모델을 만들고 학습한 언어 모델을 샘플링할 수 있습니다.

#### Sequence Generation

![image](https://user-images.githubusercontent.com/55765292/188296765-962e86af-9c79-481e-be6e-020f6e278acf.png)

이것은 언어 모델의 사례입니다. 사실 문화 수준의 언어 모델에서요. 그리고 이런 것을 실제로 구현할 수 있습니다.

만약 그 모델이 뉴스 기사에 훈련된다면 왼쪽에 보이는 그런 텍스트를 생성합니다. 이 결과는 어렴풋이 뉴스 텍스트처럼 보이거나 문법적이라기보다는 어쩌면 마치 나타나고 있는 뉴스처럼 들릴수도 있고 뇌진탕으로 인한 전염병을 진단할 수도 있습니다.

그리고 세익스피어의 책에 의해 훈련되었고 세익스피어가 쓴 것 같은 것들을 만들어 냅니다. 이 인간적인 달이 그녀의 일식을 사랑에서 빼앗는다. 그리고 이 작품의 주제는 이 것과 같은 것을 또 다른 것으로 그리지요. 베서가 나에게 사블을 볼 때 나의 눈을 휘젓는 것이 그 사람의 것이다. 기본 RNN을 위한 것입니다.

그리고 이를 통해 언어 모델을 만들 수 있는 방법과 교육받은 언어 모델의 표본을 얻을 수 있습니다. 이어서 RNN을 교육하는 몇 가지 과제와 RNN의 보다 강력한 모델을 개발함으로써 특히 소실되는 문제들을 조정하는 방법에 대해 이야기하고자 합니다.


### Vanishing Gradients with RNNs

RNN의 작동 방식 및 이름 엔티티 인식과 같은 문제와 언어 모델링에 적용하는 방법에 대해 배웠습니다. 여러분은 역전파를 사용한 RNN을 훈련 방법을 살펴보았습니다. 기본 RNN의 알고리즘의 문제 중 하나는 기울기 값이 소멸된다는 점인데요. 그것에 대해 얘기해 보고 이 문제를 해결할 방법을 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/188297274-a0d88274-69ee-44ae-8ac6-3a2af9fee842.png)

이것과 같은 RNN의 그림을 보셨을 겁니다. 언어 모델링 예제를 들어보죠. 이 문장을 봅시다.

> The cat, which already ate ..., was full

일관성을 유지하려면, cat이 단수형이기 때문에 was여야 하죠. 

> The cats, which already ate a ..., were full

그래서 일관성을 위해서는 cat was나 cats were이어야 합니다. 이것이 언어가 매우 장기적인 의존성을 가질 수 있는 예제 중 하나로 앞서 쓰여진 단어가 언어가 문장 후반부의 어떤 것이 되어야 하는지에 영향을 미칠 수 있습니다.

하지만 우리가 지금까지 살펴본 기본 RNN은 의존성을 포착하는 데는 그다지 효과적이지 않습니다. 이유를 설명하자면 매우 깊은 신경망 훈련에 대한 이전에 기울기 소멸 문제에 대해 이야기했던 것을 기억하실 겁니다.

이는 매우 매우 깊은 신경망으로 100 혹은 훨씬 더 깊습니다. 왼쪽에서 오른쪽으로 순전파를 수행한 다음 역전파를 수행합니다. 우리는 만약 이것이 매우 깊은 신경망이라면 이 output y의 기울기는 이 초기 레이어의 가중치 계산에 영향을 주기 위해 역전파를 하는데 매우 힘들 것이라 생각했습니다.

비슷한 문제를 가진 RNN의 경우 왼쪽에서 오른쪽으로 가는 순전파와 오른쪽에서 왼쪽으로 가는 역전파가 있습니다. 이는 이전 계산에 영향을 미치는 나중 시간 단계와 관련된 오류의 output에 대한 동일한 기울기 소멸 문제 때문에 꽤 어려울 수 있습니다.

실제로 이는 신경망이 암기해야 한다는 것을 깨닫는 것이 어렵다는 것을 의미하죠. 단수 명사나 복수 명사를 봤다면 시퀀스에서 단수형인지 복수형인지에 따라 was나 were을 만들어내기 위해 외워야 한다는 것입니다.

영어에서는 중간에 있는 이런 것이 임의로 길 수 있습니다. 그래서 여러분은 그 정보를 사용하기 전에 단수 복수형을 아주 오랫동안 외워야할 수 있습니다. 이러 문제들로 인해 기본 RNN 모델은 많은 로컬 영향을 받습니다. 이는 output $\hat{y}^{<3>}$은 주로 $\hat{y}^{<3>}$와 가까운 값에 영향을 받고 여기 있는 값은 주로 다소 가까운 input에 영향을 받는다는 것을 의미합니다.

뒤에 있는 output이 매우 초기 시퀀스의 input에 영향을 강하게 받기가 어렵습니다. 이는 output이 무엇이든 간에 이건 잘못 되었든 간에 이 지역이 배율을 오류가 시퀀스의 시작으로 역전파되는 것은 매우 어렵기 때문입니다. 따라서 시퀀스 초기에 신경망이 계산을 하는 방법을 수정하는 것은 어렵습니다.

그래서 이것은 기본 RNN 알고리즘의 약점으로 다음에 다룰 것입니다. 하지만 우리가 다루지 않는다면 RNN은 장거리 의존성을 잡는데 그다지 뛰어나지 않는 경향이 있습니다. 이 논의가 기울기 소멸에 초점이 맞춰지고 있지만 우리가 깊은 신경망에 대해 다룰 때 기울기 폭주에 대해서도 얘기했던 것을 기억하실겁니다.

우리가 역전파를 할 때 기울기는 단순히 기하급수적으로 감소하지 않고 통과하는 레이어의 수에 따라 기하급수적으로 증가할 수 있습니다. 기울기 소멸은 RNN 훈련의 가장 큰 문제입니다. 기울기 폭주가 일어나면 기하급수적으로 큰 그라데이션이 발생해 매개 변수가 너무 커져 신경망 매개 변수가 정말 엉망이 될 수 있어 치명적임에도 불구하고 말이죠.

기울기 폭주는 매개 변수가 그냥 폭발하기 때문에 발견하기 쉽습니다. 여러분은 종종 NaN을 볼 수 있는데요. 이는 여러분의 신경망 계산에 숫자 오버플로우의 결과를 의미합니다.

만약 기울기 폭주를 마주한 경우 하나의 솔루션으로 **그래디언트 클리핑gradient clipping**을 적용하는 방법이 있습니다. 이것이 의미하는 것은 기울기 벡터를 보고 일부 임계값보다 크다면 벡터의 일부를 다시 조정해서 너무 크지 않게 만드는 것입니다. 즉, 최대값에 따라 클리핑됩니다.

그래서 기울기 폭주를 본다면 여러분의 함수가 복원을 하려 한다면 그래디언트 클리핑을 적용하세요. 이는 폭주하는 기울기를 관리할 수 있는 비교적 강력한 해결책입니다.

하지만 기울기 소멸은 해결하기가 더 어려우며 이것에 대해서는 따로 다루게될 것입니다. 요약하자면, 이전에 매우 깊은 신경망을 훈련하는 방법에 대해서 살펴 보았습니다. 여러분은 함수가 기하급수적으로 감소하거나 레이어의 수의 함수로 기하급수적으로 증가하는 기울기 소멸 또는 폭주 문제를 맞닥뜨릴 수 있습니다.

RNN은 1,000배 이상의, 혹은 10,000배 이상의 데이터 세트를 처리하는데요. 이는 기본적으로 1,000개 혹은 10,000 신경망 레이어이죠. 이것 또한 이러한 유형의 문제에 부딪힙니다. 기울기 폭주는 그래디언트 클리핑을 적용함으로써 해결할 수 있지만 기울기 소멸은 더 많은 것이 필요합니다.

그래서 다음에 우리는 GRU greater recurrent unit에 대해서 얘기할 것입니다. 그라데이션 문제를 해결하는 데 매우 효과적인 해결책이며 여러분의 신경 네트워크가 훨씬 더 긴 범위의 의존성을 포착할 수 있게 해줍니다.
