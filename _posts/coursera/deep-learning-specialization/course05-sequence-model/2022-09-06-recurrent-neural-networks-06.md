---
title: "[Ⅴ. Sequence Models] Recurrent Neural Networks (6)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Natural Language Processing
  - Long Short Term Memory (LSTM)
  - Gated Recurrent Unit (GRU)
  - Recurrent Neural Network
  - Attention Models
toc: true
toc_sticky: true
toc_label: "Recurrent Neural Networks (6)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/187809649-f4918caf-ae96-4f46-a0cf-42ba990450d9.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Recurrent Neural Networks

## Recurrent Neural Networks

### Long Short Term Memory (LSTM)

지난번에는 GRU, 게이트 순환 유닛에 대해서 배웠습니다. 시퀀스에서 매우 긴 범위의 연결을 학습할 수 있었죠. 이것을 매우 잘 수행할 수 있게 해주는 다른 종류의 유닛은 **Long Short Term Memory, LSTM** 유닛이 있습니다. 그리고 이것은 GRU보다 훨씬 강력하죠. 한 번 보시죠.

---

![image](https://user-images.githubusercontent.com/55765292/188659783-95c48206-c28b-490b-9c14-338477aaa491.png)

GRU에 대한 이전에 나온 방정식입니다. GRU에서 $a^{< t>} = c^{< t>}$이며 업데이트 및 관련성 게이트를 갖습니다. $\tilde{c}^{< t>}$는 메모리 셀을 대체할 후보를 의미합니다. 그러고 나서 게이트를 업데이트 게이트 $\Gamma_u$를 사용했는데요. $\tilde{c}^{< t>}$를 사용하여 $c^{< t>}$를 업데이트할지 여부를 결정합니다.

LSTM은 GRU의 약간 더 강력하고 보다 일반적인 버전이며 논문으로 시퀀스 모델링에 엄청난 영향이 있었습니다만 이것은 읽기가 어려운 것 중 하나라고 생각합니다. 기울기 소멸 이론으로 꽤 깊게 들어가기 때문이죠.

그래서 더 많은 사람들이 이 특정한 논문이 아닌 다른 곳을 통해 LSTM을 더 자세히 배운다고 생각합니다. 이 논문이 딥 러닝 공동체에 훌륭한 영향을 주었음에도 불구하고 말이죠.

하지만 이들은 LSTM을 지배하는 방정식입니다. 그래서 메모리 세포인 $c$로 계속하고 그것의 업데이트를 위한 후보 값인 $\tilde{c}^{< t>}$는 이게 되겠죠. LSTM에서 우리는 더 이상 $a^{< t>} = c^{< t>}$인 케이스를 볼 수 없습니다. 그래서 이것이 우리가 사용하는 것이며 왼쪽의 방정식이랑 같습니다.

$a^{< t>}$나 $a^{< t-1>}, c^{< t-1>}$을 더 많이 사용하고 $\Gamma_r$은 이 관련성에 사용하지 않는다는 예외가 있습니다. 물론 LSTM 편차가 있을 수 있지만 더 일반적인 LSTM에서는 문제가 되지 않습니다. 그리고 나서 이전과 마찬가지로 업데이트 게이트를 갖게 될 것입니다.

> $\Gamma_u = \sigma(W_u \times [a^{< t-1>}, x^{< t>}] + b_u)$

$W_u$와 $[a^{< t-1>}, x^{< t>}]$를 곱하고 $b_u$를 더해줍니다 LSTM의 새로운 특성은 업데이트 게이트를 한 개 갖는 대신 양쪽 텀 모두 두 개의 별개의 텀을 갖는다는 점입니다. $Gamma_u, (1- \Gamma_u)$ 대신 여기 $\Gamma_u$만 갖고 이 게이트에 대해서는 $\Gamma_f$라고 부릅니다.

> $\Gamma_f = \sigma(W_f \times [a^{< t-1>}, x^{< t>}] + b_f)$

그래서 이 게이트 $\Gamma_f$는 여러분이 예상한 것과 거의 비슷하게 될 겁니다.

> $\Gamma_o = \sigma(W_o \times [a^{< t-1>}, x^{< t>}] + b_o)$

그런 다음 새 아웃풋 게이트가 있습니다. 그리고 메모리 셀의 업데이트 값 $c^{< t>}$는 $\Gamma_u$가 되고 요소별 곱셈으로 이 별표가 들어갑니다.

> $c^{< t>} = \Gamma_u * \tilde{c}^{< t>} + \Gamma_f * c^{< t-1>}$

이로써 메모리 셀에 기존 값인 $c^{< t-1>}$을 유지하는 옵션을 만들고 새 값인 $\tilde{c}^{< t>}$를 더할 수 있습니다. 그래서 별도의 업데이트를 사용하고 게이트는 잊어버리죠. 이것은 각각 update, forget, output 게이트를 의미합니다.

> $a^{< t>} = \Gamma_o * tanhc^{< t>}$

마지막으로 $a^{< t>} = c^{< t>}$ 대신에 아웃풋 게이트와 $c^{< t>}$를 요소별로 곱합니다. 그래서 이것들은 LSTM을 지배하는 방정식입니다. 그리고 그것이 두 개 대신에 세 개의 게이트를 가진 것을 볼 수 있죠. 그래서 조금 더 복잡하며 조금 다른 위치에 배치됩니다.

---

![image](https://user-images.githubusercontent.com/55765292/188659874-4bf2c287-940c-4559-b744-1507097d1101.png)

그래서 다시 여기 LSTM의 지배하는 방정식이 있습니다. 다시 말하지만, 그림을 사용하여 이러한 것들을 설명하는 것이 전통적이기에 여기다 그려 보겠습니다. 그리고 이 그림들이 너무 복잡해도 걱정하지 마세요. 개인적으로 그림보다는 방정식이 더 이해하기 쉽다고 생각하지만 그것이 전달하는 직관을 위해 보여드리는 것이니까요.

이 그림에서 빼야 할 중요한 것들은 모든 게이트 값을 구하기 위해 $a^{< t-1>}, x^{< t>}$를 사용하는 것입니다. 그림에서는 $a^{< t-1>}$와 $x^{< t>}$로 forget 게이트, update 게이트, output 게이트를 계산합니다. 또한 tanh으로 가서 $\tilde{c}^{< t>}$를 계산하죠.

이 값들은 이렇게 복잡한 방식으로 요소별 곱셈 등과 결합하여 이전의 $c^{< t-1>}$에서 $c^{< t>}$를 산출합니다. 여기서 재밌는 부분은 이를 일렬로 연결해보면 하나가 있고, 또 연결하고, 이렇게 임시로 연결할 수 있습니다. 입력은 $x^{<1>},x^{<2>},x^{<3>}$입니다.

이 단위들을 다음과 같이 모두 연결하면 이전 시간 단계의 출력 a가 다음 시간 단계의 입력이 됩니다. 제가 아래에 다이어그램을 조금 단순하게 만들었습니다. 눈치채셨겠지만 이 위에 라인이 하나 생겼죠.

forget 게이트와 update 게이트의 과정을 보여줍니다. LSTM에서는 $c^{<3>} = c^{<0>}$를 얻기 위해 오른쪽으로 전달하기가 상대적으로 쉽습니다. 그리고 이것이 GRU과 더불어 LSTM이 특정 값을 암기하는데 매우 뛰어난 이유이죠. 오랜 시간 동안이라도 특정한 실제 값이 메모리 세포에 많은 시간 단계에 거쳐 저장되어도 말이죠.

LSTM은 이런 것입니다. 사람들이 사용하는 몇 가지 변형이 있습니다. 가장 일반적인 버전에서는 $a^{< t-1>}$과 $x^{< t>}$로 결정되는 게이트 값 대신에 때로는 여기에 $c^{< t-1>}$을 슬쩍 넣기도 하죠. 이것을 **핍홀 연결peephole connection**이라고 합니다. 훌륭한 이름은 아니지만 핍홀 연결은 게이트 값이 $a^{< t-1>}$와 $x^{< t>}$에 달린 것이 아니라 이전 메모리 셀 값에 달려있다는 의미입니다.

그리고 핍홀 연결은 이 모든 세 게이트 계산에 들어갈 수 있습니다. 그래서 이것이 LSTM의 일반적인 변형입니다. 하나의 기술적 세부 사항은 이것들이 100차원 벡터라는 것입니다. 만약 100 차원의 숨겨진 메모리 세포 결합이 있다면 말이죠.

그러니까 $c^{< t-1>}$의 다섯 번째 요소는 상응하는 게이트의 다섯 번째 요소에만 영향을 미칩니다. 100차원인 $c^{< t-1>}$의 모든 요소가 게이트의 모든 요소에 영향을 미치지 않는 일대일 관계로 대신 $c^{< t-1>}$의 첫 번째 요소는 게이트의 첫 번째 요소에만 영향을 줍니다. 두 번째 요소는 두 번째 요소에 영향을 주죠.

다른 논문이나 핍홀 연결에 대한 자료를 보게 되면 해당 내용이 $c^{< t-1>}$이 게이트 값에 영향을 미친다는 게 바로 그 의미입니다.

---

그래서 지금까지 LSTM, 언제 GRU 혹은 LSTM을 써야 하는지에 대해 살펴봤습니다. 여기에는 이에 대한 광범위한 의견 일치가 있는데요. 그리고 제가 딥 러닝의 역사에서 GRU를 먼저 보여드렸음에도 불구하고 LSTM이 훨씬 먼저 나왔으며 GRU는 비교적 최근에 발명되었습니다. 보다 복잡한 LSTM 모델에서 단순화 되어 파생된 것으로 보입니다.

연구원은 여러 가지 문제에 대해 이 두 모델 모두를 시도했으며 여러 문제와 여러 알고리즘이 나왔습니다. 그래서 보편적으로 우월한 알고리즘이 없기 때문에 두 가지를 모두 보여주고 싶습니다.

그러나 이것을 사용할 때 GRU의 장점은 단순한 모델이라는 점입니다. 그래서 두 개의 게이트만을 가지고 훨씬 더 큰 네트워크를 빌드하기가 쉽습니다. 계산도 더 빠르게 실행되기 때문에 더 큰 모델을 빌드할 수 있습니다.

하지만 LSTM은 더 강력하며 게이트가 두 개가 아닌 세 개가 있기 때문에 더 유연합니다. 사용하기 위해 하나를 고르고 싶다면 LSTM은 역사적으로 검증 된 선택이었습니다. 그래서 하나를 골라야 한다면 오늘날 대부분의 사람들은 여전히 LSTM을 기본으로 골라 시도할 것입니다.

지난 몇 년간 GRU가 많은 탄력을 받고 있다고 생각하고 좀 더 많은 팀이 조금 더 단순하고 큰 문제를 조정하는데 더 쉽기 때문에 GRU를 사용합니다. 그래서 여기까지가 LSTM이었으며 이제 여러분은 훨씬 더 긴 거리의 의존성을 캡처하는 새로운 네트워크를 빌드할 수 있습니다.
