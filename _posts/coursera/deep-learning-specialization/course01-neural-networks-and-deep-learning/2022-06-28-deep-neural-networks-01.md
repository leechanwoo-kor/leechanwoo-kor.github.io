---
title: "[Ⅰ. Neural Networks and Deep Learning] Deep Neural Networks (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Deep Neural Networks (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Deep Neural Networks

## Deep Neural Network

### Deep L-layer Neural Network

지금까지 여러분은 단일 은닉층과 로지스틱 회귀를 사용하는 신경망에 맥락에서 순방향 전파와 역방향 전파를 보았고 벡터화에 대해 배웠고 방법을 무작위로 초기화하는 것이 언제 중요한지에 대해서도 배웠습니다. 지난 연습에서 이러한 아이디어 중 일부가 스스로 작동하는 것을 구현하고 보았습니다. 지금쯤이면 심층 신경망을 구현하는 데 필요한 대부분의 아이디어를 실제로 보았을 것입니다.

이제 할 일은 그 아이디어를 모아서 자신만의 심층 신경망을 구현할 수 있도록 하는 것입니다. 내용은 짧게 다룰 것이고 연습을 하면서 신경망에 대한 깊은 생각을 가질 수 있기를 바랍니다.

![image](https://user-images.githubusercontent.com/55765292/176098229-bdac3bd9-885d-44a6-a828-3cbb9df1efcf.png)

자 그러면 심층 신경망은 무엇일까요? 여러분은 로지스틱 회귀를 위해 이 그림을 보았고 하나의 은닉층이 있는 신경망도 보았습니다. 그래서 여기 2개의 은닉층이 있는 신경망과 5개의 은닉층이 있는 신경망의 예제가 있습니다.

로지스틱 회귀는 매우 "얕은" 모델인 반면 은닉층이 많은 훨씬 더 깊은 모델이며 얕음과 깊음은 정도의 문제라고 말합니다. 그래서 하나의 은닉층의 신경망은 2층 신경망이 될 것입니다. 신경망에서 레이어를 계산할 때 입력층을 계산하지 않고 출력층과 마찬가지로 은닉층만 계산한다는 것을 기억하십시오.

따라서 2층 신경망이 여전히 매우 얕지만 로지스틱 회귀 만큼 얕지는 않습니다. 기술적으로 로지스틱 회귀는 단층 신경망이므로 가능했지만 지난 몇 년 동안 AI는 머신 러닝 커뮤니티에서 심층 신경망이 학습할 수 있는 기능이 있다는 것을 깨달았습니다. 그것은 얕은 모델은 학습할 수 없는 것입니다.

주어진 문제에 대해 원하는 네트워크 깊이를 미리 예측하는 것은 어려울 수 있습니다. 로지스틱 회귀를 시도하고 1개 및 2개의 은닉층을 시도하고 은닉층의 수를 다양한 값을 시도할 수 있는 또 다른 하이퍼 매개변수로 표시하여 유효성 검증 데이터 또는 개발 세트에서 평가하는 것이 합리적입니다. 나중에 자세히 보도록 하겠습니다.

![image](https://user-images.githubusercontent.com/55765292/176098294-0040fe20-2836-481d-9029-e59857efd022.png)

이제 심층신경망을 나타내는 표기법에 대해 알아보겠습니다. 여기 4개의 레이어 신경망이 있는데 3개의 은닉층이 있고 이 은닉층의 단위 수는 5, 5, 3이고 그리고 하나의 상위 유닛이 있습니다. 그래서 우리가 사용할 표기법은 대문자 $L$을 사용하여 네트워크의 레이어 수를 나타냅니다.

이 경우, $L = 4$, 레이어의 수도 마찬가지이며 노드 수 또는 레이어 소문자 l의 단위 수를 나타내기 위해 N개의 윗첨자 [l]을 사용할 것입니다. 따라서 입력층 인덱싱하면 레이어 $0$으로 입력됩니다. 순서대로 레이어 1, 2, 3, 4입니다.

그러면 예를 들어 $n^{[2]}$이 있고 첫 번째는 5와 같을 것입니다. 왜냐하면 거기에 5개의 은닉 유닛이 있기 때문입니다. 이를 위해, 우리는 $n^{[2]}$을 가지고 있으며 2번째 은닉층의 단위 수는 5, $n^{[3]} = 3$과 같고 $n^{[4]} = n^{[L]} = 1$입니다. 대문자 $L$이 4와 같기 때문에 이 상위 단위 수는 1입니다.

그리고 여기에서는 입력 층으로 $n^{[0]} = n_x = 3$이라는 값이 됩니다. 그래서 다른 레이어에 있는 노드의 수를 설명하는 데 사용하는 표기법입니다.

각 레이어 $L$에 대해, 레이어 $l$의 활성화를 나타내기 위해 $a^{[l]}$을 사용할 것입니다. 그래서 우리는 나중에 전파를 위해 $a^{[l]}$을 활성화 $g(z^{[l]})$로 계산하고 활성화는 레이어 $l$에 의해 인덱싱될 것입니다.

그런 다음 $W^{[l]}$을 사용하여 레이어 $l$에서 $z^{[l]}$ 값을 계산하기 위한 가중치를 나타내고 마찬가지로 $b^{[l]}$은 $z^{[l]}$을 계산하는 데 사용됩니다.

마지막으로, 표기법을 마무리하기 위해 입력 기능은 $x$라고 불리지만 $x$는 레이어 0의 활성화이기도 하므로 $a^{[0]} = x$이고 최종 레이어의 활성화인 $a^{[L]} = \hat{y}$입니다. 
따라서 $a^{[L]}$은 
신경망의 예측 $\hat{y}$에 대한 예측된 출력과 같습니다.

심층신경망이 어떻게 생겼는지 보셨는데요. 심층 네트워크를 설명하고 계산하는 데 사용하는 표기법도 마찬가지입니다. 다음으로 이러한 유형의 네트워크에서 순방향 전파가 어떤 모습인지 설명하겠습니다.


### Forward Propagation in a Deep Network
심층 $L$층 신경망이 무엇인지 설명하고 그러한 네트워크를 설명하기 위해 사용하는 표기법에 대해 이야기했었습니다. 이번에는 심층 네트워크에서 순방향 전파를 수행하는 방법을 볼 수 있습니다.

![image](https://user-images.githubusercontent.com/55765292/176109526-16dcb07e-1349-4541-be39-c2789f1c81d6.png)

평소와 같이 먼저 단일 훈련 예제 $x$에 대해 순방향 전파가 어떻게 보이는지 살펴보고 나중에 전체 훈련 세트에 동시에 순방향 전파를 수행하려는 벡터화된 버전에 대해 이야기하겠습니다.

그러나 단일 훈련 예제 $x$가 주어지면 첫 번쩌 레이어의 활성화는 다음과 같이 계산됩니다. 먼저 $z^{[1]} = W^{[1]}x + b^{[1]}$을 계산합니다. 따라서 $W^{[1]}$과 $b^{[1]}$은 레이어 1의 활성화에 영향을 미치는 매개변수입니다.

이것은 신경망 레이어 1이며 그 레이어에 대한 활성화를 $a^{[1]} = g^{[1]}(z^{[1]})$로 계산합니다. 활성화 함수 $g$는 현재 레이어 및 레이어 1의 활성화 함수로서 설정된 인덱스에 따라 달라집니다. 그렇게 하면, 레이어 1에 대한 활성화를 계산한 것입니다.

레이어 2는 어떤가요? $z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$를 계산합니다. 따라서 레이어 2의 활성화는 $y$ 행렬에 레이어 1의 출력을 곱한 것입니다. 그래서, 그것은 그 값에 레이어 2에 대한 $b$ 벡터를 더한 것입니다. 그러면 $a^{[2]}$는 $z^{[2]}$에 적용되는 활성화 함수와 같습니다.

그래서 그것은 레이어 2를 위한 것이고 기타 등등이 포함될 수 있습니다. 상위 레이어에 도달할 때까지는 즉 레이어 4입니다. 여기서 $z^{[4]}$는 해당 레이어의 매개변수에 이전 레이어의 활성화를 곱하고, $b$ 벡터를 더한 값입니다.

마찬가지로 $a^{[4]} = g^{[4]}(z^{[4]})$와 같습니다. 
그래서, 그것이 예상 출력, $\hat{y}$를 계산하는 방법입니다.

따라서 한 가지 죽목해야 할 점은 $z^{[1]}$의 식에서 $x$는 $a^{[0]}$과도 같다는 것입니다. 입력 특징 벡터 $x$도 레이어 0의 활성화 이기 때문입니다. 그래서 우리는 $x$를 빼고 $a^{[0]}$을 놓으면, 이 모든 방정식들은 기본적으로 똑같아 보입니다.

일반적인 규칙은 $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$입니다. 그리고 나서 해당 레이어에 대한 활성화는 $z$값에 적용된 활성화 함수입니다. 즉 $a^{[l]} = g^{[l]}(z^{[l]})$로 일반적인 순방향 전파 방정식입니다.

지금까지 단일 훈련 예제를 위해 이 모든 것을 수행했습니다. 전체 훈련 세트에 대해 동시에 벡터화된 방식으로 수행하는 것은 어떤가요? 방정식이 예전과 상당히 비슷해 보입니다.

첫 번째 레이어의 경우 $Z^{[1]} = W^{[1]}X + b^{[1]}$과 같습니다. 그 다음, $A^{[1]} = g^{[1]}(Z^{[1]})$입니다. $X = A^{[0]}$이란걸 명심하세요. 이것은 다른 열에 쌓인 훈련 예시일 뿐입니다. 이걸 가져가서 $X$를 지우고 거기에 $A^{[0]}$을 넣을 수 있습니다. 그리고 두 번째 레이어를 보면 $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$ 및 $A^{[2]} = g^{[2]}(Z^{[2]})$와 같습니다.

여기 이런 $z$와 $a$와 같은 벡터들을 가져와서 쌓고 있습니다. 이것은 첫 번째 훈련 예제에 대한 $z$ 벡터 두 번째 훈련 예제에 대한 $z$ 벡터 등등, n 번째 훈련 예제까지 이러한 예제와 열을 쌓고 대문자 $Z$라고 부릅니다.

마찬가지로 대문자 A의 경우도 대문자 X와 같습니다. 모든 훈련 예제는 왼쪽에서 오른쪽으로 열 벡터 스택입니다. 이 과정에서 여러분은 $\hat{y} = g(Z^{[4]})$를 얻습니다. 이것은 또한 $A^{[4]}$와 같습니다. 모든 훈련 예제들이 수평으로 쌓아 올린 예측입니다.

표기법을 요약하면 소문자 $z$와 $a$를 대문자 $Z$와 $A$로 바꿀 수 있으며, 이것은 한 번에 전체 훈련세트에서 수행하는 순방향 전파의 벡터화된 버전을 제공하는데요. 이제 이 벡터화 구현을 보면 레이어 단계에서 for 루프가 있는 것 처럼 보입니다.

레이어 1, 2, 3, 4에 대한 활성화를 계산해야 합니다. 그래서 여기에 for루프가 있는 것 같습니다. 신경망을 구현할 때 우리는 일반적으로 명시적 for 루프를 제거하기를 원합니다. 하지만 여기서 명시적인 for 루프 없이는 이 방법을 구현할 수 없다고 생각합니다. 따라서 순방향 전파를 구현할 때 for 루프를 사용하면 레이어 1, 2, 3, 4에 대한 활성화를 계산하는 것은 완벽합니다. 따라서 이 위치에서는 명시적인 for 루프를 사용하는 것이 좋습니다.

이것이 심층 신경망에 대한 표기법과 이러한 네트워크에서 순방향 전파를 수행하는 방법에 대한 것입니다. 지금까지 본 내용들이 조금 익숙해 보인다면, 그것은 우리가 보고 있는 조각이 신경망에서 본 것과 매우 유사한 조각을 하나의 숨겨진 층으로 만들어 몇 번이고 반복하고 있기 때문입니다.

이제, 우리가 심층 신경망을 구현한다는 것이 밝혀졌습니다. 버그 없이 구현할 수 있는 가능성을 높이는 방법 중 하나는 작업 중인 행렬 차원에 대해 매우 체계적이고 신중하게 생각하는 것입니다.

그래서 코드를 디버그하려고할 때, 종종 종이를 꺼내서 신중하게 생각해보세요. 그래서 현재 사용하고 있는 행렬의 차원이 어느 정도인지요. 다음에 어떻게 할 수 있는지 다뤄보겠습니다.
