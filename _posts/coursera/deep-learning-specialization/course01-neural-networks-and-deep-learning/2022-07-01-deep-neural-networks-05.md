---
title: "[Ⅰ. Neural Networks and Deep Learning] Deep Neural Networks (5)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Deep Neural Networks (5)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Deep Neural Networks

## Deep Neural Network

### Parameters vs Hyperparameters

여러분이 신경망을 효과적으로 발전시키기 위해서는 파라미터뿐만 아니라 하이퍼 파라미터 또한 잘 조직화 시켜야 합니다. 그러면 하이퍼 파라미터는 무엇일까요? 한번 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/176830381-0f31ade6-f6fd-4084-a0c9-64502ba8225a.png)

여러분 모델의 파라미터는 $W$와 $b$입니다. 여러분의 알고리즘에게 알려줘야하는 것들이 있습니다. 러닝 속도 $\alpha$와 같은 것 말이죠. $\alpha$의 값을 잘 지정해서 결과적으로 이 $\alpha$의 값이 여러분의 파라미터가 어떻게 진화하는지 또는 기울기 강하의 iteration의 수가 필요할 수 있죠. 러닝 알고리즘에서 진행되는 iteration 말이죠. 또는 숨겨진 레이어의 개수들을 알려줘야 할 수도 있습니다.

이것을 대문자 $L$ 또는 은닉 유닛의 개수라고 하겠습니다. 여기에서 처럼 0, 1 그리고 2 등등 말이죠. 또한 활성화 함수를 어떤 것을 고를지에 대한 선택권도 있습니다. 즉, ReLU 함수를 이용할지, tanh 또는 시그모이드 함수를 쓸지 말이죠. 특히 은닉층에서 말이죠.

이런 모든 것들이 여러분이 러닝 알고리즘에 제공해야 하는 것들인데요. 이런 파라미터들이 ultimate 파라미터인 $W$와 $b$를 컨트롤합니다. 그렇기 때문에 여기 밑에있는 모든 것들을 하이퍼 파라미터라고 합니다.

여기 $\alpha$와 같은 학습속도, iteration 숫자, 은닉층의 수 등등이런 파라미터들은 $W$와 $b$를 컨트롤합니다. 그러므로 이것들을 하이퍼 파라미터라고 합니다. 이런 하이퍼 파라미터들이 최종적으로 $W$와 $b$ 같은 파라미터의 값을 결정짓기 때문에 딥러닝에서는 여러 개의 다양한 하이퍼 파라미터가 상존합니다.

이외에도 나중에 다루는 하이퍼 파라미터들이 있습니다. 딥러닝 분야에서는 하이퍼 파라미터의 종류가 머신 러닝 초기 시점과 비교했을 때 굉장히 많습니다. 저는 러닝속도인 알파 하이퍼 파라미터를 부르는데 일관되게 부르도록 하겠습니다. 파라미터라고 부르지 않고요.

머신 러닝 초반부에는 하이퍼 파라미터의 종류가 그리 많지 않았기 때문에 관련 종사자들은 이 내용과 관련해여 조금 천천히 받아들였습니다. 그래서 그냥 파라미터라고 불렀는데요. 엄밀히 말하면 알파는 리얼 파라미터가 맞긴 맞습니다. 파라미터를 결정하는 파라미터죠. 알파와 같은 거나 iteration의 수 등의 하이퍼 파라미터 말이죠.

그러므로 여러분 개인 어플리케이션의 딥 네트워크 학습을 하는 경우에 여러분의 하이퍼 파라미터를 사용할 수 있는 다양한 설정값이 있기 때문에 단순히 시도를 해봐야 알 수 있습니다.

![image](https://user-images.githubusercontent.com/55765292/176830401-a7f1c27f-5f57-4308-ac78-0574a63a3920.png)

그렇기 때문에 오늘날 Applied deep learning 절차는 여러분이 어떤 아이디어가 있으면 예를 들어, 학습 속도에 대한 최적값을 생각하고 있다면 알파의 값이 0.01이라고 할 수 있겠죠. 이 값을 시도해보고 싶을 수 있습니다. 그러면 도입해서 시도해보고 어떻게 작동하는지 볼 수 있습니다. 그리고 그 결과를 바탕으로 그 값들을 바꾸고 싶을 수 있습니다. 학습속도를 0.05로 증가시키고 싶을 수 있겠죠.

만약 러닝 속도를 어떤 값으로 설정해야 최고일지 잘 모르겠으면 하나의 알파 값을 시도해보고 비용함수(J)가 내려가는 것을 직접 관찰한 뒤에 조금 더 큰 값의 알파를 러닝 속도로 지정할 수 있겠죠. 그렇게 했는데 비용함수가 폭발적으로 변화를 일으켜 그래프가 갈라지게 되면 다른 버전을 시도해서 빨리 내려가는 것을 볼 수도 있겠죠.

이 경우 큰 값의 반대이기 때문에 또 다른 버전을 시도해서 지켜봅니다. 비용함수가 이러는 것을 볼 수 있게 되겠죠. 값을 시도해본 다음에 여기 이 알파의 값이 꽤 빠른 러닝 속도를 주고 더 낮은 비용함수로 수렴하게 만들어주기 때문에 여기 이 알파 값을 이용하라고 이야기할 수 있겠죠.

이전 그림에서 봤듯이 여러가지 종류의 하이퍼 파라미터가 있습니다. 그리고 여러분이 새로운 어플리케이션을 시작하는 경우에 저는 사실 가장 좋은 하이퍼 파라미터의 값이 무엇인지 미리 그 값을 알아내는 것이 매우 어렵다고 생각합니다. 그러므로 어떤 일이 벌어지냐면 여러분이 다양한 값들을 시도해보는 방법 밖에는 없습니다.

이 동그란 싸이클을 돌면서 5개의 은닉층을 시도해보고 이것을 도입해서 잘 되는지 확인하고 시도를 반복하는 것입니다. 이 슬라이드의 제목은 apply deep learning is very empirical process 인데요. empirical process라고 하는 것은 여러 가지 다른 것들을 시도해보고 어떤 것이 잘 작동하는지 확인하는 것을 멋스럽게 표현한 것입니다.

추가적으로 본 효과는 오늘날의 딥러닝이 컴퓨터 비전 음성인식, 자연어처리와 같은 수 많은 다양한 문제에 적용되고 있다는 점입니다. 또한, 구조적 데이터 어플인 온라인 광고 또는 웹서칭 또는 제품 추천 등과 같은 것에 적용될 수도 있죠.

한가지 특정 분야의 리서치 연구원들이 다른 분야로 가려고 하는데 가끔은 하이퍼 파라미터에 대한 직관은 이어서 연계되는 경우도 있고 다른 때에는 안 그런 경우도 있습니다. 그러므로 특히 새로운 문제를 시작하는 사람들에게 여러가지 범위의 값들을 먼저 시도해보고 어떤 것이 잘되는지 확인하라고 말씀드립니다.

다음에는 시스템적인 방법을 보겠습니다. 여러가지 범위의 값들을 시도하는 시스템적인 방법을 살펴볼 텐데요.

여러분이 한가지 어플에 오랜 기간 작업을 하면서 가장 최적의 값이 변할 수도 있는데요. 러닝속도나 은닉 유닛의 총 개수등의 값들이 업무를 진행하는 단계에서 변할 수도 있습니다. 만약 매일 최적의 하이퍼 파라미터의 값으로 시스템을 튜닝한다고 하더라도 가장 최적의 값이 지금으로부터 1년후에는 변할 수도 있습니다. 컴퓨터 인프라가 변해서 CPU나 GPU와 같은 것이 완전히 변해서 그럴 수 있습니다.

그러나 한가지 적용할 수 있는 경험의 근거한 규칙은 간간히 몇 개월마다 여러분이 어떤 특정 문제를 장기간 작업한다고 하면 또는 수년 간 작업하는 경우도 말이죠.

일단 먼저 몇개의 값을 하이퍼 파라미터 값을 시도해보고 더 좋은 값의 하이퍼 파라미터가 있는지도 확인을 해봅니다. 이렇게 계속 진행을 하다보면 하이퍼 파라미터에 대한 직관적인 이해도가 천천히 늘어나고 어떤 것이 본인 문제에 가장 적합한지 배울 수 있습니다.

이렇게 계속해서 반복적으로 여러가지 하이퍼 파라미터값을 시도해봐야하는 이런 부분이 매우 불만족스러운 부분일 수도 있다고 생각하는데요. 이 부분은 어쩌면 리서치 부분의 딥러닝에서는 아직도 발전하고 있는 분야이기 때문에 어느 정도 시간이 지나면 어떤 하이퍼 파리미터가 가장 좋은 값인지 더 나은 가이드를 해줄 수도 있을 것입니다.

하지만 동시에 CPU와 GPU와 네트워크 그리고 데이터 세트들이 항상 변하기 때문에 특정 가이드라인이 한동안은 수렴하지 않을 가능성도 있습니다. 그렇기 때문에 계속 다른 값들을 시도해보고 cross-validation set와 같은 것을 통해 평가하고 그렇게 해서 여러분의 문제에 가장 잘 맞는 값을 선택하도록합니다.

### What does this have to do with the brain?

딥러닝은 뇌와 어떤 연관이 있을까요? 많이는 아니지만 사람들이 딥 러닝과 인간의 뇌를 계속 비유하는 이유를 간단히 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/176837460-bb40d3ef-418d-45e0-8b08-b77a18641552.png)

신경망을 구현할 때 이게 바로 여러분이 하는 일인데요. 순방향 전파와 역방향 전파입니다. 이 방정식들이 무엇을 하고 있는지에 대한 직관을 전달하는 것이 어려웠기 때문에 정말 매우 복잡한 기능에 대한 감각을 만들어냈고 뇌가 무엇을 하고 있는지에 대한 비유가 지나치게 단순화된 설명이 된 것 같습니다.

하지만 이런 단순함은 사람들이 그것을 공개적으로 말하는 것은 물론 언론에서도 보도하게끔하는 유혹을 불러일으키는데 그것은 확실히 대중적 상상력이라고 불립니다.

예를 들어, 시그모이드 활성화 함수를 가진 로지스틱 회귀 단위 사이에는 매우 느슨한 비유가 있습니다. 옆에는 뇌에 있는 단일 뉴런의 그림입니다.

생물학적 뉴런의 그림에서 뇌의 세포인 이 뉴런은 다른 뉴런 $x_1,x_2,x_3$ 또는 아마도 다른 뉴런 $a_1,a_2,a_3$로부터 전기 신호를 받아 간단한 임계 계산을 한 다음 이 뉴런이 발사되면 축삭 아래로 긴 와이어 아래로 아마도 다른 뉴런으로 전기 펄스를 보냅니다.

신경망의 단일 뉴런과 오른쪽에 표시된 것과 같은 생물학적 뉴런 사이에는 매우 단순한 비유가 있습니다. 하지만 오늘날 신경과학자들조차 단일 뉴런이 무엇을 하는지 거의 알지 못한다고 합니다.

하나의 뉴런은 우리가 신경과학으로 특징지을 수 있는 것보다 훨씬 더 복잡한 것으로 보이며 뉴런이 하는 일 중 일부는 약간 로지스틱 회귀와 비슷하지만 여전히 단일 뉴런이 무엇을 하는지에 대해서는 오늘날 아무도 이해하지 못하는 부분이 있습니다.

예를 들어, 인간의 뇌에서 뉴런이 어떻게 학습하는지는 여전히 미스터리한 과정이며 오늘날 인간의 뇌가 알고리즘을 사용하는지 역방향 전파 또는 경사하강법과 같은 작업을 하는지 아니면 인간의 뇌가 사용하는 근본적으로 다른 학습 원리가 있는지 아직까지도 불분명합니다.

딥러닝을 생각할 때 저는 그것이 매우 훌륭하고 매우 유연한 함수, 매우 복잡한 함수를 학습해서 X부터 Y까지 매핑을 배우고 지도 학습에서 입출력 매핑을 배우는 것으로 생각합니다.

반면에, 이것은 뇌에 비유되는 것과 같은데요. 아마도 한때는 유용했을지 모르지만 그 비유가 무너지는 지점으로 분야가 옮겨갔다고 생각하며 더 이상 그 비유를 많이 사용하지 않는 경향이 있습니다.

신경망과 뇌에 대한 내용은 여기까지입니다.

컴퓨터 비전 분야가 딥 러닝을 적용하는 다른 분야보다 인간의 뇌에서 조금 더 많은 영감을 얻었을지도 모른다고 생각합니다만 개인적으로 예전보다 인간의 뇌에 대한 비유를 덜 사용하는 편입니다.

이제 심층 신경망에서도 순방향 전파와 역방향 전파 및 경사 강하를 구현하는 방법을 알게 되었습니다.
