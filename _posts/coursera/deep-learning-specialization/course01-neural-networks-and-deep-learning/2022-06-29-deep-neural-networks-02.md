---
title: "[Ⅰ. Neural Networks and Deep Learning] Deep Neural Networks (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Deep Neural Networks (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Deep Neural Networks

## Deep Neural Network

### Getting your Matrix Dimensions Right
심층 신경망을 구현할 때, 코드의 정확성을 확인하기 위해 자주 사용하는 디버깅 도구 중 하나는 종이 한 장을 꺼내서 작업 중인 행렬의 차원을 살펴보는 것입니다. 이렇게 하면 심층 네트워크도 간단하게 구현할 수 있을 것 같으니 그 방법을 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/176329821-f03d254c-366f-4595-8d59-9eb0d042cafa.png)

따라서 $L = 5$입니다. 입력층을 세지 않기 때문에, 여기에는 5개의 층이 있고 4개의 은닉층과 1개의 출력층이 있습니다. 순방향 전파를 구현하는 경우 첫번째 단계는 $Z^{[1]} = W^{[1]}X + b^{[1]}$입니다.

우선 $b$는 무시하고 매개변수 $W$에 집중합시다. 첫 번째 은닉층은 3개의 은닉 유닛이 있습니다. 이전에 얻은 표기법을 사용하면 레이어 1의 은닉 유닛 수인 $n^{[1]}$이 3이 됩니다. 여기서 $n^{[2]} = 5, n^{[3]} = 4, n^{[5]} = 1$입니다.

지금까지 우는 단일 출력 유닛을 가진 신경망만을 보았지만 이후 과정에서는 다중 출력 유닛을 가진 신경망에 대해서도 설명하겠습니다. 마지막으로 입력층의 경우 또한 $n^{[0]} = n_x = 2$입니다.

이제 $Z,W,X$의 차원에 대해 생각해 봅시다. $Z$는 이 첫 번쨰로 은닉층에 대한 활성화 벡터입니다. 그러면 $Z$는 $3 \times 1$이 될 것이고, 3차원 벡터가 될 것입니다. 1차원 행렬로 $n^{[1]$을 이 경우에 $3 \times 1$이라고 쓰겠습니다.

이제 입력 특성 $X$를 보겠습니다. $X$는 2개의 입력 특성이 있습니다. 따라서 $X$는 이 예제에서 $2 \times 1$이지만 더 일반적으로는 $n^{[0]} \times 1$이 됩니다. 우리가 필요한 것은 행렬 $W^{[1]$이 $n^{[0]}$에 1벡터를 곱하면 $n^{[1]} \times 1$ 벡터가 되는 것입니다.

따라서 3차원 벡터는 2차원 벡터에 뭔가를 곱한 것입니다. 행렬 곱셈의 규칙에 따르면 $3 \times 2$행렬에 $2 \times 1$ 행렬을 곱하거나 $2 \times 1$ 벡터를 곱하면, $3 \times 1$벡터를 얻을 수 있습니다. 더 일반적으로, 이것은 $n^{[1]}$에 의한 $n^{[0]}$ 차원 행렬이 될 것입니다.

그래서 여기서 우리가 알아낸 것은 $W^{[1]}$의 차원이 $n^{[1]} \times n^{[0]}$이어야 하고, 더 일반적으로 $w^{[l]}$의 차원이 $n^{[l]} \times n^{[l-1]}$이어야 한다는 것입니다. 예를 들어, $w^{[2]}$의 차원은 $5 \times 3$이거나 $n^{[2]} \times n^{[1]}$이어야 합니다. 왜냐하면 $Z^{[2]}$를 $W^{[2]} \times a^{[1]}$로 계산할 것이기 때문입니다.

다시 말하지만, 이번에도 바이어스를 무시할 겁니다. 그러면 그것은 $3 \times 1$이 될 것입니다. 이것은 $5 \times 1$이 되어야 합니다. 따라서 $5 \times 3$이 더 좋습니다.

마찬가지로 $W^{[3]}$은 다음 레이어의 차원 이전 레이어의 차원입니다. 따라서 $4 \times 5$가 됩니다. $W^[4]$는 $2 \times 4$가 되며, $W^{[5]}$는 $1 \times 2$가 됩니다. 확인해야할 일반적인 공식은 레이어 $L$에 대한 행렬을 구현할 때 해당 행렬의 차원이 $n^{[l]} \times n^{[l-1]}$이라는 것입니다.

이제 이 벡터 $b$의 차원에 대해 생각해 봅시다. $b^{[1]}$은 $3 \times 1$ 벡터가 될 것이고, $3 \times 1$ 벡터를 출력으로 얻기 위해 다른 $3 \times 1$ 벡터에 더해야 합니다. $b^{[2]}$은 $5 \times 1$이 될 것입니다.

좀 더 일반적인 규칙은 왼쪽에 있는 예제에서, $b^{[1]}$는 $n^{[1]} \times 1$이라는 것이죠. 두 번째 예제로는 $n^{[2]} \times 1$이므로 보다 일반적인 경우 $b^{[l]}$은 $n^{[l]} \times 1$차원이어야 합니다.

역전파를 구현한다면 dw의 차원은 w의 차원과 같아야 합니다. 따라서 dw는 w와 같은 차원이어야 하고, db는 b와 같은 차원이어야 합니다.

이제 차원을 확인할 수 있는 또 다른 중요한 수량은 $Z, X$ 및 $A^{[l]}$인데요. 여기서 많이 이야기하지 않았던 것들입니다. $Z^{[l]} = g^{[l](a^{[1]})$이기 때문에, 요소별로 적용한 다음 $Z$와 $A$는 이러한 유형의 네트워크에서 동일한 차원을 가져야합니다.

이제 한 번에 여러 예제를 살펴보는 벡터화된 구현이 있는 경우 어떤 일이 발생하는지 보겠습니다. 물론 벡터화의 구현의 경우에도 $w, b, dw ,db$의 차원은 동일하게 유지됩니다. 그러나 벡터화된 구현에서는 $x$와 마찬가지로 $za$의 차원도 약간 변경됩니다.

![image](https://user-images.githubusercontent.com/55765292/176329873-45d9d194-16bd-41a2-b4a4-d11d649796e9.png)

이전에 $z^{[1]} = w^{[1]}x + b^{[1]}$를 보면 $z^{[1]}$는 $n^{[1]} \times 1$이었죠. $w^{[1]}$는 $n^{[1]} \times n^{[0]}$, $x$는 $n^{[0]} \times 1$, $b$는 $n^{[1]} \times 1$입니다. 벡터화된 구현에서는 $z^{[1]} = w^{[1]}x + b^{[1]}$와 같습니다.

여기서 $Z^{[1]}$은 개별 예에 대하여 $z^{[1]}$을 취함으로써 얻을 수 있습니다. 따라서, $z^{\[1\](1)},z^{\[1\](2)},...,z^{\[1\](m)}$까지 있고 다음과 같이 쌓으면 $Z^{[1]}$이 됩니다. $Z^{[1]}$의 차원은 $m$일 결정적인 훈련 세트인 경우 $n^{[1]} \times 1$ 대신 $n^{[1]} \times m$이 된다는 것입니다.

$W^{[1]}$의 차원은 동일하게 유지되므로 이것은 $n^{[1]} \times n^{[0]}이고, $X$가 $n^{[0]} \times 1$인 대신에, 이제 모든 훈련 예제가 수평으로 찍혀 있으므로 이제 $n^{[0]} \times m$ 입니다. $n^{[1]} \times n^{[0]}$ 행렬을 취하고 이를 $n^{[0]} \times m$ 행렬로 곱하면 $n^{[1]} \times m$ 차원 행렬을 예상대로 얻을 수 있습니다.

마지막으로 $b^{[1]}$은 여전히 $n^{[1]} \times 1$ 입니다. 그러나 이것을 가져와서 $b$에 더하면 파이썬 브로드캐스팅을 통해 $m$ 행렬로 $n^{[1]}$로 복제된 다음 요소별로 더해집니다.

이전에 $w,b,dw,db$의 차원에 대해 이야기했습니다. 여기서 우리가 볼 수 있는 것은 $z^{[1]}, a^{[1]}$가 $n^{[1]} \times$ 차원이지만 이제 $Z^{[l]}, A^{[l]}$는 $n^{[l]} \times m$인 것입니다.

이것이 특별한 경우는 $l = 0$일 경우 입니다. 이경우 $A^{[0]}$은 훈련 세트 입력 특성 $X$와 동일하며 예상한 대로 $n^{[0]} \times m$이 됩니다. 물론 이것을 역전파로 구현할 때 나중에 $da$뿐만 아니라 $dz$도 계산하게 되는 것을 보게 될 것입니다. 물론 이 방법은 $Z,A$와 같은 차원을 갖습니다.

간단한 실습을 통해 작업할 다양한 행렬의 차원을 명확히 하는 데 도움이 되기를 바랍니다. 심층 신경망에 대해 역전파를 구현할 때 코드를 통해 작업하고 모든 행렬 또는 차원이 일치하는지 확인하는 한 일반적으로 어떤 종류의 가능한 버그를 제거하는 데 도움이 될 것입니다. 작업할 다양한 행렬의 차원을 알아내는 연습이 도움이 되었으면 합니다. 심층 신경망을 구현할 때 작업 중인 다양한 행렬과 벡터의 차원을 일직선으로 유지한다면, 가능한 버그들의 종류를 제거하는 데 도움이 될 것입니다.

신경망에서 순방향 전파를 수행하는 방법에 대한 몇 가지 메카니즘을 살펴보았습니다. 왜 심층 신경망이 그렇게 효과적이며 또 shallow representations 보다 왜 더 잘 작동하는 것일까요? 다음에 살펴보겠습니다.
