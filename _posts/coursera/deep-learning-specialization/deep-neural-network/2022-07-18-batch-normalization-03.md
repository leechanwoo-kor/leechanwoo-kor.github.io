---
title: "[Ⅱ. Deep Neural Network] Batch Normalization (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - Hyperparameter Tuning
toc: true
toc_sticky: true
toc_label: "Batch Normalization (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Batch Normalization

### Why does Batch Norm into a Neural Network

배치 노름이 작동하는 이유는 무엇일까요? 한가지 이유는 X 라는 입력 특성을 정규화 하여 평균값을 0, 변수 1로 만들었는데요. 이것이 학습률을 높혀줍니다. 어떤 특성이 0에서 1의 값을 갖는다거나 다른거는 1에서 1000의 범위를 갖는 것이 아니라 모든 특성을 정규화함으로써 입력 특성 x가 비슷한 범위의 값을 주고 이로 인해 학습률도 증가합니다.

배치 노름이 잘 작동 하는 한가지 직관적인 부분은 이것은 유사한 작업을 수행하지만 입력에 대한 것이 아니라 숨겨진 단위의 추가 값입니다. 이제 이것은 배치 노름이 수행하는 작업에 대한 부분적인 그림일 뿐입니다. 다른 직관적인 부분이 더 있는데요. 그것은 여러분이 더 깊이 배치 노름의 역할을 이해하는데 도움을 줄 것입니다.

---

![image](https://user-images.githubusercontent.com/55765292/179453578-6f7d7192-e420-4000-96fb-cab6ee5193fc.png)

배치 노름이 잘 작동하는 두번째 이유는 이것은 가중을 만드는데요. 네트워크보다 더 나중 또는 더 깊숙이 말이죠. 예를 들어, 10번째 층에서는 가중의 변화에 더 튼튼하고 신경망에서 더 이른쪽의 층 ,예를 들어 첫번째 층, 보다 말이죠. 무슨 말인지 설명 드리자면 더 명확한 예제를 보겠습니다.

여기 네트워크의 훈련을 보겠습니다. 얕은 훈련일 수도 있는데요. 로지스틱 회귀분석 또는 신경망일 수도 있는데요. 이 회귀와 같은 얕은 네트워크일 수도 있고 우리의 유명한 고양이 탐지 토스에 대한 깊은 네트워크일 수도 있습니다. 그런데 모든 데이터를 검은 고양이의 사진으로 훈련 했다고 해봅시다.

이제 이 네트워크를 왼쪽과 같이 검은 고양이만 긍정적인 예가 아닌 유색 고양이가 있는 데이터에 적용하려고 하면 하지만 오른쪽에 있는 색깔 있는 고양이도 말이죠. 그러면 잘 작동되지 않을 수 있습니다. 그러므로 그림에서 훈련 세트가 만약 이렇게 생겼으면 여기에 긍정적인 예가 있고 여기에 부정적인 예가 있는 경우죠.

하지만 여러분이 일반화 해야하면 긍정적인 예가 여기에 있고 부정적인 예가 여기에 있는 데이터 세트로 그러면 왼쪽의 데이터에 대해 훈련된 모듈이 오른쪽의 데이터에 대해 잘 수행될 것으로 기대하지 않을 수 있습니다. 실제로 잘 작동하는 동일한 기능이 있을 수 있지만 그러나 학습 알고리즘이 왼쪽의 데이터를 보고 녹색 결정 경계를 발견할 것으로 예상하지는 않습니다.

그리하여 이런 데이터의 분포가 변하는 이 아이디어를 공변량 변화라고 하는 거창한 이름으로 불리는데요. 기본적인 아이디어는 만약 X에서 Y로 가는 매핑을 배운 경우 x의 분포도가 만일 변경하면 그런 다음 학습 알고리즘을 다시 훈련해야 할 수도 있습니다. 이것은 x에서 y로 가는 매핑 ground truth 함수가 변하지 않더라도 참으로 적용됩니다. 이번 예제같이 말이죠.

그 이유는 ground truth 함수는 이 그림이 고양인지 아닌지에 대한 것입니다. 그리고 함수를 다시 유지 시켜야하는 필요성이 격상되는데요. Ground True 기능도 함께 이동하면 더욱 악화됩니다. 그러면, 이러한 공변량 변화 문제가 신경망에서는 어떻게 적용될까요?

---

![image](https://user-images.githubusercontent.com/55765292/179453594-51868fe8-5529-41d4-961c-753882f0c7c9.png)

이와 같은 심층 신경망을 고려해보겠습니다. 이제 학습 절차를 어떤 특정 층의 기준으로 볼텐데요. 세번째 층이라고 해보겠습니다. 그럼 이 네트워크는 w3와 b3 파라미터를 배웠습니다. 그러면 3번째 은닉 층의 시각으로는 특정 값들을 이전 층에서 가지고 오는데요. 그런 다음 출력 ŷ을 Ground True 값 Y에 가깝게 만들기 위해 몇 가지 작업을 수행해야 합니다.

왼쪽에 있는 노드들은 가려보겠습니다. 그래서 세번째 은닉 층의 관점에서는, 어떠한 값을 갖는데요. 이 값들을 A_2_1, A_2_2, A_2_3 그리고 A_2_4 라고 하겠습니다. 그러나 이런 값들은 차라리 X1, X2, X3, X4인 특성이라고 칭하는 것이 낫겠고 그리고 세번째 은닉 층의 역할은 이 값들을 갖고 ŷ 을 매핑하는 것입니다.

그러면 훌륭한 가로채기를 상상할 수 있는데요. 그래서 여기 파라미터들 W_3_B_3, 그리고 어쩌면 W_4_B_4, 그리고 심지어 W_5_B_5도 말이죠. 잘하면 이런 파리미터를 배워서 네트워크가 여기 왼쪽에 그린 값들에 대해 매핑을 잘할 수 있게 될 수도 있겠죠. ŷ 값을 말이죠.

그럼 이제 왼쪽의 네트워크를 다시 보여드리겠습니다. 네트워크 또한 W_2_B_2와 W_1B_1의 파라미터를 조정하며 그렇기 때문에 여기 파라미터의 값이 변하면 여기 값, 즉 A_2, 역시 변할 것입니다. 그러면 세번째 은닉 층의 시각에서 보면 이러한 은닉 단위의 값은 항상 변하는 것이고 그러므로 이전에 언급했던 공변량 변화 문제로 인해 고통 받게 되는 것입니다.

그러므로 배치 노름이 하는 것은 이런 은닉 단위 값의 분포가 왔다 갔다 이동하는 양을 줄여줍니다. 그리고 여기 은닉 단위 값들에 대해 분포도를 그리자면 이것은 엄밀히 기술적으로 재정규화 된 Z 값입니다. 그러면 실제로 이것은 Z_2_1 그리고 Z_2_2이며 그리고 4개의 값 대신 2개의 값을 나타내어 2D로 시각화할 수 있습니다.

배치 노름이 주장하는 것은 Z_2_1 Z 와 Z_2_2의 값이 변할 수 있다는 것이며 그러면 정말로 신경망이 이전에 있는 층들의 파라미터들을 업데이트하면서 변할 것입니다. 하지만 배치 노름이 보장 하는 것은 어떻게 변하더라도, Z_2_1 와 Z_2_2 의 평균값과 분산은 똑같을 것이라는 점입니다. 그러므로 구체적으로 Z_2_1와 Z_2_2 값이 변하더라도 말이죠.

그들의 평균값과 분산은 변동없이 그대로 각각 0과 1의 값을 유지할 것입니다. 또는 꼭 평균값 0과 분산 1이 아닐수는 있는데요. 그러나 값이 무엇이든 베타 2와 감마 2가 적용됩니다. 신경망이 결정하는 경우에는 평균이 0이 되고 분산이 1이 되도록 강요할 수 있어요. 또는 실제로 다른 모든 값 또는 분산으로 말이죠.

이것이 본질적으로 하는 것은 이전의 층에서 파라미터가 업데이트 되면서 세번째 층에 영향을 줄 수 있는 양을 제한시켜 줍니다. 이 변화량은 세번째 층이 새로 배워야 하는 양이기도 하구요. 그러므로 배치 노름은 입력값이 변해서 생기는 문제를 줄여줍니다.

여기 이런 값들을 더 안정감 있게 해주고 그럼으로 인해 신경망의 나중의 층들이 그 자리를 지킬 수 있게 틀을 마련해줍니다. 그리고 비록 입력값의 분포도가 약간 변하기는 하지만 더 작게 변하고 이것이 하는 것은 이전 층이 계속 학습하면서도 여기 이 부분이 다른 층에게 압박하는 다음 층들이 적응할 수 있도록 배워야 하는 양이 줄어드는데요.

또는 원한다면 초기 레이어 매개변수가 수행해야 하는 작업 간의 결합 이후 레이어 매개변수가 수행해야 하는 작업을 약화시킵니다. 따라서 네트워크의 각 계층이 스스로 학습할 수 있도록 하고, 다른 층과는 조금 더 독립적으로 말이죠 이것은 결과적으로 전체 네트워크의 속도를 올려주는 효과를 줍니다.

중요한 것은 배치 노름의 뜻인데요. 특히 신경망에서 나중의 층의 시각에서 봤을 때 말이죠. 이전 층은 별로 이동폭이 크지 않은데 왜냐하면 똑같은 평균과 분산을 갖도록 제한되어 있기 때문입니다. 이것이 다른 층들이 배우는 업무 과정을 쉽게 해줍니다.

![image](https://user-images.githubusercontent.com/55765292/179453623-f7f37027-925e-4156-8070-110b47014990.png)

배치 노름은 두번째 효과도 있는데요. 일반화 효과가 있습니다. 따라서 배치 노름의 직관적이지 않은 한 가지는 각 미니 배치가 X_t라고 하겠는데요. Z_t의 값을 갖고요. 그리고 Z_l의 값을 갖는데요. 하나의 미니 배치에서 계산된 평균과 분산으로 조정됩니다.

이제 전체 데이터 세트에서 계산되는 것과는 대조적으로 해당 미니 배치에서만 평균과 분산이 계산되기 때문에 그 평균과 분산에는 약간의 노이즈가 있습니다. 왜냐하면 미니 배치 에서만 계산되기 때문이죠.

예를 들어, 64 또는 128 또는 256 또는 더 큰 훈련 예제에서 말이죠. 따라서 평균과 분산은 비교적 작은 데이터 샘플로 추정되기 때문에 약간 잡음이 있기 때문에 Z_l에서 Z_2_l로 가는 스케일링 프로세스 그 프로세스도 약간 잡음이 있는데 약간 잡음이 있는 평균과 분산을 사용하여 계산되기 때문입니다.

그렇기 때문에 드롭아웃과 비슷하게 각 은닉 레이어의 활성화에 약간의 잡음을 추가합니다. 드롭아웃에 잡음이 있는 방식 은닉 단위를 가지고 0에 곱하고 어떨 확률에 곱합니다. 그리고 어느 정도 확률로 1을 곱합니다. 그러면 드롭아웃은 복수의 잡음이 있는데요. 0 또는 1로 곱해지기 때문이죠. 

배치 노름은 표준 미분에 의한 스케일링 때문에 잡음의 배수가 되지만 추가적인 잡음도 있는데요. 평균값을 빼기 때문이죠. 여기에서 평균과 표준 미분의 추정치는 잡음이 있습니다. 그러므로 드롭아웃과 비슷하게 배치 노름도 약간의 일반화 효과가 있습니다. 여기 은닉 단위에 잡음을 더함으로서 밑에 있는 은닉 단위들이 어느 특정 은닉 단위에 의존하지 않게 하기 때문입니다.

그리고 드롭아웃과 비슷하게 은닉 층에 잡음을 더하여 아주 작은 일반화 효과가 생깁니다. 왜냐하면 더해지는 잡음이 꽤 작기 대문에 이것은 큰 일반화 효과는 아닙니다. 그리고 여러분은 배치 노름과 드롭 아웃과 같이 사용하도록 선택할 수 있고 그리고 드롭아웃에서 조금 더 강력한 일반화 효과가 생기길 바라면 배치 노름을 드롭아웃과 같이 사용할수도 있습니다. 약간 직관적이지 않은 또 다른 효과는 다음과 같은데요.

만약 여러분이 더 큰 미니 배치 크기를 이용하면 예를들어 64 대신에 512를 쓴다고 해보죠. 이렇게 더 큰 미니 배치 크기를 이용하면 여기 이 잡음을 줄이고 결과적으로 일반화 효과를 줄이는 것입니다. 이것이 이상한 드롭 아웃의 속성 중 하나입니다. 바로 더 큰 미니 배치 크기를 사용하여 일반화 효과를 줄인다는 것입니다.

이것을 말하기 했지만 배치 노름을 일반화로 사용하지 않을 것이며 그것이 배치 노름의 의도가 아닙니다. 하지만 때로는 학습 알고리즘에 의도 하거나 의도하지 않은 추가 효과가 있습니다.

하지만 일반화로 배치 노름을 사용하지 마세요. 정규화하는 방편으로 사용하세요. 은닉 단위 활성화를 말이죠. 이 학습이 빨라지게요. 그리고 저는 일반화가 거의 의도치 않은 부작용이라고 생각합니다. 따라서 배치 노름이 수행하는 작업에 대해 더 나은 직관을 제공하기를 바랍니다.

배치 노름에 대한 내용을 정리하기 전에 여러분이 알아야하는 내용이 한가지 있습니다. 즉, 배치 노름은 한 번에 하나의 미니 배치로 데이터를 처리합니다. 그것은 미니 배치의 평균과 분산을 계산합니다. 그래서 테스트 타임에서는 예측하는 것들을 만드려고 하고 신경망을 평가하려 하며 여러분은 미니 배치 예시가 없을 수도 있고 여러분은 1개의 예시씩 처리하고 있을수도 있습니다.

그래서 이런 경우 테스트 타임에 무엇을 해야합니다. 조금다르게 말이죠. 예상하는 것들이 말이 되게요.

