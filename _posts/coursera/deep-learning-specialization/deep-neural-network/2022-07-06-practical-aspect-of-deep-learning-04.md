---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (4)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (4)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Regularizing your Neural Network

### Dropout Regularization

L2 일반화에 추가로 더욱 강력한 일반화 테크닉인 dropout이라는 것이 있습니다. 어떻게 작동하는 것인지 알아보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/177447990-2f10cd38-332a-4fea-93d2-9ab8934faa38.png)

왼쪽과 같은 신경망을 트레이닝 하는데 overfitting하는 경우입니다. 오른쪽에 신경망을 복사해보겠습니다. dropout에서는 무엇을 할 것이나면, 이 네트워크의 각각 레이어별로 살펴보면서 신경망의 노드를 제거하는 확률을 세팅해볼 것입니다.

각각의 레이어별로 각각의 노드별로 동전을 던져서 노드를 유지할 것인지 제거할 것인지 각각 0.5 활률이 있다고 할 것입니다. 동전을 던진 후에 노드들을 제거하겠다고 할 수도 있습니다. 그러면 들어오고 나가는 링크 또한 제거를 같이 할 것입니다.

그렇게 되면 훨씬 더 작은 감소된 네트워크가 남을 것입니다. 그 다음에 후방향전파 트레이닝을 진행합니다. 여기처럼 감소된 네트워크에 예제가 있고 또 다른 예제가 있는데요. 이 경우에는 동전을 몇 개 던진 이후에 노드의 뭉치를 유지하거나 제거하는 방법이 있습니다.

각각의 트레이닝 예시에 대해서, 여기 있는 방법 중에서 신경망 트레이닝을 시킬 것입니다. 아마도 조금 미친 테크닉이라고 생각할 수도 있는데요. 이것을 코딩하는 방법이 무작위로 이루어지지만 실제로 작동을 합니다.

각각의 예세에 대해서 훨씬 더 작은 네트워크를 트레이닝 시키기 때문에 네트워크를 일반화시킬 수 있는지에 대해 감을 잡을 수 있을 수도 있습니다. 이런 훨씬 더 작은 네트워크가 트레이닝되고 있기 때문이죠. dropout을 어떻게 도입하는지 알아보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/177448028-65bcd66f-0dfc-4a03-993a-b071e3d33f2b.png)

dropout을 도입하는 방법은 몇가지가 있습니다. 가장 흔한 방법을 보여드리겠습니다. 바로 inverted dropout이라고 하는 기술입니다. 완성도를 위해서 이것을 l=3이라는 층으로 묘사한다고 해봅시다.

코딩할 때는 이것을 몇개의 "3"들로 적을 텐데요. 싱글 레이어에서 dropout을 묘사하는 방법을 보여드리고 있습니다.

여기서 할 것은 벡터 d를 설정해서 $d^3$는 3이라는 레이어에 대한 dropout 벡터 일 것인데요. $d^3$이라는 것은 np.random.rand(a)입니다. 이것은 $a^3$와 같은 모양일 것입니다. 이것이 어떤 숫자보다 작은지 확인할 것입니다.

그걸 keep.prob이라 부를 텐데요. 그러면 keep.prob은 숫자가 됩니다. 이전엔 0.5 였습니다. 이번 예제에서는 0.8을 이용하겠습니다. 또한 특정 은닉 유닛이 유지될 확률이 있을 것입니다. 그러므로 keep.prob은 0.8입니다.

이러면 0.2는 은닉 유닛을 제거할 확률입니다. 이것이 하는 것은 random matrix를 생성합니다. $d^3$는 메트릭스일 것입니다. 각각의 예시나 은닉 유닛에 대해서 $d^3$가 1이 될 확률이 0.8에 해당하는 경우를 가르킵니다. 또 0이 될확률이 20%인 경우를 가르키죠. 0.8보다 작은 랜덤 숫자가 0.8의 확률로 1이거나 참일 확률입니다. 20퍼센트 또는 0.2의 확률로 거짓일 것입니다. 0이 되는 경우를 말하죠.

그다음으로 할 일은, 세번째 층에서 activation을 가집니다. 밑에 예제에서 $a^3$라고 부르겠습니다. 그러면 $a^3$를 산출하는 activation이 있습니다. 그리고 $a^3$를 이전의 $a^3$과 곱하면 element wise multiplication인데요. 이것은 또한 $a^3 * d^3$와 동일하다고 적을 수 있습니다. 이것이 하는 것은 $d^3$의 모든 요소가 0인 경우 각각의 element가 0인 경우는 20퍼센트 확률이었는데요. 여기 이 multiply operation은 $d^3$의 요소들을 0으로 만드는 역할을 합니다.

파이썬에서 하면 $d^3$가 boolean array가 됩니다. 그 값이 참, 거짓인 경우를 얘가하죠, 0과 1의 값 대신에요. 그러나 multiply operation은 잘 작동하기 때문에 참, 거짓의 값을 0과 1로 이해하도록 하겠습니다. 이 과정을 직접 파이썬에서 시도해보시면 확인할 수 있습니다.

마지막으로 $a^3$를 가지고 확장을 할 것입니다. 50 단위 또는 50개의 신경세포가 세번째 레이어에 있다고 가정해보겠습니다. 그러면 $a^3$가 $50 \times 1$ 또는 인수분해를 하는 경우 $50 \times m$ 차원일 수 있겠습니다.

그럼 80퍼센트의 확률로 유지하거나, 20퍼센트 확률로 제거하는 경우의 수가 있다고 해봅시다. 이 뜻은 평균적으로 10 유닛은 0으로 된다는 뜻과 같습니다. 그러면 이제 $z^{[4]}$의 값을 보면 $z^{[4]} = w^{[4]} \cdot a^{[3]} + b^{[4]}$일 것입니다. 그러면 예상되는 것은 20퍼센트까지 줄어들 것입니다.

무슨 뜻이냐면 20퍼센트의 $a^3$의 element가 0이 될 것이라는 겁니다. 그러면 $z^{[4]}$의 기대값을 감소하지 않게 하기 위해서는 이것을 가져야 합니다. 그리고 0.8로 나눕니다. 왜냐면 이게 조정되거나 다시 20퍼센트만큼 올라갈 수 있기 때문입니다. 그러면 $a^3$의 기대 값이 변하지 않았습니다.

inverted dropout technique의 효과는 keep.prob을 어떻게 설정하더라도 0.8이던 0.9이던 심지어 1이더라도 만약 1로 설정이 되면 dropout은 없습니다. 0.5 이던 어떤 값이라도 모두 유지하기 때문이죠. iverted dropout technique은 keep.prob으로 나누면서 $a^3$의 기대값이 동일하게 유지하도록 해줍니다.

그리고 테스트를 하는 경우 신경망을 평가할 때에 inverted dropout technique는 초록색 박스를 그린 부분이 바로 해당하는 부분인데요. 이것이 test time을 쉽게 만들어 줍니다. Scaling 문제가 덜하기 때문이죠.

가장 흔한 dropout의 도입방식은 바로 invert dropout방식입니다. 이전에 dropout에서 반복 테스트 업무를 하던 때에, keep.prob라인 부분을 사용하지 않던 적이있습니다. 그렇게 해서 test time에서 평균이 더욱 더 점차 복잡해지는 것이죠. 하지만 다른 버전들은 이제는 잘 쓰지 않습니다. 그러면 d 벡터를 사용해서 훈련 예제의 유형에 따라 서로 다른 은닉 유닛을 0으로 만듭니다. 

동일한 훈련 세트에서 multiple pass를 진행하면, 훈련 세트에 통과하는 다른 pass들에 따라서 다른 은닉 유닛을 무작위로 0으로 만들 것입니다. 꼭 동일한 은닉 유닛을 0으로 만들고 한 기울기 강하 수행절차에서 말이죠. 그리고 또 다른 2번째 기울기 강하에서 훈련 세트를 2번째로 통과시켜서 또 다른 패턴이 은닉 유닛을 0으로 만들어야 하는 것이 아닙니다. 3번째 층의 벡터 d 또는 $d^3$는 어떤 것을 0으로 만들지 결정하는데 사용됩니다. 순전파와 역전파 모두에서 말이죠. 여기서는 순전파만 보여주고 있습니다.

![image](https://user-images.githubusercontent.com/55765292/177448054-8e8abf04-573c-4ec6-a12f-c7d583da71e2.png)

test time에서 알고리즘을 트레이닝 했으니 이제 새롭게 해보겠습니다. test time에서는 X라는 것이 주어지고, 또는 예측하고 싶어하는 상대가 주어집니다. 그리하여 표준 노테이션을 사용해여 여기서 $a^{[0]}$을 사용하겠습니다. 0 레이어의 activation을 나타내는데요. 테스트 예시 X를 나타냅니다.

저희가 할 것은 테스트 타임이 dropout을 사용하지 않을 것입니다. 특히 $z = wa + b$을 말이죠. 식을 따라서 마지막 레이어인 예측수치 $\hat{y}$까지 내려갑니다.

확인할 수 있는 것은 dropout을 여기서 명백히 사용하지 않고 동전을 무작위로 던지지 않고 어떤 은닉 유닛을 제거할지 동전을 던지지 않는다는 것입니다. 그 이유는 test time에서 예측을 하는 경우 여러분의 결과값이 임의의 숫자가 되면 좋지 않기 때문입니다.

만약 dropout을 테스트타임에 도입한다 하면 그것은 예측수치에 noise를 더할 뿐입니다. 이론적으로 할 수 있는 것은 예측을 몇 번이고 진행하는 것입니다. 임의로 dropout된 은닉 유닛을 가지고 말이죠. 하지만 산출하는 이런 과정이 효율적이지 않고 어렴풋이 비슷한 결과값을 줄 것입니다. 진행절차는 다르지만 결과는 아주 비슷할 것입니다.

inverted dropout은 이전 라인에서 진행한 것을 기억하시겠지만 keep.prob으로 나눴었습니다. 이 효과는 테스트 타임에 scaling에서 dropout을 도입하지 않더라도 여기 이런 activation의 기대값은 변하지 않습니다. 그러므로 이렇게 쌩뚱맞은 부가적인 scaling 매개 변수를 넣을 필요가 없습니다. 이것은 training time에서 있었던 것과는 다릅니다.

다음은 dropout이 정확히 무엇을 하는지 알아보겠습니다.
