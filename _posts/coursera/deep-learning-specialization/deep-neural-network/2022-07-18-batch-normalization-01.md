---
title: "[Ⅱ. Deep Neural Network] Batch Normalization (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - Hyperparameter Tuning
toc: true
toc_sticky: true
toc_label: "Batch Normalization (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Batch Normalization

### Normalizing Activations in a Network

딥러닝의 상승하는 시점에서 가장 중요했던 아이디어 중 하나는 정규화라는 알고리즘입니다. 이것은 2명의 개발자가 만들었는데요. Sergey Ioffe 와 Christian Szegedy입니다. 배치 정규화를 통해 하이퍼 파라미터 검색 문제가 훨씬 쉬워지고 신경망을 더욱 튼튼하게 만들어주죠.

하이퍼 파라미터의 선택은 잘 작동하는 훨씬 더 큰 범위의 하이퍼 파라미터인데요. 심층 네트워크도 훨씬 쉽게 교육할 수 있습니다. 그러면 배치 정규화가 어떻게 작동하는지 한번 보겠습니다 .

---

![image](https://user-images.githubusercontent.com/55765292/179444705-0fe28909-0d9e-4fc9-aa71-6c68704bdd6f.png)

모델을 훈련 하는 경우 로지스틱 회귀분석이나 같은 경우에 입력 기능을 정규화하면 평균을 계산할 때 학습 속도를 높일 수 있다는 것을 기억 하실 겁니다. 훈련 세트에서 평균값을 빼고 분산을 계산합니다. xi 제곱의 합이죠. 이것은 요소별 제곱입니다. 그리고 분산대로 데이터를 정규화 시켜줍니다.

그리고 이전에 이것이 학습 문제의 윤곽을 매우 길어질 수 있는 것에서 더 둥글고 경사 하강과 같은 알고리즘이 최적화하기 쉬운 것으로 바꾸는 방법을 보았습니다. 따라서 이것은 입력 특성 값을 신경망으로 정규화하는 측면에서 회귀를 변경하는 효과가 있습니다.

더 깊은 모델은 어떨까요? 입력 특성 x 만 있는것이 아니라 각 층에는 활성화 a1 도 있고 활성화 a2 등이 있습니다. 그러면 파라미터를 트레이닝 하고 싶은 경우엔, 예를 들어, w3, b3과 같이 말이죠, a2의 평균과 분산을 정규화하여 w3, b3의 훈련을 더 효율적으로 만들 수 있다면 좋지 않을까요?

로지스틱 회귀분석 같은 경우엔 x1,x2,x3를 정규화 시켜줌으로써 w와 b를 더 효율적으로 훈련 시킬 수 있다는 것을 알았습니다. 여기서 어떤 은닉 층에 대해 정규화 할 수 있는지가 의문입니다. a의 값을, a2라고 하겠는데요, 여기 예제에서는 어떤 은닉 층에서도 이런데요. w3 b3를 더 빨리 훈련시키려면 a2는 다음 층의 입력이기 때문에 w3와 b3의 훈련에 영향을 줍니다.

그러면 이것이 배치 노름, 배치 정규화가 하는 역할인데요. 보통 줄여서 배치 노름이라고도 합니다. 기술적으로는 실제로 a2가 아닌 z2의 값을 정규화합니다. 활성화 함수 이전에 값을 정규화해야 하는지 여부에 대해 딥 러닝 문헌에 몇 가지 논쟁이 있는데요. z2 또는 a2 활성화 함수를 적용한 다음에 정규화를 해야 하는지에 대한 토론이 있습니다. 실제로는 z2를 정규화 하는 것을 훨씬 더 자주 수행합니다. 그러므로 그 버전을 보여드리겠는데요. 그리고 저는 이 값을 기본 값으로 하도록 추천드리겠습니다 

---

![image](https://user-images.githubusercontent.com/55765292/179444720-7c158c5f-5ac1-4116-a71d-beb8c7759ea9.png)

그러면 배치 노름을 어떻게 도입하는지 보겠습니다. 신경망에서는 중간값이 있는데요. z1에서 zm까지의 은닉 단위 값이 있고 이것은 실제로 은닉 층에서 가져온 것이라고 가정해 보겠는데요. 그러므로 이것을 z로 쓰는 것이 조금 더 정확하겠죠. 어떤 은닉층 i에 대해서 말이죠. i는 1에서 m까지가 되겠습니다. 하지만 적는 것을 줄이기 위해서 여기 [l]는 생략하겠습니다. 여기 라인에 있는 표기를 심플하게 하기 위해서 말이죠.

이런 값들이 주어지면 이제 평균값을 아래와 같이 구합니다. 그리고 분산의 값은 여러분이 예상하는 공식을 이용하여 계산하고요. zi들을 각각 이용해서 정규화합니다. 따라서 평균을 빼고 표준편차로 나누어 zi를 정규화합니다. 수치적 안정감을 주기 위해서 보통 앱실론을 분모에 더합니다. 이것은 어떤 추정치의 경우 0이 되는 경우도 있습니다. 이제 우리는 이 값 z를 취하여 평균 0과 표준 단위 분산을 갖도록 정규화했습니다.

그럼 z의 모든 요소는 평균 값 0과 분산 1의 값을 갖습니다. 그렇지만 저희는 은닉 유닛이 항상 평균값 0 과 분산 1의 값을 갖길 바라진 않죠. 은닉 유닛이 다른 분포를 갖는 것이 합리적일 수 있으므로 대신 우리가 할 일은 계산. 이것을 z틸더 = 감마 zi 노름 + 베타라고 부르겠습니다. 여기에서 감마와 베타는 모델의 학습 가능한 파라미터입니다. 그러므로 저희는 기울기 하강, 또는 다른 알고리즘을 이용할 것인데요. 모멘텀의 기울기 하강 또는 RMSProp 또는 Adam을 이용해서 감마와 베타 파라미터를 업데이트 할 것인데요. 신경망의 가중을 업데이트 하는 것과 마찬가지로 말이죠.

그럼 감마와 베타의 효과는 z 틸더의 평균값이 여러분이 원하는 값으로 되게 해주는 것입니다. 사실, 만약 감마가 시그마 제곱 더하기 앱실론의 루트값인 경우, 즉 감마가 여기 분모 항이라고 하면 그리고 만약 베타가 뮤와 같다면, 즉 여기 위의 값이라면 감마 z 노름 더하기 베타의 효과는 이 식을 거꾸로 하는 것과 일치합니다. 그래서 이것이 만약 맞다면 z 틸더 i 는 zi와 같을 것입니다. 그러면 감마와 베타 파라미터의 적합한 설정을 통해 여기 정규화 단계는, 즉 여기 4개의 식은 항등 함수를 계산합니다.

하지만 감마와 베타값을 다르게 고르면서 이것은 여러분이 은닉 유닛이 다른 평균값과 분산을 갖게 해줍니다. 그러면 이것을 신경망에 맞추는 방법은 이전에는 이러한 z1, z2와 같은 값 등등 이제는 여러분이 zi 대신에 z 틸더 i를 신경망에서 나중 계산에 사용하겠습니다.

여기서 얻어가실 직관적인 부분은 우리가 알게 된것이 x의 입력값을 정규화함으로써 신경망의 학습을 도와줄 수 있다는 것입니다. 그리고 배치 노름이 하는 것은 정규화 절차를 적용하여 입력 층에만이 아니라 신경망의 일부 은닉 층에 있는 값까지 적용합니다.

따라서 이러한 유형의 정규화를 적용하여 일부은닉 단위 값 z의 평균과 분산을 정규화합니다. 하지만 훈련 입력 값과 은닉 단위의 한가지 차이는 여러분은 은닉 단위가 강제적으로 평균값 0과 분산 1이 되는 것을 원치 않을 수 있다는 것입니다.

예를 들어, 시그모이드 활성화 함수의 경우 가치가 항상 여기에 군집되는 것을 원하지 않아요. 이 것이 더 큰 분산이나 평균값을 가져 0보다 다른 값을 가지면서 비선형의 시그모이드 함수가 나오길 바랄 것인데요. 선형함수의 모양을 갖는 것보다 말이죠.

그러므로 파라미터 감마와 베타를 이용해서 zi 값이 원하는 범위의 값을 갖도록 할 수 있는 것입니다. 하지만 실질적으로 하는 것은 은닉 단위가 평균화 한 평균 값과 분산 값을 갖고 감마와 베타와 같이 학습 알고리즘이 설정할 수 있는 값들에 의해 조정됩니다.

따라서 실제로 하는 일은 이러한 은닉 단위 값, 실제로는 zi들의 평균과 분산에서 일부 고정된 평균과 분산을 갖도록 정규화하는 것입니다. 평균값과 분산은 각각 0과 1일 수 있고 또는 또 다른 값이 될 수도 있는데요. 그리고 이것은 감마와 베타 파라미터에 의해 제어됩니다
