---
title: "[Ⅱ. Deep Neural Network] Optimization Algorithms (6)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Optimization Algorithms (6)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Optimization Algorithms

### Learning Rate Decay

학습 알고리즘의 속도를 높이는 데 도움이 될 수 있는 것 중 하나는 시간이 지남에 따라 학습률을 천천히 줄이는 것입니다. 이것을 학습률 감소라고 합니다. 이것을 어떻게 구현할 수 있는지 한번 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/178630333-9b77e5f0-8f87-4acf-820d-de63fa6fe690.png)

왜 학습률 감소를 써 볼 필요가 있는지 그에 대한 예로 시작해보죠. 미니 배치가 상당히 작은 미니 배치를 사용하여 미니 배치 경사 하강을 구현한다고 가정하면 미니 배치는 64, 128개의 예제만 있을 수 있습니다. 그리고 반복하면 단계에 약간의 노이즈와 최소값으로 가는 경향은 있겠지만 정확히 수렴되지는 않을 겁니다.

하지만 알고리즘은 결국 주변을 배회하고 절대 수렴되지 않을 수도 있습니다. 왜냐하면 알파에 대해 고정된 값을 사용하고 있고 다른 미니 배치에 약간의 노이즈가 있기 때문입니다.

만약 천천히 학습률 알파 그리고 초기 단계 동안 학습률 알파가 여전히 큰 동안 비교적 빠르 학습이 가능할 것입니다. 하지만 알파가 작아지면 단계는 더디고 작아질거예요. 그래서 훈련이 계속되어도 멀리 돌아다니기 보다는 이 최소의 좁은 지역에서 진동하게 됩니다.

알파를 천천히 감소시키는 이면에 있는 직관적인 이유는 아마도 학습의 초기 단계에서 훨씬 더 큰 단계를 밟을 수 있지만 학습이 융합에 가까워질수록 학습률이 늦려지면 더 작은 단계를 밟을 수 있습니다.

![image](https://user-images.githubusercontent.com/55765292/178630366-d7c40933-7cc5-4ac1-a20a-23e5bab67eab.png)

학습률 감소의 구현은 이렇습니다. 하나의 에포크는 데이터를 한 번 통과하는 것입니다. 그럼 이런 훈련 세트가 있다고 하면 아마도 그것은 다른 미니 배치들로 나눌 수 있을 건데요. 그리고 훈련 세트를 통과하는 첫 번째 통과는 첫 번째 에포크라고 불리며, 두번째 통과를 둘째 에포크라고하고 이런 식으로 이어집니다.

여기서 할 수 있는 것은 알파를

$\alpha = \cfrac{1}{1 + decay-rate * epoch-num} \alpha_0$

로 나타내는 것입니다.

감소율에 에포크의 넘버를 곱합니다. 이건 초기 학습률 알파_0의 몇 배입니다. 여기 감소율은 또 하나의 하이퍼파라미터가 되는데 잘 알아야합니다. 여기 구체적인 예가 있습니다. 만약 몇 개의 에포크를 가지고 있다면, 그래서 몇 개는 데이터를 통과하는데, 만약 알파가 0.2와 같고 감소율은 1과 같다면 첫번째 에포크에서 알파 즉 학습률은 0.1이 될 것입니다.

그것은 감소율이 1이고 에포크-넘버가 1일 때 위 공식에서 나온 값입니다. 두번째 에포크에서는 0.067, 세번째 0.05, 네번째는 0.04,.. 가 됩니다. 이러한 값들 중 더 많은 값을 스스로 계산해보고 학습률이 점차 감소한다는 것을 느껴보세요.

학습률 감소를 사용하려면 이 감소율 하이퍼 파라미터뿐만 아니라 하이퍼 파라미터 알파_0의 다양한 값을 시도하고 제대로 작동하는 값을 찾는 것입니다. 이 학습률 감소의 수식 외에도 사람들이 쓰는 몇가지 또 다른 방법들이 있습니다.

![image](https://user-images.githubusercontent.com/55765292/178630381-2135840a-3362-4d6b-b4b7-da9066184827.png)

예를 들어 지수 감소라고하는 것입니다. 알파 값은 1보다 작은 값인데, 예를 들어, $\alpha = 0.95^{epoch-num} \times \alpha_0$입니다. 이것은 학습률을 기하급수적으로 떨어뜨릴 것입니다.

사람들이 사용하는 다른 공식들은 다음 알파와 같은 것들입니다. $\alpha = \cfrac{k}{\sqrt{epoch-num}} \times \alpha_0$ 또는 $\alpha = \cfrac{k}{\sqrt{t}} \times \alpha_0$.

종종 사람들이 다음을 이해하고 분해하는 학습률을 사용하는 것을 보곤하는데요. 여기서 몇 가지 단계는 어느 정도의 학습률을 가지고 있는데, 그리고나서 잠시 후 2분의 1로 줄이고 또 2분의 1로 줄이고 이것은 별개의 계단입니다.

지금까지 시간이 지남에 따라 학습률인 알파가 어떻게 변화하는지 관리하기 위해 공식을 사용하는 것에 대해 이야기했습니다. 가끔 사람들이 하는 한 가지 방법은 수작업식 감소입니다.

만약 한번에 한가지 모델로 트레이닝 한다면, 만약 모델이 훈련하는데 많은 시간 혹은 심지어 많은 날이 걸린다면, 어떤 사람들은 모델을 그저 며칠 동안 훈련하는 동안 지켜보기만 하면 됩니다. 그리고 나서 학습률이 느려진 것처럼 보인다, 알파를 조금 줄일 것이라고 말합니다.

물론 이것은 수동으로 알파를 조종하고 실제로 손으로 알파를 조정하여 매시간 매일 작동합니다. 이 방법은 소수의 모델만 교육하는 경우에만 효과가 있지만 때때로 다른 사람들도 그렇게 합니다.

---

이제 학습률 알파를 제어하는 방법에 대한 몇 가지 추가 옵션이 있습니다. 수많은 하이퍼 파라미터가 있다고 생각하신다면, 이 많은 옵션 중에서 어떻게 선택해야 할까요? 일단, 저는 걱정하지 마시라고 말씀드리고요. 그리고 다음주에 하이퍼 파라미터를 체계적으로 선택하는 방법에 대해 자세히 알아보겠습니다.

학습률 감소는 시도하는 것 중에 낮은 쪽에 있다고 말할 수 있습니다. 알파를 단지 알파의 고정된 값으로 설정하고 잘 튜닝되도록 하는 것은 큰 영향을 미치는데 학습률 감소가 도움이 됩니다. 가끔 훈련 속도를 정말로 올려주기도 하지만 해볼만한 것들에 대해서는 조금 아래쪽에 있습니다.

다음에는 하이퍼 파라미터 튜닝에 관한 이야기를 하면서 이러한 모든 하이퍼 파라미터를 보다 체계적으로 구성하고 효율적으로 검색하는 방법을 확인할 수 있습니다. 여기까지가 학습률 감소이었고요. 마지막으로 신경망의 로컬 최적화와 안장점에 대해 조금 더 자세히 설명하겠습니다.

그러면 최적화 알고리즘이 이러한 신경망을 훈련시킬 때 해결하고자하는 최적화 문제의 유형에 대해 조금 더 나은 직관을 가질 수 있습니다.
