---
title: "[Ⅱ. Deep Neural Network] Optimization Algorithms (5)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Optimization Algorithms (5)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Optimization Algorithms

### RMSprop

모멘텀을 사용하면 경사 하강 속도를 높일 수 있습니다. 또 다른 RMSprop라는 알고리즘이 있는데요. Root Mean Square prop의 약자로 경사 하강의 속도를 높일 수도 있습니다. 어떻게 작동하는지 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/178430401-031e72d3-1aef-47eb-b030-1b199f8a5dde.png)

이전 사례를 상기해보면 경사 하강을 구현하면 수직 방향으로 엄청난 진동이 발생할 수 있으며 수평 방향으로 나아가는 동안에도 말이죠. 이 예에 대한 직관을 제공하기 위해 수직 축이 매개변수 b이고 수평 축이 매개변수 w라고 가정해 보겠습니다. $w_1, w_2$가 될 수 있으며, 여기서 일부 중심 매개변수는 직관을 위해 b와 w로 명명됩니다. 따라서 학습 속도를 b방향이나 수직방향으로 늦추고자 합니다.

그리고 학습 속도를 높이거나 최소한 수평 방향으로 속도를 늦추지 마세요. 이를 위해서 RMSprop 알고리즘이 수행하는 작업은 다음과 같습니다. 반복 t에서는 현재 미니 배치에서, 미분 $dw,db$를 평소처럼 계산합니다. 이 지수 가중 평균은 유지하려고 했는데요. $V_{dw}$ 대신 $S_{dw}$ 표기를 사용하겠습니다.

따라서 $S_{dw} = \beta S_{dw} + (1-\beta)dw^2$와 같습니다. 비용을 설명하기 위해 $dw^2$로 쓰겠습니다. 명료성을 위해 여기 제곱 연산은 요소별 제곱 연산입니다. 즉 미분 제곱의 지수 가중 평균을 유지하는 것입니다. 비슷하게 $S_{db} = \beta S_{db} + (1-\beta)db^2$가 있습니다. 그리고 다시 제곱은 요소별 연산입니다.

다음으로 RMSprop은 다음과 같이 파라미터를 업데이트합니다.

$w := w - \alpha \cfrac{dw}{\sqrt{S_{dw}+\epsilon}}$

이전에는 알파 곱하기 $dw$였지만 이제는 $dw$를 $S_{dw}$의 제곱근으로 나누었습니다. 그리고 $b$는 $b$에서 학습률 시간을 뺀 값으로 업데이트되는데요. 그림의 옆에 식으로 나타납니다.

이제 이게 어떻게 작동하는지 직관적인 부분을 다루어 보겠습니다. 가로 방향으로 또는, 이번 예제에서는, w 방향에서 매우 빠르게 학습하기를 원합니다. 반면에 수직 방향 또는, 이 예제에서는, b 방향의 모든 진동을 수직 방향으로 느리게 하고 싶습니다.

$S_{dw}$와 $S_{db}$는, 우리가 하고 싶은 것은, $S_{dw}$는 상대적으로 작기 때문에, 여기서는 비교적 작은 숫자로 나눕니다. 반면 $S_{db}$는 상대적으로 크므로, 수직적 차원에서 업데이트를 늦추기 위해 yt를 상대적으로 크게 나눕니다

그리고 실제로 미분을 보면, 이 미분은 훨씬 수평 방향에서보다 수직 방향으로 더 큽니다. 그러면 기울기는 b방향 으로 매우 크게되죠. 따라서 이와 같은 미분의 경우 이것은 매우 큰 db와 상대적으로 작은 dw입니다. 왜냐하면 함수는 수평 방향보다 b방향, w방향보다 수직방향으로 훨씬 더 가파르게 기울어져 있기 때문입니다

따라서 db 제곱은 상대적으로 클 것입니다. 따라서 Sdb는 상대적으로 큰 반면 그 dW에 비해 더 작을 것입니다. 또는 dW 제곱은 더 작거나, SdW가 더 작을 것입니다. 따라서 이것의 순 효과는 수직 방향의 상승일을 훨씬 더 큰 숫자로 나누어 진동을 완화하는 데 도움이 된다는 것입니다. 반면 수평 방향의 업데이트는 더 작은 숫자로 나뉩니다.

따라서 RMSprop를 사용할 경우 업데이트 내용이 이와 더 비슷해집니다. 수직 방향, 그리고 수평 방향은 계속 진행하시면 됩니다. 그리고 이것의 한 가지 효과는 따라서 더 큰 학습률 알파를 사용할 수 있고 수직 방향으로 발산하지 않고 더 빠른 학습을 얻을 수 있다는 것입니다.

좀 더 알기 쉽게 이것을 설명하기 위해 수직과 수평의 방향을 b와 w라고 불렀습니다. 실제로는 아주 고차원의 파라미터 공간에 있는 것인데요. 따라서 진동을 완화하려는 수직 치수는 파라미터 w1, w2, w17의 합계가 될 수 있습니다 그리고 수평 치수는 w3, w4 등이 될 수 있습니다.

실제로 dw는 굉장히 고차원의 파라미터 벡터입니다. db 또한 매우 고차원의 파라미터 벡터인데요. 하지만 직관은 이러한 진동을 얻는 차원에서 이렇게 더 큰 합을 산출하게 됩니다. 이러한 제곱과 미분에 대한 가중 평균이므로 결국 이러한 진동이 있는 방향을 버리게 됩니다.

이것이 RMSprop인데요. 이것은 Root Mean Squared Prop의 약자이고 왜냐하면 여기서는 미분을 제곱하고 여기서는 마지막 부분에서 제곱근을 적용하기 때문입니다.

마지막으로 계속 진행하기 전에 이 알고리즘에 대한 몇 가지 마지막 세부 사항만 살펴보겠습니다. 다음에는 RMSprop과 모멘텀을 합칠 것입니다. 그래서 모멘텀에 사용했던 하이퍼파라미터 베타를 사용하기 보다는 충돌하지 않기 위해 이 하이퍼 파라미터를 베타 2라고 부르겠습니다. 모멘텀과 RMSprop 모두에 대해 동일한 하이퍼파라미터 입니다. 또, 알고리즘이 0으로 나뉘지 않게 하기 위한 방법입니다.

sdW의 제곱근이 바로 0에 가까우면 어떻게 될까요? 그러면 여기 이 부분이 폭발적인 값을 가질 것입니다. 단지 수치 안정성을 보장 하기 위해 실제로 이것을 구현할 때 분모에 아주 작은 엡실론을 추가합니다. 어떤 앱실론이 쓰여도 사실 상관은 없습니다. 10의 -8승이 합리적인 기본값이지만 이것은 수치적 반올림이나 어떤 이유로든 약간 더 큰 수치적 안정성을 보장하는데요. 아주 작은 숫자로 나누지 않도록 하기 위한 것입니다.

이것이 RMSprop인데요. 모멘텀과 비슷하게 그것은 기울기 하강, 미니 배치 기울기 하강에서 변동을 무디게하는 효과가 있습니다. 그리고 더 큰 학습률 알파를 사용할 수도 있습니다. 그리고 당연히 알고리즘 학습률을 높혀줍니다.

이제 RMSprop을 구현하는 방법을 알게 되었으며, 이는 학습 알고리즘의 속도를 높이는 또 다른 방법이 될 것입니다. 모멘텀에 대해서 이야기 했고 RMSprop에 대해서도 이야기했죠. 결국, 그것들을 합치면 훨씬 더 나은 최적의 알고리즘이 나옵니다. 이 내용은 다음에 이야기하겠습니다.
