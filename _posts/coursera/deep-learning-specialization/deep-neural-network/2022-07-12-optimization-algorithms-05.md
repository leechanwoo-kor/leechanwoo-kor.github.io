---
title: "[Ⅱ. Deep Neural Network] Optimization Algorithms (5)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Optimization Algorithms (5)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Optimization Algorithms

### RMSprop

모멘텀을 사용하면 경사 하강 속도를 높일 수 있습니다. 또 다른 RMSprop라는 알고리즘이 있는데요. Root Mean Square prop의 약자로 경사 하강의 속도를 높일 수도 있습니다. 어떻게 작동하는지 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/178430401-031e72d3-1aef-47eb-b030-1b199f8a5dde.png)

이전 사례를 상기해보면 경사 하강을 구현하면 수직 방향으로 엄청난 진동이 발생할 수 있으며 수평 방향으로 나아가는 동안에도 말이죠. 이 예에 대한 직관을 제공하기 위해 수직 축이 매개변수 b이고 수평 축이 매개변수 w라고 가정해 보겠습니다. $w_1, w_2$가 될 수 있으며, 여기서 일부 중심 매개변수는 직관을 위해 b와 w로 명명됩니다. 따라서 학습 속도를 b방향이나 수직방향으로 늦추고자 합니다.

그리고 학습 속도를 높이거나 최소한 수평 방향으로 속도를 늦추지 마세요. 이를 위해서 RMSprop 알고리즘이 수행하는 작업은 다음과 같습니다. 반복 t에서는 현재 미니 배치에서, 미분 $dw,db$를 평소처럼 계산합니다. 이 지수 가중 평균은 유지하려고 했는데요. $V_{dw}$ 대신 $S_{dw}$ 표기를 사용하겠습니다.

따라서 $S_{dw} = \beta S_{dw} + (1-\beta)dw^2$와 같습니다. 비용을 설명하기 위해 $dw^2$로 쓰겠습니다. 명료성을 위해 여기 제곱 연산은 요소별 제곱 연산입니다. 즉 미분 제곱의 지수 가중 평균을 유지하는 것입니다. 비슷하게 $S_{db} = \beta S_{db} + (1-\beta)db^2$가 있습니다. 그리고 다시 제곱은 요소별 연산입니다.

다음으로 RMSprop은 다음과 같이 파라미터를 업데이트합니다.

$w := w - \alpha \cfrac{dw}{\sqrt{S_{dw}+\epsilon}}$

이전에는 알파 곱하기 $dw$였지만 이제는 $dw$를 $S_{dw}$의 제곱근으로 나누었습니다. 그리고 $b$는 $b$에서 학습률 시간을 뺀 값으로 업데이트되는데요. 그림의 옆에 식으로 나타납니다.

이제 이게 어떻게 작동하는지 직관적인 부분을 다루어 보겠습니다. 가로 방향으로 또는, 이번 예제에서는, w 방향에서 매우 빠르게 학습하기를 원합니다. 반면에 수직 방향 또는, 이 예제에서는, b 방향의 모든 진동을 수직 방향으로 느리게 하고 싶습니다.

$S_{dw}$와 $S_{db}$는, 우리가 하고 싶은 것은, $S_{dw}$는 상대적으로 작기 때문에, 여기서는 비교적 작은 숫자로 나눕니다. 반면 $S_{db}$는 상대적으로 크므로, 수직적 차원에서 업데이트를 늦추기 위해 yt를 상대적으로 크게 나눕니다

그리고 실제로 미분을 보면, 이 미분은 훨씬 수평 방향에서보다 수직 방향으로 더 큽니다. 그러면 기울기는 b방향 으로 매우 크게되죠. 따라서 이와 같은 미분의 경우 이것은 매우 큰 db와 상대적으로 작은 dw입니다. 왜냐하면 함수는 수평 방향보다 b방향, w방향보다 수직방향으로 훨씬 더 가파르게 기울어져 있기 때문입니다

따라서 db 제곱은 상대적으로 클 것입니다. 따라서 Sdb는 상대적으로 큰 반면 그 dW에 비해 더 작을 것입니다. 또는 dW 제곱은 더 작거나, SdW가 더 작을 것입니다. 따라서 이것의 순 효과는 수직 방향의 상승일을 훨씬 더 큰 숫자로 나누어 진동을 완화하는 데 도움이 된다는 것입니다. 반면 수평 방향의 업데이트는 더 작은 숫자로 나뉩니다.

따라서 RMSprop를 사용할 경우 업데이트 내용이 이와 더 비슷해집니다. 수직 방향, 그리고 수평 방향은 계속 진행하시면 됩니다. 그리고 이것의 한 가지 효과는 따라서 더 큰 학습률 알파를 사용할 수 있고 수직 방향으로 발산하지 않고 더 빠른 학습을 얻을 수 있다는 것입니다.

좀 더 알기 쉽게 이것을 설명하기 위해 수직과 수평의 방향을 b와 w라고 불렀습니다. 실제로는 아주 고차원의 파라미터 공간에 있는 것인데요. 따라서 진동을 완화하려는 수직 치수는 파라미터 w1, w2, w17의 합계가 될 수 있습니다 그리고 수평 치수는 w3, w4 등이 될 수 있습니다.

실제로 dw는 굉장히 고차원의 파라미터 벡터입니다. db 또한 매우 고차원의 파라미터 벡터인데요. 하지만 직관은 이러한 진동을 얻는 차원에서 이렇게 더 큰 합을 산출하게 됩니다. 이러한 제곱과 미분에 대한 가중 평균이므로 결국 이러한 진동이 있는 방향을 버리게 됩니다.

이것이 RMSprop인데요. 이것은 Root Mean Squared Prop의 약자이고 왜냐하면 여기서는 미분을 제곱하고 여기서는 마지막 부분에서 제곱근을 적용하기 때문입니다.

마지막으로 계속 진행하기 전에 이 알고리즘에 대한 몇 가지 마지막 세부 사항만 살펴보겠습니다. 다음에는 RMSprop과 모멘텀을 합칠 것입니다. 그래서 모멘텀에 사용했던 하이퍼파라미터 베타를 사용하기 보다는 충돌하지 않기 위해 이 하이퍼 파라미터를 베타 2라고 부르겠습니다. 모멘텀과 RMSprop 모두에 대해 동일한 하이퍼파라미터 입니다. 또, 알고리즘이 0으로 나뉘지 않게 하기 위한 방법입니다.

sdW의 제곱근이 바로 0에 가까우면 어떻게 될까요? 그러면 여기 이 부분이 폭발적인 값을 가질 것입니다. 단지 수치 안정성을 보장 하기 위해 실제로 이것을 구현할 때 분모에 아주 작은 엡실론을 추가합니다. 어떤 앱실론이 쓰여도 사실 상관은 없습니다. 10의 -8승이 합리적인 기본값이지만 이것은 수치적 반올림이나 어떤 이유로든 약간 더 큰 수치적 안정성을 보장하는데요. 아주 작은 숫자로 나누지 않도록 하기 위한 것입니다.

이것이 RMSprop인데요. 모멘텀과 비슷하게 그것은 기울기 하강, 미니 배치 기울기 하강에서 변동을 무디게하는 효과가 있습니다. 그리고 더 큰 학습률 알파를 사용할 수도 있습니다. 그리고 당연히 알고리즘 학습률을 높혀줍니다.

이제 RMSprop을 구현하는 방법을 알게 되었으며, 이는 학습 알고리즘의 속도를 높이는 또 다른 방법이 될 것입니다. 모멘텀에 대해서 이야기 했고 RMSprop에 대해서도 이야기했죠. 결국, 그것들을 합치면 훨씬 더 나은 최적의 알고리즘이 나옵니다. 이 내용은 다음에 이야기하겠습니다.


### Adam Optimization Algorithm

딥러닝의 역사를 보면 많은 연구자들, 아주 유명한 연구자들을 포함해서 최적화 알고리즘을 간혹 제안하여 문제해결에 잘 쓰이는 것을 보여주었습니다. 하지만 그러한 최적화 알고리즘은 훈련 시키고자하는 광범위한 신경망에 그렇게 잘 일반화되지 않았습니다. 시간이 지남에 따라 딥러닝 커뮤니티는 새로운 최적화 알고리즘에 대한 회의감을 어느정도 키웠다고 생각합니다.

많은 사람들이 모멘텀이 있는 경사 하강이 정말 잘 작동한다고 느꼈고 훨씬 더 잘 작동하는 것을 제안하기가 어려웠습니다. RMSprop와 Adam 최적화 알고리즘을 이야기해볼텐데요. 정말 잘 알려진 희귀한 알고리즘 중 하나이며 광범위한 딥러닝 아키텍처에서 잘 작동하는 것으로 나타났습니다.

이 알고리즘은 직접 시도해 보시기를 추천하는데요. 왜냐하면 많은 사람들이 시도해봤고 많은 문제에서 잘 작동하는 것을 보았기 때문입니다. 아담 최적화 알고리즘은 기본적으로 모멘텀과 RMSprop을 취해서 그것들을 합치는 것입니다. 어떻게 작동하는 것인지 알아보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/178624553-ab86c4e5-cf09-463d-8471-a2a1203d0528.png)

Adam을 구현하려면 $V_{dw} = 0, S_{dw} = 0$으로 초기화하면 되며, 마찬가지로 $V_{db}, S_{db} = 0$입니다. 그리고 t를 반복하면 현재 미니 배치를 사용하여 미분 $dw, db$를 계산합니다.

보통 미니 배치 경사 하강을 이용하면 되고 그러고 나서 지수 가중 평균 모멘텀을 합니다. 모멘텀과 RMSprop 모두 베타를 사용하여 하이퍼 파라미터와 구별하기 위해 모멘텀에서 사용하는 것은 베타 1, RMSprop 부분에 사용할 것은 베타 2라고 부르겠습니다.

일반적으로 Adam 구현에는 바이어스 수정도 같이 합니다. 만약 모멘텀만 사용하는 경우 $V_{dw}$를 사용하거나 또는 $V_{dw}$가 수정될 수 있습니다. 하지만 이제 RMSprop부분을 추가하겠습니다.

$w$와 $b$를 위 공식대로 업데이트합니다.

이 알고리즘은 경사 하강 효과를 RMSprop와 경사 하강과 함께 모멘텀과 결합합니다. 이것은 일반적으로 사용되는 학습 알고리즘이며, 이는 매우 다양한 아키텍처의 다양한 신경망에 매우 효과적인 것으로 입증되었습니다.

![image](https://user-images.githubusercontent.com/55765292/178624573-cd39e4ef-e211-415f-b78b-e759bfdcc773.png)

이 알고리즘에는 많은 하이퍼 파라미터가 있습니다. 학습률 파라미터 알파는 여전히 중요합니다. 그리고 일반적으로 튜닝될 필요가 있습니다. 따라서 다양한 값을 시도하고 무엇이 효과가 있는지 보기만 하면 됩니다.

베타_1의 기본 선택은 0.9이므로 이것은 가중 평균 $dw$입니다. 이는 모멘텀과 같은 항입니다. 베타_2의 하이퍼 파라미터 Adam 논문의 저자 Adam 알고리즘은 0.999를 권장하였습니다. 이것은 $db$ 제곱과 마찬가지로 $dw$ 제곱의 이동 가중 평균을 계산하는 것입니다.


앱실론의 선택은 그다지 중요하지 않습니다 하지만 아담 논문의 저자들은 $10^{-8}을 추천합니다. 하지만 이 파라미터 같은 경우 정말 설정할 필요가 없고 굳이 바꿀 필요가 없고 성능에도 거의 영향이 없습니다. Adam을 구현할 때는 일반적으로 사용자가 수행하는 작업은 기본값을 사용하는 것입니다.

베타_1 및 베타_2의 경우 앱실론을 말이죠. 누가 앱실론을 진정으로 튜닝했다고 생각하지 않습니다. 그리고 알파값의 범위를 시도해서 무엇이 가장 잘 작동하는지 알아보세요. 또한 베타_1 및 베타_2를 튜닝할 수도 있지만 그렇게 자주 하지 않습니다.

그러면 Adam의 뜻은 어디서 유래된 것일까요? 아담은 적응적인 모멘트 추정을 나타내며 그래서 베타_1는 미분의 평균값을 계산하고 있습니다. 이것을 1차 모멘트라고 합니다. 베타_2는 다음을 위해 사용됩니다. 제곱의 지수 가중 평균을 계산하고 이를 두 번째 모멘트라고 합니다.

그래서 적응 모멘트 추정이라는 이름이 생겨났습니다. 하지만 다들 그걸 아담 최적화 알고리즘이라고 부릅니다.

지금까지 아담 최적화 알고리즘에 대해 알아보았습니다. 이 기술을 사용하면 신경망을 훨씬 더 빠르게 훈련할 수 있습니다.
