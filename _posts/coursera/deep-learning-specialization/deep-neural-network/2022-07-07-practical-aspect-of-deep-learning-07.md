---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (7)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (7)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Setting Up your Optimization Problem

### Weight initialization for Deep Networks

이전에 심층 신경망에서는 기울기 소실과 폭주 문제가 있을 수 있다는 것을 알았습니다. 이것에 대한 부분적 해결법으로 완전히 해결해 주지는 않지만 신경망의 무작위 초기화 선택을 더 괜찮거나 조심스럽게 하는 것입니다.

이를 이해하기 위해, 하나의 신경 세포를 초기화하는 방법을 예시로 시작해 봅시다. 이것을 심층 네트워크로 일반화할 것입니다.

![image](https://user-images.githubusercontent.com/55765292/177688457-c77280b3-0caf-477b-8473-07f3ccfd239b.png)

단 하나의 뉴런으로 된 예를 살펴보고 그리고 나서 나중에 심층 네트워크에 대해 얘기하겠습니다. 하나의 뉴런으로 4가지 특성을 입력할 수 있는데요. $x_1$에서 $x_4$까지 그리고 $a = g(z)$가 있고 
$\hat{y}$가 출력됩니다.

그러면 z는 위와 같은 식으로 나타나고 b는 일단 0으로 설정하고 무시하겠습니다. 그러면 z가 폭주하지 않으며 너무 작지 않게 하기 위해서 n이 클수록 $w_i$가 더 작기를 원할 것입니다. 왜냐하면 z는 $x_iw_i$의 합이기 때문에 그렇죠. 그리고 이런 항을 더하는 경우 각각의 항이 작은 값을 가져야 좋습니다.

한가지 합리적인 방법은 $w_i$의 분산을 $\cfrac{1}{n}$과 같도록 하는 하는 것으로 여기서 $n$은 뉴런으로 들어가는 입력 특성의 개수입니다. 실제로는 특정 레이어에 대한 가중치 행렬 $W$을 설정하는 것으로 이것은 np.random.randn 다음에 이것이 행렬의 모양이 어떻던 간에 여기 넣고 레이어 l의 각 단일 세포에 제공한 특징의 수 분의 1의 제곱근을 곱합니다. 따라서 이것은 n(l-1)으로 그것이 레이어 l에 들어가는 각각의 유닛별 숫자입니다.

만약에 여러분이 1/n을 쓰는 것 대신에 ReLU 활성화 함수를 쓰는 경우 분산을 2/n으로 설정하는 것이 조금 더 잘 작동합니다. 따라서 ReLU 활성화 함수를 쓸 때 초기화에서 종종 볼 수 있습니다. $g^{[l]}(z) = ReLU(z)$인 경우죠. 

그리고 그것은 확률변수에 얼마나 익순한지에 달려 있습니다. 알고 보니 가우스 확률 변수에 이것의 제곱근을 곱하면 분산이 2/n로 인용되고요. 그리고 n에서 n위첨자 l-1로 간 이유는 이 예에서는 n개의 입력 특성에 있는 로지스틱 회귀를 사용하지만 보다 일반적인 경우 레이어 l에는 해당 레이어의 각 단위가 n(l-1)개의 입력이 있습니다.

따라서 활성화의 입력 특성이 대략적으로 평균 0이고 분산이 1이면 z도 유사한 척도를 가집니다. 그리고 이것은 해결되지 않지만 그것은 기울기 소실 및 폭주 문제를 줄이는 데 확실히 도움이 되는데요. 각각의 가중치 행렬 w를 설정하려하기 때문에 1보다 너무 크지 않고 너무 작지 않아서 빠르게 폭주하거나 소실되지 않도록 합니다.

몇 가지 다른 분산을 언급했습니다. 방금 설명한 버전은 ReLU 활성화 함수로 가정하고 있고 이것은 논문에서 나왔습니다. 또 다른 변수 몇 가지가 있는데요.

tanh 활성화 함수를 이용하는 경우 상수 2를 쓰는 것보다 상수 1을 쓰는 것이 낫다는 논문이있습니다. 따라서 위의 분산으로 항을 대체합니다. 이것을 Xavier 초기화라고 합니다. 또 다른 버전은 Yoshua Bengio와 그의 동료들이 쓴 논문에서 있습니다. 하지만 이 공식을 이용하고 여러 이론적인 정의가 있습니다.

하지만 ReLu 활성화 함수를 이용하는 경우가 가장 흔한 활성화 함수입니다. 그래서 기존의 항을 사용할 것입니다. tanh를 사용한다면 대신 Xavier 초기화를 사용할 수 있습니다.

하지만 실제로는 이 공식들은 모두 시작점을 제시합니다. 가중치 행렬 초기화의 분산에 사용할 기본값을 줍니다. 여기 있는 분산을 원한다면 분산 매개변수가 튜닝할 수 있는 하이퍼 파라미터 중 하나일 수 있습니다. 그래서 이 공식에 곱해지는 또 다른 매개변수를 가지고 이 공식에 곱해지는 또 다른 매개변수를 가지고 하이퍼 파라미터 급증의 일부로 해당 거듭제곱 횟수를 튜닝할 수 있습니다.

때때로 하이퍼 파라미터 튜닝은 적당한 크기 효과가 있습니다. 이것은 튜닝을 시도할 만한 첫 번째 하이퍼 파라미터는 아니지만 이것을 통해 튜닝한 몇 개의 문제들을 접했었는데요. 비교적 도움이 됩니다.

하지만 선호도로만 따지면 밑에 있는 편입니다. 튜닝할 수 있는 다른 하이퍼 파라미터와 비교했을 때 말이죠. 따라서 그것이 가중치를 초기화하는 방법에 대한 합리적인 크기를 선택하는 것뿐만 아니라 기울기 소실 및 폭주 문제에 대한 약간의 직관을 제공하기를 바랍니다.

그것이 가중치를 너무 빠르게 폭주하게 하지 않고 너무 빨리 0으로 줄어들지 않도록 할 것이며 따라서 가중치나 기울기가 폭발적으로 늘어나거나 소실하지 않게 합리적으로 심층 네트워크를 훈련시킬 수 있습니다.

심층 네트워크를 훈련할 때 신경망을 훨씬 더 빠르게 훈련시키는 데 도움이 되는 또 다른 기술입니다.

![image](https://user-images.githubusercontent.com/55765292/177688521-a12d5606-cafe-4d71-82ff-9465baee6b64.png)

