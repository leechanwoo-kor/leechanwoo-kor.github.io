---
title: "[Ⅱ. Deep Neural Network] Hyperparameter Tuning (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - Hyperparameter Tuning
toc: true
toc_sticky: true
toc_label: "Hyperparameter Tuning (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Hyperparameter Tuning

### Tuning Process

지금까지 신경망을 변경하려면 다양한 하이퍼파라미터를 설정해야 한다는 것을 알았습니다. 그러면 최적의 하이퍼 파라미터 세팅을 찾는 방법은 어떻게 알 수 있을까요?

이번에는 가이드라인에 대해서 공유하겠는데요. 하이퍼 파라미터 튜닝 절차를 시스템적으로 조직화하는 방법과 팁에 대한 가이드라인 말이죠. 이런 가이드라인은 이상적으로 좋은 하이퍼 파라미터 값에 수렴하도록 효율적인 방안을 제시해 줄 것입니다. 깊은 네트워크를 훈련하는 과정에서 힘든 부분중에 한가지는 신경써야하는 많은 양의 하이퍼 파라미터 개수입니다.

![image](https://user-images.githubusercontent.com/55765292/179152966-c1d2f9fa-a2c9-426a-9d87-accb67a9b2ba.png)

위 그림처럼 하이퍼 파라미터들은 학습률을 나타내는 알파값, 모멘텀을 쓰는 경우의 모멘텀 값, 아니면 Adam 최적화 알고리즘의 하이퍼 파리미터인 베타1, 베타2 및 앱실론입니다. 레이어의 수를 선택햐아 할 수도 있는데요. 다른 레이어에 대한 은닉 유닛의 수를 선택해야 할 수도 있고 학습률 감소를 사용하고 싶을 수도 있으므로 단일 학습률 알파를 특정 하이퍼 파라미터가 더 특별히 중요할 수 있겠죠.

대부분의 학습 응용 프로그램, 알파, 학습률은 조정해야 하는 가장 중요한 파라미터입니다. 알파 외에 다음에 조정하는 경향이 있는 몇 가지 다른 하이퍼파라미터는 아마도 모멘텀 항이 될 것입니다. 예를 들어 0.9가 좋은 기본값입니다. 또한 최적화 알고리즘이 효율적으로 실행되고 있는지 확인하기 위해 미니 배치 크기를 조정합니다.

종종 은닉 유닛을 만지작거리기도 합니다. 오랜지색으로 동그라미친 것 중에서, 이것들은 학습률 알파에 대해 두 번째로 중요하다고 생각하는 세 가지이고 학습률 알파, 그리고 세번째 중요하다고 생각하는 세가지입니다.

레이어의 개수도 아주 큰 차이를 불러올 수 있고 학습률 감소도 마찬가지입니다. 그리고 Adam 알고리즘을 이용하는 경우에는 거의 항상 베타1, 베타2 및 앱실론입니다. 거의 항상 0.9와 0.999, 10^-8 값으로 사용하는데요. 이 값들도 튜닝할 수 있습니다. 그러나 하이퍼파라미터가 다른 것보다 더 중요할 수 있는 대략적인 감각을 제공하기를 바랍니다.

알파는 가장 중요한 것이 되겠고요. 그 다음으로 오렌지색으로 동그라미 친 것이 중요하고 물론 다른 시각을 가지고 있는 분도 있겠습니다. 그렇다면 직접 하이퍼 파라미터 값을 튜닝하려고 하는 경우에 탐색할 값 집합을 어떻게 선택할까요?

![image](https://user-images.githubusercontent.com/55765292/179152978-c097d56a-2287-48c8-bd1e-9fa5bf4d2aca.png)

머신 러닝 알고리즘의 초기에는 2개의 하이퍼파라미터가 있는 경우에 이 경우에 하이퍼파라미터 1과 하이퍼파라미터 2라고 하겠습니다. 점들을 격자판 형식으로 샘플링하는 경우가 많았습니다. 그리고 이러한 값들을 탐색해나가는 것입니다.

여기서는 5 x 5 격자판 형식으로 설정했는데요. 실제로는 더 큰 격자판이나 이것보다 작은 격자판으로 설정할 수도 있습니다. 이 경우에서는 25개의 포인트가 있고 이 중에서 가장 잘 작동하는 하이퍼파라미터를 고르는 것입니다. 이런 방법은 하이퍼파라미터의 개수가 비교적 많지 않은 경우에 잘 작동하는 편입니다.

딥러닝의 경우 지점들을 임의로 선정하는 것입니다. 그러면 포인트의 개수는 똑같이해서 진행합니다. 그 후 임의로 지정된 포인트에서 임의로 하이퍼파라미터를 골라 시도해봅니다. 그렇게 하는 이유는 문제에 대해 어떤 하이퍼파라미터가 가장 중요할지 미리 알기가 어렵기 때문입니다.

그리고 이전에 보았듯이 특정 하이퍼파라미터들은 다른 것들보다 더 중요할 수 있습니다. 그래서 예를 들어서 하이퍼파라미터 1의 값이 학습률을 나타내는 알파값이었다고 해봅시다. 그리고 극단적인 경우로 하이퍼 파리미터 2는 Adam 알고리즘의 분모에 속하는 앱실론 값이었다고 해보겠습니다.

이런 경우에는 알파값이 매우 중요하고 앱실론의 선택은 겅의 중요하지 않겠죠. 따라서 격자판에서 샘플링하면 5개의 알파 값을 실제로 시도한 것이며 앱실론의 모든 다른 값이 본질적으로 동일한 답을 제공한다는 것을 알 수 있습니다. 그래서 이제 25개의 모델을 훈련했고 학습률 알파에 대해 5개의 값만 시도했는데요.

반대로 샘플링을 무작위로 진행하였을 때는 학습률 알파의 25가지 고유한 값을 시도하게 되므로 실제로 잘 작동하는 값을 찾을 가능성이 더 높아집니다. 예제에서는 오로지 2개의 하이퍼파라미터만 사용하여 설명했지만 실제로는 더 많은 숫자의 하이퍼파라미터를 사용하여 그 값을 찾을 수 있습니다.

그래서 만약 3개의 하이퍼파라미터일 수도 있겠죠. 이런 경우에는 사각형이 아니라 하이퍼파라미터 3인 정육면체에서 값을 찾을 것입니다. 그러면 샘플링을 3차원의 정육면체에서 진행하여 하이퍼파라미터들을 두고 더 많은 값들을 시도할 수 있습니다. 그리고 실제로는 더 많은 수의 하이퍼파라미터에서 검색을 진행할 수도 있습니다.

때로는 어떤 것이 애플리케이션에 정말 중요한 하이퍼파라미터로 판명되는지 미리 알기가 어렵고 격자판이 아닌 무작위로 샘플링하는 것이 더 풍부하다는 것을 보여줍니다. 가장 중요한 하이퍼파라미터가 무엇이든 간에 가능한 값의 합을 탑색합니다.

![image](https://user-images.githubusercontent.com/55765292/179152999-0b6d815d-a2dd-4651-a35b-0d519e41e763.png)

하이퍼파라미터를 샘플링하는 경우에 또 다른 일반적인 방법은 거칠고 미세한 샘플링 방식을 사용하는 것입니다. 2차원 예제에서 포인들을 샘플링한다고 해보겠습니다. 그리하여 가장 잘 작동하는 포인트를 알아냈다고 해봅시다. 아마도 그 주변의 몇 가지 다른 점이 정말 잘 작동하는 경향이 있었을 것인데요.

그런 다음 최종 계획 과정에서 하이퍼파라미터의 더 작은 영역을 확대한 다음 이 공간 내에서 더 많은 밀도를 샘플링할 수 있습니다. 또 다시 임의의 샘플링을 진행할 수도 있겠는데요.

그러나 최상의 설정인 하이퍼파라미터가 이 영역에 있을 수 있다고 의심되는 경우 파란색 사각형 내에서 검색하는 데 더 많은 리소스를 집중할 수 있습니다. 따라서 이 전체 사각형의 대략적인 샘플을 수행한 후 더 작은 사각형의 집중하라는 메시지가 표시됩니다. 그런 다음 더 작은 정사각형으로 더 조밀하게 샘플링 할 수 있습니다. 따라서 이러한 유형의 검색도 자주 사용됩니다. 

그리고 하이퍼파리미터의 다양한 값을 시험해봄으로써 훈련 세트 목표에서 가장 잘 할 수 있는 값이나 개발 세트에서 가장 잘하는 값 또는 하이퍼파라미터 검색 프로세스에서 최적화하려는 모든 값을 선택할 수 있습니다.

따라서 하이퍼파라미터 검색 프로세르를 보다 체계적으로 구성할 수 있는 방법이 제공되기를 바랍니다. 두 가지 핵심 내용은 임의의 샘플링을 진행하고 충분히 검색을 진행합니다. 선택적으로 거친 검색 프로세스에서 정밀 검색 프로세스를 구현하는 것을 고려하세요 그러나 하이퍼파라미터 검색에는 이것보다 더 많은 것이 있습니다.
