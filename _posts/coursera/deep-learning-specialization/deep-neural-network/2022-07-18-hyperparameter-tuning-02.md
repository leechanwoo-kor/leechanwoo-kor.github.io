---
title: "[Ⅱ. Deep Neural Network] Hyperparameter Tuning (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - Hyperparameter Tuning
toc: true
toc_sticky: true
toc_label: "Hyperparameter Tuning (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Optimization Algorithms

## Hyperparameter Tuning

### Using an Appropriate Scale to pick Hyperparameters
지난번에는 여러 범위의 하이퍼파라미터에서 임의의 샘플링을 통해 더 효율적으로 하이퍼파라미터의 공간을 검색하는 방법을 배웠습니다. 하지만 임의로 샘플링을 하는 것은 균일화된 임의의 작업을 뜻하는 것이 아닌데요. 특정 범위의 유효한 값에서 말이죠. 대신에 적절한 척도를 선택하여 하이퍼파라미터를 탐색해야 합니다. 이번에는 그 방법을 살펴보겠습니다.

---

![image](https://user-images.githubusercontent.com/55765292/179431172-46eed3ab-3c07-4edb-b96d-19a1adf0489e.png)

은닉 단위의 수인 $n^{[l]}$을 찾으려고 한다고 가정하면, 1개의 레이어에서 말이죠. 그리고 좋은 범위는 50에서 100사이라고 해보겠습니다. 이런 경우 50에서 100사이 숫자 라인을 보면 이 숫자 라인 내에서 임의의 숫자 값을 선택할 수 있습니다. 이 특정 하이퍼파라미터를 검색하는 꽤 눈에 띄는 방법이 있습니다. 또는 신경망 네트워크 레이어의 개수를 결정하려고 한다면요.

이것을 대문자 L이라고 하겠습니다. 총 레이어의 개수가 2에서 4개 사이여야한다고 생각할 수도 있습니다. 그러면 2, 3, 4에 따라 랜덤으로 균일하게 예제를 추출하는 것이 합리적일 수 있습니다. 또는 심지어 격자판 검색 같은 경우 명백히 2, 3 그리고 4의 값을 평가하는 것이 합리적일 수 있습니다. 이러한 몇 개의 예제들과 같이 균일화된 임의의 방법으로 샘플링을 하는 경우 생각하고 있는 범위가 합리적인 방법일 수도 있습니다. 하지만 이런 방법이 모든 하이퍼파라미터에서 적용되는 것은 아니죠.

---

![image](https://user-images.githubusercontent.com/55765292/179431186-c1773ebc-a834-4499-a629-1a168a9a3870.png)

다른 예제를 보시죠. 학습률인 알파 값을 검색한다고 해보겠습니다. 그리고 이 경우에 0.0001이 낮은 경계선이고 또는 높은 경계선이 1일 수 있겠죠. 자 만약 0.0001에서 1까지인 숫자라인을 그리고 임의로 균일화되게 샘플링을 진행하면 대략적으로 샘플값의 90퍼센트 정도가 0.1과 1사이에 있을 것입니다. 따라서 90% 자원을 사용하여 0.1과 1사이를 검색하고 그리고 자원의 10%만 0.0001과 0.1사이를 검색합니다. 옳은 방법처럼 보이지 않죠.

대신에 로그 스케일에서 하이퍼파라미터를 검색하는 것이 더 합리적으로 보입니다. 선형 척도를 이용하는 대신에 0.0001과 0.001, 0.01, 0.1 그리고 1가 척도에 있습니다. 그리고 이러한 유형의 로그 스케일에서 무작위로 균일하게 샘플링합니다. 그렇게하면 0.0001과 0.001사이와, 0.001과 0.01 사이등 검색에 더 많은 자원을 사용할 수 있습니다.

그러므로 파이썬에서는 이렇게 도입하는게 좋습니다.

`r = -4 * np.random.rand()`

이런식으로 말이죠. 그리고 임의로 선택된 알파값은 $10^r$이 됩니다. 이 첫번째 라인 이후로는 임의 숫자가 -4와 0사이의 값일 것입니다. 그래서 여기 알파값은 $10^{-4}$과 $10^0$사이의 값을 가질 것입니다.

더 일반적인 경우 $10^a, 10^b$을 로그 척도에서 샘플링하려고 하는 경우 말이죠. 그리고 r을 임의로 a와 b사이에서 균일하게 샘플링합니다. 이 경우에는 r의 값은 -4와 0사이입니다. 그리고 알파의 값을 임의로 샘플링된 하이퍼 파라미터의 값인 $10^r$로 지정할 수 있습니다.

요약하자면 로그 척도로 샘플링하려면 낮은 값을 취하고 로그를 취하여 a의 값을 알아냅니다. 높은 값을 취하고 로그를 취하여 b가 무엇인지 알아냅니다. 이제 여러분은 $10^a$에서 $10^b$을 로그 척도에서 샘플링하려 합니다. 그러면 r의 값을 균일화되게 임의로 설정해요. $a$와 $b$사이로 말이죠. 그리고 하이퍼파라미터를 $10^r$로 설정합니다. 그래서 이것은 로그 척도로 샘플링을 구현하는 방법입니다.

---

![image](https://user-images.githubusercontent.com/55765292/179431196-c7ac4c76-d02f-4cea-8660-8888c2990ef7.png)

마지막으로 조금 헷갈리는 부분은 하이퍼파라미터 베타 샘플링인데요. 지수 가중 평균을 계산하는 데 사용됩니다. 베타가 0.9에서 0.999사이 어딘가라고 가정하겠습니다. 따라서 기억할 것은 지수 가중 평균을 계산할 때 0.9의 값을 사용하는 것은 마치 10개의 값에서 평균을 구하는 것과 같습니다. 마치 10일 동안의 평균 기온을 구하는 것처럼 말인데요.

반면에 0.999의 값을 사용하는 것은 1000개의 값에서 평균치를 구하는 것과 같습니다. 그래서 이전에 본 것과 비슷하게 0.9와 0.999의 값에서 검색을 하고 싶은 경우에 선형 척도에서 검색을 하는 것이 말이 안되죠. 0.9와 0.999사이에서 말이죠.

이에 대해 생각하기 쉬운 방법으로는 $1 - beta$에서 그 값의 범위를 찾는 것인데요. 그러면 이제 범위는 0.1에서 0.001이 되겠죠. 그래서 베타 사이에서 샘플링을 할 텐데요. 0.1에서 0.01사이의 값 그리고 0.01과 0.001사이에서 말입니다. 알파와 같은 방법을 통해서 자원을 더 효율적으로 사용할 수 있습니다.

그렇다면 왜 선형 척도로 샘플링하는게 나쁜 생각일까요? 즉 베타가 1에 가까울 때 그 민감도 결과값에 대한 민감도가 변합니다. 아주 조금의 베타값 변화에도 말이죠. 그러므로 베타가 만약 0.9에서 0.9005로 바뀌면 이것은 그리 큰 의미를 갖진 않습니다. 결과값을 거의 바꾸지 않기 때문이죠. 하지만 만약 베타가 0.999에서 0.9995로 간다면 알고리즘이 어떻게 하고 있는지에 대해서 아주 큰 영향을 끼칩니다.

이 두 경우에 대략 10개 이상의 값을 평균화합니다. 그러나 여기서는 지난 1000개 정도의 예제에 대한 지수 가중 평균에서 이제 2000개 예제로 바뀌었습니다. 그리고 그것은 공식 $\dfrac{1}{1-\beta}$ 때문에 베타가 1에 가까울 때 베타의 작은 변화에 매우 민감합니다. 그러면 여기 전체 샘플링 절차가 하는 것은 베타가 1이 근접한 범위에서 샘플링을 밀도있게 진행하게 합니다. 또는 대안적으로 1-베타가 0에 가까울 때 그럴 수 있죠. 샘플을 배포하는 방법 측면에서 더 효율적일 수 있도록 가능한 결과값에 대해 더 효율적으로 탐색할 수 있게 말이죠.

---

