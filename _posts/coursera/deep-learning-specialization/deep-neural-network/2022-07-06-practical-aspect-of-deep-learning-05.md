---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (5)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (5)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Regularizing your Neural Network

### Understanding Dropout

드롭아웃이 신경망의 유닛을 무작위로 누락시키는 것이 말도 안되는 것처럼 보이나요? 왜 효과가 있을까요? 일반적인 사용자로서 좀 더 나은 직관을 살펴봅시다.

![image](https://user-images.githubusercontent.com/55765292/177467908-6bafddc8-fc11-43c1-a26e-b135ae53fa46.png)

이전에 드롭아웃은 무작위로 신경망의 유닛을 누락시킨다는 직관을 드렸습니다. 따라서 모든 반복에서 더 작은 신경망을 사용하는 것과 같습니다. 그래서 더 작은 신경망을 사용하는 것은 정규화 효과가 있는 것처럼 보입니다. 두 번째 직관은 아시다시피 단일 유닛의 관점에서 살펴보겠습니다.

4개의 입력이 있을 때 이 유닛은 의미 있는 결과값을 생성해야 합니다. 이제 드롭아웃으로 입력이 무작위로 제거될 수 있습니다. 간혹 2개의 유닛이 제거될 것입니다. 가끔은 다른 유닛이 제거될 수도 있습니다.

즉 이것이 의미하는 것은 보라색의 유닛입니다. 어떤 한 가지의 특성에 의존하면 안되는데요. 그것이 언제든지 임의로 제거될 수 있기 때문이죠. 특히 모든 것을 이 입력에 거는 것을 기피할 것입니다. 어떤 입력은 사라질 수 있기 때문에 그것에 많은 비중을 주는 방법을 기피하는 것이죠.

그러므로 이 유닛은 이런 방법으로 퍼지도록 유도되고 여기 4개의 입력에 비중을 나눠 주는 것이죠. 그리고 가중치를 분산시키면 가중치의 squared 노름을 줄이는 효과가 있고 그러므로 L2 정규화에서 본 것과 비슷하게 드롭아웃 구현의 효과는 그 강도가 L2 정규화와 유사하며 과적합을 방지하는데 도움이 됩니다.

드롭아웃은 공식적으로 L2 정규화의 조정된 형태로 나타낼 수 있습니다. 하지만 이 방법으로 곱해진 활성화의 크기에 따라 L2 패널티가 달라집니다.

요약하면  드롭아웃이 L2 정규화와 유사한 효과를 갖는 것을 알 수 있습니다. L2 정규화와 적용되는 부분에서의 차이만 있고 다른 입력값의 스케일에 따라 더 변형한다는 차이가 있습니다.

드롭아웃을 도입하는 것에 대한 내용을 한 가지 더 살펴보면 여기는 3개의 입력 특성이 있는 네트워크입니다. 첫 번째 레이어에는 7개의 은닉 유닛이 잇습니다. 그리고 7, 3, 2, 1개의 은닉 유닛을 가진 레이어가 있습니다. 그래서 선택해야 하는 것 중 하나는 keep.prob이며 레이어마다 유닛을 유지할 확률을 나타냅니다. 따라서 레이어별로 keep.prob을 다르게 하는 것도 가능합니다.

따라서 첫 번째 레이어의 경우 행렬 $w^{[1]}$은 $7 \times 3$입니다. 두 번째 가중치 행렬은 $7 \times 7$이 될 것입니다. $w^{[3]}$은 $3 \times 7$이 될 것입니다. 그러면 $w^{[2]}$가 사실 가중치가 가장 큰 행렬인데요. 왜냐하면 그것들은 실제로 가장 큰 매개변수 세트이기 때문입니다. $b$와 $w^{[2]}$는 $7 \times 7$입니다.

그래서 그 행렬의 과적합을 줄이기 위해서 아마도 이 레이어를 위해서 레이어 2의 keep.prob이 비교적 낮을 수 있습니다. 예를 들어 0.5이며 다른 레이어의 보다 큰 값에 대한 덜 걱정할 수 있는 큰 keep.prob이 있을 수 있겠죠. 0.7 정도.

keep.prob이 0이 될 수 있습니다. 그래서 명확함을 위해 보라색 상자에 그린 숫자를 보겠습니다. 이것은 다른 레이어에 대한 다른 keep.prob값입니다. keep.prob 1.0은 모든 유닛을 유지한다는 뜻입니다. 드롭아웃을 그 레이어에는 쓰지 않는 것으로 해석할 수 있습니다.

그러나 과적합에 대해 조금 더 걱정을 하는 매개변수가 많은 레이어에는 더 강한 형태의 드롭아웃을 적용하기 위해서 keep.prob이 더 작을 수 있습니다. 마지 L2 정규화의 매개변수 람다를 증가시키는 것과 같습니다.

일부 레이어를 다른 레이어보다 더 많이 정규화하는 것이죠. 엄밀히 말하면 입력층에도 드롭아웃을 적용시킬 수 있습니다. 여기서 입력 특징 중 하나 이상을 실행할 수 있습니다. 실제로 자주 사용하는 방법은 아닙니다. 그래서 입력값으로는 1.0의 keep.prob이 상당히 흔합니다. 0.9의 매우 높은 값을 사용할 수도 있지만 입력 특징의 절반을 제거할 가능성은 낮아서 대개 keep.prob은 이 모든 것을 적용할 때 1에 가깝고 입력층에 드롭아웃을 전부 적용하게 됩니다.

요약하자면 일부 레이어의 과적합이 걱정이라면 일부 레이어에 대해 낮은 keep.prob을 설정할 수 있습니다. 단점은 교차 검증을 사용하여 찾을 수 있는 하이퍼 파라미터가 훨씬 더 많다는 것입니다.

또 다른 방법으로는 드롭아웃을 적용한 레이어와 적용하지 않은 레이어가 있을 때 드롭아웃을 적용할 레이어의 keep.prob인 하이퍼 파라미터 하나만 있으면 됩니다.

대부분의 성공적인 첫 번째 드롭아웃은 컴퓨터 비전이었습니다. 그래서 컴퓨터 비전에서는 입력 크기가 너무 커서 모든 픽셀들을 입력하는 데에 거의 항상 데이터가 부족합니다. 그렇기 때문에 컴퓨터 비전에서는 드롭아웃이 매우 흔하게 활용되고 몇 가지 컴퓨터 비전 연구는 거의 항상 기본값으로 사용합니다.

기억하셔야 할 것은 드롭아웃은 정규화 테크닉이고 과적합을 예방하는 데 도움이 된다는 것입니다. 따라서 알고리즘이 과적합되지 않는 이상 굳이 드롭아웃을 사용하지 않을 것이며 다른 응용프로그램 분야에서는 드물게 사용할 수 있습니다.

주로 컴퓨터 비전인데 보통 데이터가 충분하지 않기 때문에 거의 항상 과적합하고 그래서 컴퓨터 비전 연구자들은 직관에 의해 드롭아웃을 신뢰합니다. 다른 분야에서는 항상 정규화되지 않는다고 봅니다.

드롭아우승 한 가지 큰 단점은 비용함수 J가 모든 반복에서 더이상 잘 정의되지 않는다는 것입니다. 무작위로 많은 노드를 취소하고 있는데요. 경사 하강법의 성능을 이중으로 확인한다면 실제로 다시 확인하기는 어렵습니다. 비용함수 J가 잘 정의되어 있습니다. 모든 반복에서 감소하고 있는데요. 최적화하는 비용함수 J가 사실은 잘 정의되지 않았거나 계산하기가 확실히 어렵기 때문입니다.

그래서 주로 드롭아웃을 끄거나 keep.prob을 1로 설정하고 코드를 실행하여 J가 빠르게 줄어드는지 확인하는 것입니다. 그리고 드롭아웃을 켜고 드롭아웃 중에 내 코드를 소개한 건 다른 방법이 필요하기 때문입니다. 드롭아웃으로 코드나 경사 하강법이 작동하는지 확인하기 위해 이런 그래프를 그리지는 않습니다.

따라서 알고 있다고 느끼는 몇 가지 정규화 테크닉이 아직 더 있습니다.
