---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (5)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (5)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Regularizing your Neural Network

### Understanding Dropout

드롭아웃이 신경망의 유닛을 무작위로 누락시키는 것이 말도 안되는 것처럼 보이나요? 왜 효과가 있을까요? 일반적인 사용자로서 좀 더 나은 직관을 살펴봅시다.

![image](https://user-images.githubusercontent.com/55765292/177467908-6bafddc8-fc11-43c1-a26e-b135ae53fa46.png)

이전에 드롭아웃은 무작위로 신경망의 유닛을 누락시킨다는 직관을 드렸습니다. 따라서 모든 반복에서 더 작은 신경망을 사용하는 것과 같습니다. 그래서 더 작은 신경망을 사용하는 것은 정규화 효과가 있는 것처럼 보입니다. 두 번째 직관은 아시다시피 단일 유닛의 관점에서 살펴보겠습니다.

4개의 입력이 있을 때 이 유닛은 의미 있는 결과값을 생성해야 합니다. 이제 드롭아웃으로 입력이 무작위로 제거될 수 있습니다. 간혹 2개의 유닛이 제거될 것입니다. 가끔은 다른 유닛이 제거될 수도 있습니다.

즉 이것이 의미하는 것은 보라색의 유닛입니다. 어떤 한 가지의 특성에 의존하면 안되는데요. 그것이 언제든지 임의로 제거될 수 있기 때문이죠. 특히 모든 것을 이 입력에 거는 것을 기피할 것입니다. 어떤 입력은 사라질 수 있기 때문에 그것에 많은 비중을 주는 방법을 기피하는 것이죠.

그러므로 이 유닛은 이런 방법으로 퍼지도록 유도되고 여기 4개의 입력에 비중을 나눠 주는 것이죠. 그리고 가중치를 분산시키면 가중치의 squared 노름을 줄이는 효과가 있고 그러므로 L2 정규화에서 본 것과 비슷하게 드롭아웃 구현의 효과는 그 강도가 L2 정규화와 유사하며 과적합을 방지하는데 도움이 됩니다.

드롭아웃은 공식적으로 L2 정규화의 조정된 형태로 나타낼 수 있습니다. 하지만 이 방법으로 곱해진 활성화의 크기에 따라 L2 패널티가 달라집니다.

요약하면  드롭아웃이 L2 정규화와 유사한 효과를 갖는 것을 알 수 있습니다. L2 정규화와 적용되는 부분에서의 차이만 있고 다른 입력값의 스케일에 따라 더 변형한다는 차이가 있습니다.

드롭아웃을 도입하는 것에 대한 내용을 한 가지 더 살펴보면 여기는 3개의 입력 특성이 있는 네트워크입니다. 첫 번째 레이어에는 7개의 은닉 유닛이 잇습니다. 그리고 7, 3, 2, 1개의 은닉 유닛을 가진 레이어가 있습니다. 그래서 선택해야 하는 것 중 하나는 keep.prob이며 레이어마다 유닛을 유지할 확률을 나타냅니다. 따라서 레이어별로 keep.prob을 다르게 하는 것도 가능합니다.

따라서 첫 번째 레이어의 경우 행렬 $w^{[1]}$은 $7 \times 3$입니다. 두 번째 가중치 행렬은 $7 \times 7$이 될 것입니다. $w^{[3]}$은 $3 \times 7$이 될 것입니다. 그러면 $w^{[2]}$가 사실 가중치가 가장 큰 행렬인데요. 왜냐하면 그것들은 실제로 가장 큰 매개변수 세트이기 때문입니다. $b$와 $w^{[2]}$는 $7 \times 7$입니다.

그래서 그 행렬의 과적합을 줄이기 위해서 아마도 이 레이어를 위해서 레이어 2의 keep.prob이 비교적 낮을 수 있습니다. 예를 들어 0.5이며 다른 레이어의 보다 큰 값에 대한 덜 걱정할 수 있는 큰 keep.prob이 있을 수 있겠죠. 0.7 정도.

keep.prob이 0이 될 수 있습니다. 그래서 명확함을 위해 보라색 상자에 그린 숫자를 보겠습니다. 이것은 다른 레이어에 대한 다른 keep.prob값입니다. keep.prob 1.0은 모든 유닛을 유지한다는 뜻입니다. 드롭아웃을 그 레이어에는 쓰지 않는 것으로 해석할 수 있습니다.

그러나 과적합에 대해 조금 더 걱정을 하는 매개변수가 많은 레이어에는 더 강한 형태의 드롭아웃을 적용하기 위해서 keep.prob이 더 작을 수 있습니다. 마지 L2 정규화의 매개변수 람다를 증가시키는 것과 같습니다.

일부 레이어를 다른 레이어보다 더 많이 정규화하는 것이죠. 엄밀히 말하면 입력층에도 드롭아웃을 적용시킬 수 있습니다. 여기서 입력 특징 중 하나 이상을 실행할 수 있습니다. 실제로 자주 사용하는 방법은 아닙니다. 그래서 입력값으로는 1.0의 keep.prob이 상당히 흔합니다. 0.9의 매우 높은 값을 사용할 수도 있지만 입력 특징의 절반을 제거할 가능성은 낮아서 대개 keep.prob은 이 모든 것을 적용할 때 1에 가깝고 입력층에 드롭아웃을 전부 적용하게 됩니다.

요약하자면 일부 레이어의 과적합이 걱정이라면 일부 레이어에 대해 낮은 keep.prob을 설정할 수 있습니다. 단점은 교차 검증을 사용하여 찾을 수 있는 하이퍼 파라미터가 훨씬 더 많다는 것입니다.

또 다른 방법으로는 드롭아웃을 적용한 레이어와 적용하지 않은 레이어가 있을 때 드롭아웃을 적용할 레이어의 keep.prob인 하이퍼 파라미터 하나만 있으면 됩니다.

대부분의 성공적인 첫 번째 드롭아웃은 컴퓨터 비전이었습니다. 그래서 컴퓨터 비전에서는 입력 크기가 너무 커서 모든 픽셀들을 입력하는 데에 거의 항상 데이터가 부족합니다. 그렇기 때문에 컴퓨터 비전에서는 드롭아웃이 매우 흔하게 활용되고 몇 가지 컴퓨터 비전 연구는 거의 항상 기본값으로 사용합니다.

기억하셔야 할 것은 드롭아웃은 정규화 테크닉이고 과적합을 예방하는 데 도움이 된다는 것입니다. 따라서 알고리즘이 과적합되지 않는 이상 굳이 드롭아웃을 사용하지 않을 것이며 다른 응용프로그램 분야에서는 드물게 사용할 수 있습니다.

주로 컴퓨터 비전인데 보통 데이터가 충분하지 않기 때문에 거의 항상 과적합하고 그래서 컴퓨터 비전 연구자들은 직관에 의해 드롭아웃을 신뢰합니다. 다른 분야에서는 항상 정규화되지 않는다고 봅니다.

드롭아우승 한 가지 큰 단점은 비용함수 J가 모든 반복에서 더이상 잘 정의되지 않는다는 것입니다. 무작위로 많은 노드를 취소하고 있는데요. 경사 하강법의 성능을 이중으로 확인한다면 실제로 다시 확인하기는 어렵습니다. 비용함수 J가 잘 정의되어 있습니다. 모든 반복에서 감소하고 있는데요. 최적화하는 비용함수 J가 사실은 잘 정의되지 않았거나 계산하기가 확실히 어렵기 때문입니다.

그래서 주로 드롭아웃을 끄거나 keep.prob을 1로 설정하고 코드를 실행하여 J가 빠르게 줄어드는지 확인하는 것입니다. 그리고 드롭아웃을 켜고 드롭아웃 중에 내 코드를 소개한 건 다른 방법이 필요하기 때문입니다. 드롭아웃으로 코드나 경사 하강법이 작동하는지 확인하기 위해 이런 그래프를 그리지는 않습니다.

따라서 알고 있다고 느끼는 몇 가지 정규화 테크닉이 아직 더 있습니다.


### Other Regularization

L2정규화와 dropout 정규화 외에도 신경망의 overfitting을 줄일 수 있는 몇 가지의 테크닉이 있습니다. 한번 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/177471921-9516ae49-c9f2-45c5-bc0e-9446012553b1.png)

고양이 분류를 피팅한다고 해봅시다. 만약 과적합의 경우에는 훈련 데이터가 도와줄 수 있는데요. 훈련 데이터는 비쌀 수 있습니다. 그리고 또 데이터를 단순히 더 수집할 수 없는 경우도 있습니다. 하지만 이런 이미지를 가지고 훈련 세트를 증가시킬 수 있는데요.

예를 들어 가로로 뒤집어서 훈련 세트에 추가할 수도 있고요. 그러면 여기 훈련 세트의 한가지 예뿐만 아니라 추가적으로 예시에 넣을 수 있습니다. 그러면 이렇게 가로로 이미지를 뒤집어서 트레이닝 세트의 크기를 2배로 만들 수 있는 것입니다. 이제 트레이닝 세트가 중복되기 중복되기 때문에 완전히 추가로 새로운 독립적인 이미지 샘플을 추가한 것보다는 좋지 못합니다.

하지만 더 추가적인 고양이 사진을 찾는데 시간 소비할 필요 없이 이렇게 빨리 진행할 수 있는 점이 있습니다. 가로로 뒤집는 방법 말고 또 이미지를 크롭할 수 있습니다. 여기서는 회전하고 임의로 이미지를 줌인 처리 했는데요. 이 이미지는 아직까지 고양이처럼 보입니다. 이렇게 이미지를 찌그러트리거나 변형을 주어 데이터 세트를 확장하는 방법이 있습니다.

바로 이와 같이 가짜의 추가 트레이닝 샘플을 만드는 것이죠. 이러한 가짜의 샘플은 정보적으로는 완전히 새로운 고양이 이미지를 추가하는 것보다는 도움이 되지 않습니다. 하지만 이렇게 하는 것 또한 방법이기 때문에 비용을 들이지 않고 진행할 수 있습니다. 비싸지 않은 방법으로써 알고리즘에 데이터를 더 제공하고 일반화를 통해 과적합을 결과적으로 줄일 수 있는 하나의 방법입니다.

시각적 캐릭터 인식 기능에서는 여러분은 특정 숫자를 갖고 임의의 회전과 찌그러트림으로 데이터세트를 불러올 수있습니다. 만약에 이런 것을 트레이닝 세트에 추가하면 이 숫자들은 아직 모두 4입니다. 예제에서 아주 강한 찌그러트림을 적용했는데요. 이것은 웨이브가 많이 적용됐고 실제로 이렇게 많이 찌그러트릴 필요는 없습니다만 여기서보다 조금 덜 적용시키면 되겠습니다. 그리하여 data augmentation을 이용해서 일반화 테크닉에 일부로 적용시키면 됩니다.

![image](https://user-images.githubusercontent.com/55765292/177471937-ad46485d-9871-4af2-8a67-5f63471b203d.png)

일반화와 비슷하게 쓰이는 기술 중 early stopping이라고 하는 기술이 있습니다. 여기서는 무엇을 할 것이냐면, 기울기 강하를 실행하면서 트레이닝 오류나 트레이닝 세트에서의 분류 오류 또는 최적화 J 비용함수를 그립니다. 그 그래프는 J 비용 함수를 둘러싸 트레이닝 시키므로 이상적으로 감소할 것입니다.

early stopping에서는 이 그래프를 그리고 또한 개발 세트의 오류를 그립니다. 개발 세트에서의 분류 오류일 수도 있고 logistic loss 처럼 비용 함수 일 수도 있습니다. 개발 세트 오류는 어느 정도 감소하다가 특정 지점 이후로 다시 증가하는 것을 볼 수 있습니다.

ealry stopping이 하는 것은 신경망이 그 지점까지 테스트를 잘 수행했으니 트레이닝을 그만 진행하는 것입니다.

만약 신경망에서 반복 테스트를 몇 번 수행 안 한 경우 w 매개 변수는 거의 에 가까울 것입니다. 임의의 초기화에서 아마 w를 작은 값으로 초기화하기 때문에 w를 긴 시간 트레이닝 하기 전에는 작은 값일 것입니다. 그리고 반복 수행을 하면서 w의 값은 계속해서 커집니다. 그러면 신경망에서의 w 파라미터의 값은 더 큰 값으로 되어 있을 것입니다.

early stopping은 w의 비율이 중간 정도 되는 시점에서 중지시키는 역할을 합니다. 그러므로 L2 일반화와 미슷하게 신경망에서 w 파라미터 값이 비슷한 노름을 선정하여 결과적으로 신경망이 덜 overfitting하게 만들어주는 것입니다. 그리고 early stopping 이라는 용어는 말 그대로 신경망의 트레이닝을 일찍 중간에 스탑 시키는 것을 말합니다. 

한가지 단점이 있기는 한데요. 머신러닝이 몇 가지 다른 단계를 갖고 있다고 생각합니다. 첫 번째로 J 비용함수를 최적화 시킬 수 있는 알고리즘을 원할텐데요. 기울기 강하와 같은 여러 가지 도구를 이용해서 그렇게 할 수 있죠. 그러나 J 비용함수를 최적화 시킨 다음으로는 overfit 또한 원치 않았습니다.

이렇게 하기 위한 일반화와 같은 도구도 있었죠. 또는 단순히 데이터를 더 많이 수집하는 방법도 있습니다. 이미 머신러닝에는 너무 많은 하이퍼 파라미터가 있습니다. 가능한 알고리즘들 사이에서 고르는 것이 굉장히 복잡합니다. J 비용함수를 최적화 시킬 수 있는 도구가 있거나 J 비용함수를 최적화하는 데 집중을 하는 경우 더 쉽게 이해할 수 있는데요. 유일히 신경쓰는 것은 w와 b를 찾고 J(w,b)가 최대한 작게 되도록 만드는 것입니다. 이외에는 신경 쓰지 않습니다.

그리고 overfit을 하지 않기 위한 방법은 완전히 별개의 업무입니다. 다시말해 편차를 줄이기 위한 방법 말이죠. 이것을 진행하는 경우에 쓰이는 도구가 따로 있습니다. 이 원리는 가끔 직교화(orthogonalization)라고 불리는데요. 이것은 아이디어를 하나씩 생각한다는 건데요. 나중에 따로 다루겠습니다.

early stopping의 주된 단점은 이것이 이 2개의 업무를 묶는다는 것입니다. 그러면 이 2개의 별개 문제를 단독으로 풀 수가 없게 됩니다. 기울기 강하를 일찍이 정지시키기 때문에 비용함수를 최적화 시키는 진행 단계에서 중간에 자르는 것과 마찬가지 인데요. 그 이유는 비용함수를 잘 못 줄이고 있기 때문에 그런 것이죠. 어떻게 보면 잘 못했기 때문이죠. 그와 동시에 overfit이 되지 않도록 하는 것입니다.

그렇기 때문에 이런 2문제를 다른 도구를 사용해서 푸는 것이 아니라 한가지를 이용해서 2가지가 약간 섞이는 것입니다. 그러면 결과적으로 여러 가지 시도를 할 수 있는 방법들에 대해서 더 복잡하게 만드는 경향이 있습니다.

early stopping을 사용하는 대신 그냥 L2 일반화를 사용하는 방법이 있는데요. 이 경우 신경망을 최대한 길게 트레이닝 시키면 되겠습니다. 저는 이 경우가 더 하이퍼 파리미터의 서치 넓이를 분해시키는데 쉽다고 생각합니다. 그리고 다시 서치하는데도 말이죠.

하지만 이것의 단점은 여러 가지 일반화 파라미터 Lambda 값들을 시도해봐야 한다는 점입니다. 그렇기 때문에 여러가지 Lambda 값을 서칭하는 것은 더 비싼 방법이겠습니다. 그리고 ealry stopping의 장점은 기울기 강하를 한번만 실행하고 작은 w값, 중간 w값, 큰 w값을 한번에 시도할 수 있습니다. L2 하이퍼 파라미터 일반화의 여러가지 Lambda 값들을 시도해 볼 필요도 없이 말이죠.

단점이 있음에도 불구하고 사람들은 많이 사용합니다. 저는 개인적으로 L2 일반화를 이용해서 여러가지 Lambda의 값을 시도하는 것을 선호하는데요. 이것은 computation 비용을 감안했을 때 그렇겠죠. 하지만 early stopping 또한 비슷한 효과를 얻게 해주는데요. Lambda의 여러가지 값을 시도할 필요없이 말이죠.

데이터 확장을 어떻게 활용하는지 봤습니다. 또한 편차 또는 신경망의 overfitting을 막기 위한 early stopping에 대해서도 살펴보았습니다. 다음은 트레이닝을 빠르게 하도록 최적화 문제를 세팅하는 기술들에 대해 다뤄보겠습니다.
