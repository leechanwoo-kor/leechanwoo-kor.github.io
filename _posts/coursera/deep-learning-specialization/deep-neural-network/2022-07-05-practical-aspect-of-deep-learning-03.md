---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Regularizing your Neural Network

### Regularization

신경망이 데이터에 과적합된 것으로 의심되는 경우, 즉 고분산 문제가 있는 경우 가장 먼저 시도해야 하는 것 중 하나는 아마도 정규화일 것입니다. 네트워크의 분산을 줄이기 위해 더 많은 훈련 데이터를 얻기 위한 것이기도 하며 이는 매우 신뢰할 수 있습니다.

그러나 항상 더 많은 훈련 데이터를 얻을 수는 없고 또는 더 많은 데이터를 얻는데 비용이 많이 들 수 있습니다. 그러나 정규화를 추가하면 종종 과적합을 방지하는데 도움이 됩니다. 고분산을 해결하는 다른 방법입니다.

자, 그럼 정규화가 어떻게 이루어지는 한번 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/177271391-badf0493-85fa-4d9f-85ed-643e3ac26b44.png)

로지스틱 회귀를 사용하여 이러한 아이디어를 발전시켜 봅시다. 로지스틱 회귀의 경우 비용 함수 J를 최소화하려고 한다는 점을 기억하셔야하며, 그것은 이 비용 함수로 정의되어 있습니다. 로지스틱 회귀에서 w와 b가 매개변수라는 것을 기억하는 다른 예제에서 개별 예측의 손실에 대한 훈련 예제 중 일부입니다.

따라서 w는 x차원의 파라미터 벡터이고 b는 실수입니다. 그래서 로지스틱회귀에 정규화를 추가하려면 정규화 매개변수라고 하는 람다를 추가해야 합니다. 그러나 람다는 w제곱의 노름 / 2m 을 곱합니다.

여기서 $\Vert w \Vert ^2_2$은 $j=1$에서 $w_j^2$의 $n_x$까지의 합과 같거나 또는 $w^Tw$로 쓸 수도 있습니다. w를 전치하면 벡터 w에 대한 소수의 제곱 유클리드 노름입니다. 그리고 이것은 L2 정규화라고 불립니다.

여기서 유클리드 노름을 사용하고 있기 때문에 그것은 또한 매개변수 벡터 w가 있는 L2 노름이라고도 합니다.

매개변수 w만 정규화하는 이유는 무엇일까요? 실제로 b를 추가할 수있지만 그냥 생략하겠습니다. 매개변수를 보면 w는 일반적으로 매우 높은 차원의 매개변수 벡터이며 특히 고분산 문제가 있습니다. 어쩌면 w가 그저 변수가 많아서 그럴수도 있지만요. 그래서 그렇다고 모든 파라미터들을 그냥 넣진 않지만, 반면에 b는 그저 단수일 뿐입니다. 그래서 거의 모든 매개변수들이 b보다는 w에 있습니다.

그리고 만약에 마지막 용어를 추가한다면 실제로는 큰 차이가 없겠지만 b는 매우 많은 수의 매개변수에 대한 하나의 매개변수이기 때문입니다. 실제로 주로 그것을 포함시키는 데에는 신경을 쓰지 않지만 포함시키고 싶다면 그렇게 하셔도 됩니다.

따라서 L2 정규화는 가장 일반적인 유형의 정규화입니다. 다른 사람들이 L1 정규화에 대해 얘기하는 것을 들어본 적이 있을거에요. 그리고 그것은 이 L2 노름 대신에 $\cfrac{\lambda}{2m} \Vert w \Vert_1$ 을 더하였을 때 입니다. 그리고 이것은 파라미터 벡터 w의 L1 노름이라고도 하고 아래에 있는 작은 첨자는 1로 씁니다. 그리고 분모에 m을 넣든 2m을 넣든 스케일링 상수일 뿐입니다.

여러분이 L1 정규화를 사용한다면 w는 결국 희박해집니다. 즉, w벡터는 많은 0을 가지고 있습니다. 그리고 일부 사람들은 이것이 모델을 압축하는 데 도움이 될 수 있다고 말합니다. 왜냐하면 매개변수 집합이 0이면 모델을 저장하는 데 더 적은 메모리가 필요합니다.

하지만 실제로는 L1 정규화가 모델을 희소하게 만들기 위해 아주 조금밖에 도움이 되지 않기 때문입니다. 그래서 적어도 여러분의 모델을 압축할 목적이 아니라서 그렇게 많이 사용되지는 않는다고 생각합니다. 사람들이 네트워크를 훈련할 때, L2 정규화가 훨씬 더 자주 사용된다는 것을 알았습니다.

마지막으로 한가지 세부사항입니다. 여기 이 람다는 정규화 파라미터라고 불립니다. 그리고 일반적으로 개발 세트를 사용하여 이것을 설정하거나 홀드아웃 교차 검증을 사용합니다. 다양한 값을 시도하고 무엇이 가장 좋은지 알 때, 훈련 세트에서 좋은 성과를 내는 것 또한 매개변수의 두 법선을 작게 설정하면 과적합을 방지하는 데 도움이 됩니다.

따라서 람다는 조정해야 할 수 있는 또 다른 하이퍼 매개변수입니다. 이것이 로지스틱 회귀를 위한 L2 정규화를 구현하는 방법입니다.

![image](https://user-images.githubusercontent.com/55765292/177271458-2dedd4b8-7ccf-4519-89a2-b10080d68b0d.png)
