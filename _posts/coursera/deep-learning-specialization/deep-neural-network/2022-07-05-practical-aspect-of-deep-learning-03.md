---
title: "[Ⅱ. Deep Neural Network] Practical Aspects of Deep Learning (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Tensorflow
  - Deep Learning
  - Mathematical Optimization
  - hyperparameter tuning
toc: true
toc_sticky: true
toc_label: "Practical Aspects of Deep Learning (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/177095282-038ee3ed-f543-4793-9eff-f2d5ac239f36.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Practical Aspects of Deep Learning

## Regularizing your Neural Network

### Regularization

신경망이 데이터에 과적합된 것으로 의심되는 경우, 즉 고분산 문제가 있는 경우 가장 먼저 시도해야 하는 것 중 하나는 아마도 정규화일 것입니다. 네트워크의 분산을 줄이기 위해 더 많은 훈련 데이터를 얻기 위한 것이기도 하며 이는 매우 신뢰할 수 있습니다.

그러나 항상 더 많은 훈련 데이터를 얻을 수는 없고 또는 더 많은 데이터를 얻는데 비용이 많이 들 수 있습니다. 그러나 정규화를 추가하면 종종 과적합을 방지하는데 도움이 됩니다. 고분산을 해결하는 다른 방법입니다.

자, 그럼 정규화가 어떻게 이루어지는 한번 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/177271391-badf0493-85fa-4d9f-85ed-643e3ac26b44.png)

로지스틱 회귀를 사용하여 이러한 아이디어를 발전시켜 봅시다. 로지스틱 회귀의 경우 비용 함수 J를 최소화하려고 한다는 점을 기억하셔야하며, 그것은 이 비용 함수로 정의되어 있습니다. 로지스틱 회귀에서 w와 b가 매개변수라는 것을 기억하는 다른 예제에서 개별 예측의 손실에 대한 훈련 예제 중 일부입니다.

따라서 w는 x차원의 파라미터 벡터이고 b는 실수입니다. 그래서 로지스틱회귀에 정규화를 추가하려면 정규화 매개변수라고 하는 람다를 추가해야 합니다. 그러나 람다는 w제곱의 노름 / 2m 을 곱합니다.

여기서 $\Vert w \Vert ^2_2$은 $j=1$에서 $w_j^2$의 $n_x$까지의 합과 같거나 또는 $w^Tw$로 쓸 수도 있습니다. w를 전치하면 벡터 w에 대한 소수의 제곱 유클리드 노름입니다. 그리고 이것은 L2 정규화라고 불립니다.

여기서 유클리드 노름을 사용하고 있기 때문에 그것은 또한 매개변수 벡터 w가 있는 L2 노름이라고도 합니다.

매개변수 w만 정규화하는 이유는 무엇일까요? 실제로 b를 추가할 수있지만 그냥 생략하겠습니다. 매개변수를 보면 w는 일반적으로 매우 높은 차원의 매개변수 벡터이며 특히 고분산 문제가 있습니다. 어쩌면 w가 그저 변수가 많아서 그럴수도 있지만요. 그래서 그렇다고 모든 파라미터들을 그냥 넣진 않지만, 반면에 b는 그저 단수일 뿐입니다. 그래서 거의 모든 매개변수들이 b보다는 w에 있습니다.

그리고 만약에 마지막 용어를 추가한다면 실제로는 큰 차이가 없겠지만 b는 매우 많은 수의 매개변수에 대한 하나의 매개변수이기 때문입니다. 실제로 주로 그것을 포함시키는 데에는 신경을 쓰지 않지만 포함시키고 싶다면 그렇게 하셔도 됩니다.

따라서 L2 정규화는 가장 일반적인 유형의 정규화입니다. 다른 사람들이 L1 정규화에 대해 얘기하는 것을 들어본 적이 있을거에요. 그리고 그것은 이 L2 노름 대신에 $\cfrac{\lambda}{2m} \Vert w \Vert_1$ 을 더하였을 때 입니다. 그리고 이것은 파라미터 벡터 w의 L1 노름이라고도 하고 아래에 있는 작은 첨자는 1로 씁니다. 그리고 분모에 m을 넣든 2m을 넣든 스케일링 상수일 뿐입니다.

여러분이 L1 정규화를 사용한다면 w는 결국 희박해집니다. 즉, w벡터는 많은 0을 가지고 있습니다. 그리고 일부 사람들은 이것이 모델을 압축하는 데 도움이 될 수 있다고 말합니다. 왜냐하면 매개변수 집합이 0이면 모델을 저장하는 데 더 적은 메모리가 필요합니다.

하지만 실제로는 L1 정규화가 모델을 희소하게 만들기 위해 아주 조금밖에 도움이 되지 않기 때문입니다. 그래서 적어도 여러분의 모델을 압축할 목적이 아니라서 그렇게 많이 사용되지는 않는다고 생각합니다. 사람들이 네트워크를 훈련할 때, L2 정규화가 훨씬 더 자주 사용된다는 것을 알았습니다.

마지막으로 한가지 세부사항입니다. 여기 이 람다는 정규화 파라미터라고 불립니다. 그리고 일반적으로 개발 세트를 사용하여 이것을 설정하거나 홀드아웃 교차 검증을 사용합니다. 다양한 값을 시도하고 무엇이 가장 좋은지 알 때, 훈련 세트에서 좋은 성과를 내는 것 또한 매개변수의 두 법선을 작게 설정하면 과적합을 방지하는 데 도움이 됩니다.

따라서 람다는 조정해야 할 수 있는 또 다른 하이퍼 매개변수입니다. 이것이 로지스틱 회귀를 위한 L2 정규화를 구현하는 방법입니다.

![image](https://user-images.githubusercontent.com/55765292/177271458-2dedd4b8-7ccf-4519-89a2-b10080d68b0d.png)

신경망은 어떨까요? 신경망에는 L이 신경망의 모든 매개변수인 레이어 수의 함수인 비용 함수가 있습니다. 따라서 비용 함수는 손실의 합, m개의 훈련 예제에 대한 합계입니다. 그래서 정규화를 추가하려면 모든 매개변수 w에 대한 합계에 대한 람다값을 추가합니다.

매개변수 매트릭스는 w인데요. 그들의 노름 제곱이라고 합니다. 여기서 행렬의 이 노름, 실제로 노름 제곱은 i의 합, j의 합이며 해당 매트릭스의 각 요소의 제곱입니다. 그리고 만약 여러분이 이 합계의 지표들을 원한다면 이것은 $i=1$에서 $n^{[l-1]}$까지의 합계, $j=1$에서 $n^{[l]}$의 합계이고, w는 $n^{[l]} \times n^{[l-1]}$ 차원 행렬이기 때문에, 여기서 이들은 레이어 l의 레이어 [l-1]에 있는 은닉 유닛의 수 또는 유닛의 수입니다.

따라서 이 매트릭스 노름을 Frobenius라고 합니다. 아래 첨자에 F로 표시되는 매트릭스의 노름입니다. 이를 프로베니우스 노름이라고하고 단지 매트릭스 요소의 제곱의 합을 의미합니다.

그럼 이걸로 경사 하강을 어떻게 구현할까요? 이전에는 역전파를 사용하여 dw를 완성했는데요. 여기서 역전파는 w에 대한 J의 편도 함수를 제공하거나 주어진 [l]에 대해 실제로 w를 제공합니다. 그런 다음 $w^{[l]}$에서 학습률을 뺀 값에 d를 곱한 값으로 $w^{[l]}$을 업데이트합니다. 이것은 우리가 이 추가 정규화 항을 목적에 추가하기 전입니다.

이제 이 일반화 영어를 목표에 추가했으니 dw를 가져와서 람다값을 더하면 됩니다. 그후에 그저 업데이트 된 것을 이전같이 계산하면 됩니다. 그리고 파라미터와 관련하여 비용 함수의 $dw^{[l]}$에 대한 이 새로운 $dw^{[l]}$은 여전히 미분의 올바른 정의이며, 이 새로운 정의를 사용하면 끝에 추가 정규화 항을 추가한 것으로 나타났습니다.

그리고 이러한 이유로 L2 정규화를 때때로 가중 감소라고도 합니다. 이는 초록색 식으로 나타나있습니다. 가중치 감소라는 이름을 사용하지 않을 것이지만, 왜 그런지에 대한 직관, 가중치 감소라고 하는 것은 여기에서 첫 번째 항이 같다는 것입니다.

따라서 가중치 매트릭스에 1보다 약간 작은 숫자를 곱하면 됩니다. 이것이 신경망에서 L2 정규화를 구현하는 방법입니다. 다음으로 정규화가 과적합을 방지하는 이유에 대해서 살펴보겠습니다.


### Why Regularization Reduces Overfitting?

정규화가 어떻게 과적합 문제를 도와줄 수 있는 것일까요? 왜 그것이 분산 문제를 줄이는데 도움이 될까요? 그것이 어떻게 작동하는지에 대한 약간의 직관을 얻기 위해 몇가지 예를 들어봅시다.

![image](https://user-images.githubusercontent.com/55765292/177287399-491f1760-35ee-4a34-8568-91bb21052934.png)

따라서 우리의 높은 편향과 높은 분산 그리고 적합한에 대한 그림이 있습니다. 이제 적합한 큰 심층신경망을 살펴보겠습니다. 이걸 너무 크거나 깊이 있게 그리진 않았지만 일부 신경망이 현재 괒거합되는지 봅시다.

비용함수에 정규화를 위해 한 것은 람다를 포함한 보라색 항을 추가한 것입니다. L2노름을 축소하는 이유는 무엇인가요? 또는 파라미터를 줄이는 것이 덜 과적합하게 만드는 것일까요?

직관적으로 만약 정규화 람다를 정말 크게 만들면 가중 매트릭스들인 w를 0에 가깝게 설정하도록 유도할 수 있다는 것입니다. 따라서 가중치를 0에 가깝게 설정할 수 있따는 것입니다. 기본적으로 이러한 은닉 유닛의 많은 영향을 없애면 단순한 신경망은 더 작은 신경망이 됩니다. 실제로는 거의 로지스틱 회귀 단위와 비슷합니다.

하지만 여러 층을 깊이 쌓아야 합니다. 이러한 과적합한 경우에서 큰 편향을 갖는 왼쪽의 경우처럼 이동시킵니다. 하지만 이상적으로 람다의 적당한 중간 값을 통해 결과는 중간에 있는 딱 맞는 경우에 더 가깝습니다.

직관적인 부분은 바로 람다의 값을 매우 크게 올리면 정말 w를 0에 가깝게 설정하는데요. 물론 실제로는 이렇게 되지는 않습니다. 많은 은닉 유닛의 영향을 제로화하거나 최소한으로 줄이는 것으로 생각할 수 있습니다. 따라서 더 단순한 네트워크처럼 느껴질 수 있으며 이는 마치 로지스틱 회귀를 사용하는 것처럼 점점 더 가까워집니다.

은닉 유닛의 무리를 완전히 제로화하는 직관은 옳지 안습니다. 실제로 일어나는 현상은 지속적으로 은닉 유닛을 사용할 것인데요. 각각의 효과가 훨씬 작아진다는 것입니다. 하지만 결국 더 단순한 네트워크로 이어지게 되고 그리고 따라서 과적합 가능성이 낮은 소규모 네트워크를 사용하는 경우입니다.

그래서 이 직관이 도움이 되는지 모르겠지만 프로그램 연습에서 정규화를 구현하면 실제로 이러한 분산 감소 결과 중 일부를 직접 볼 수 있습니다.

![image](https://user-images.githubusercontent.com/55765292/177287524-50287e42-6103-427b-8650-b0b101d66372.png)

정규화가 어떻게 과적합 문제에 도움을 주는지 한가지 예제를 더 살펴보겠습니다. 이를 위해 다음을 사용한다고 가정하겠습니다. tanh 활성화 함수는 다음과 같습니다. 이것은 $g(z) = tanh(z)$라는 식입니다. z가 파라미터의 작은 범위만 취하면 아마도 주변에서 선형 체제를 사용하고 있을 것입니다. z가 더 큰 값이나 더 작은 값으로 방황하도록 허용되는 경우에만 가능하고 활성화 함수가 덜 선형적인 함수로 정의되는 것입니다.

여기서 여러분들이 얻을 수 있는 직관은 만약 람다가, 즉 정규화 파라미터가 크면, 파라미터들이 작아질 것입니다. 왜냐하면 비용 함수에서 더 큰 값을 가진다는 이유이죠. 그래서 만약 가중치 w가 작다면 z는 w와 같기 때문에 그런데요. 기술적으로 +b 입니다.

그런데 만약 w 값이 작은 경우 z의 값 또한 꽤 작을 것입니다. 특히 z가 이 작은 범위에서 비교적 작은 값을 취하면 g(z)는 대략적으로 선형일 것입니다. 이것은 마치 모든 층이 대략적으로 선형인 것과 비슷합니다. 마치 선형 회귀와 같이 말이죠.

이전에 살펴봤듯이 만약 모든 층이 선형이라고 하면 전체 네트워크가 선형 네트워크입니다. 그러므로 심층신경망인 경우에도 선형 활성화 함수로 이루어져 있는 심층망의 경우 결국에는 선형 함수만 계산할 수 있습니다.

따라서 과적합 고분산 사례에서 보았듯이 데이터 세트에 정말 과적합할 수 있도록 하는 매우 복잡한 결정, 매우 비선형적인 결정 경계를 맞출 수 없습니다.

그래서 요약해보자면 정규화 파라미터가 매우 큰 경우, w 파라미터가 굉장히 작아지고 그리하여 z도 상대적으로 작아질 것인데요. b의 효과는 일단 무시하도록 하겠습니다. 그러나 z는 상대적이므로 z는 상대적으로 작아요. 또한 사실은 작은 범위의 값을 갖는다고 하는게 더 맞습니다. 

그래서 활성화 함수가 tanh라면 상대적으로 선형이 될텐데요. 따라서 전체 신경망은 멀지 않은 곳에서 이러한 효과 중 큰 선형 함수, 꽤 매우 복잡한 고도의 비선형 함수라기 보다는 단순한 함수입니다. 그래서 과적합도 훨씬 덜 합니다.



















