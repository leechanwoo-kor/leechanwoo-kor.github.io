---
title: "[Ⅰ. Neural Networks and Deep Learning] Shallow Neural Networks (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Shallow Neural Networks (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Shallow Neural Networks

### Computing a Neural Network's Output
지난 번에 단일 은닉층 신경망이 어떻게 생겼는지 보았습니다. 이번에는 신경망이 이러한 출력을 정확하게 계산하는 방법에 대해 자세히 살펴보겠습니다. 여러분이 보는 것은 로지스틱 회귀와 비슷하지만 여러 번 반복된다는 것입니다. 한번 보도록 하죠.

![image](https://user-images.githubusercontent.com/55765292/175211134-6f07c96f-c611-46c2-a9fe-4919b83598fd.png)

이전에 본 신경망의 모습이 2레이어 신경망의 모습입니다. 이 신경망이 정확히 무엇을 계산하는지 좀 더 자세히 살펴봅시다. 이제, 로지스틱 회귀 이전에 로지스틱 회귀 분석의 원은 실제로 계산 행의 두 단계를 나타냅니다. $z$는 $z = w^Tx + b$와 같이 계산하고 $a$는 두 번째로 $a = \sigma{(z)}$로 $z$의 시그모이드 함수로 활성화를 계산합니다.

따라서 신경망은 이 작업을 훨씬 더 많이 수행합니다. 은닉층의 노드 중 하나에 집중하는 것으로 시작합니다. 은닉층의 첫 번째 노드를 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175210773-9f7bb52c-4d17-405f-8be9-912a59f7d56c.png)

다른 노드는 일단 회색으로 표시해 두었습니다. 왼쪽의 로지스틱 회귀와 마찬가지로 은닉층의 이 노드는 두 단계의 계산을 수행합니다. 첫 번째 단계는 이 노드의 왼쪽 절반으로 생각하면 $z$가 $w^Tx + b$와 같다고 계산하고 여기서 사용할 표기법은 모두 첫 번째 은닉층과 관련된 양입니다. 그래서 대괄호가 많이 있는 거죠.

이것은 은닉층 내의 첫 번째 노드입니다. 그래서 거기에 아래 첨자를 가지고 있습니다. 그래서 다음 두 번째 단계는 $a_1^{[1]} = \sigma{(z_1^{[1]})}$와 같은 것을 계산하는 것입니다. 따라서 $z$와 $a$의 경우 위의 대괄호 안의 $l$인 $a^{[l]}$는, $l$은 레이어 번호를 나타내고 여기서 $i$ 첨자는 노드를 해당 레이어의 노드를 나타냅니다.

그래서 우리가 보고 있는 노드는 레이어 1이고 그것은 은닉 레이어 노드 1입니다. 그래서 윗 첨자와 아래 첨자가 1, 1이었습니다. 그래서, 그 작은 원은, 신경망에 있는 첫 번째 노드이며, 이 두 단계의 계산을 수행하는 것을 나타냅니다.

이제 신경망의 두 번째 노드 또는 신경망의 은닉층에 있는 두 번째 노드를 살펴봅시다. 왼쪽의 로지스틱 회귀 단위와 마찬가지로 이 작은 원은 계산의 두 단계를 나타냅니다. 첫 번째 단계는 $z$를 계산하는 것입니다. 이것은 여전히 레이어 1이지만 이제 두 번째 노드는 $z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}$와 같고 다음으로 $a_2^{[1]} = \sigma{(z_2^{[1]})}$와 같습니다.

하지만 윗첨자와 아래 첨자 표기법은 위에서 보라색으로 쓴 것과 일치하는지 다시 확인할 수 있습니다. 그래서, 신경망의 처음 두 개의 은닉 유닛를 통해 이야기했으며 유닛3과 4는 또한 일부 계산을 나타냅니다.

자, 이제 이 두 쌍의 방정식을 가져와서 다음 그림에서 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175210912-f95c0f5c-82f2-4576-9e52-231fb8a44f62.png)

여기 우리의 신경망이 있고 여기 첫 번째와 두 번째 은닉 유닛에 대해 이전에 알아냈던 두 방정식이 있습니다. 그런 다음 세 번째와 네 번째 은닉 유닛에 해당하는 방정식을 작성하면 위와 같은 결과를 얻을 수 있습니다.

그래서 이 표기법은 명확하다는 것을 보여드리겠습니다. 먼저 $w_1^{[1]}$, 이것은 벡터 전치 곱하기 $x$입니다. 이것이 바로 윗첨자 T가 의미하는 겁니다. 이것은 벡터 전치입니다.

이제 짐작하시겠지만 실제로 신경망을 구현하는 경우 for 루프를 사용하여 이 작업을 수행하는 것은 정말 비효율적입니다. 그래서 우리가 하려는 것은, 이 4가지 방정식을 가지고 벡터화하는 것입니다. $z$를 벡터로 계산하는 방법을 보여주는 것으로 시작하겠습니다.

여러분이 다음과 같이 할 수 있다는 것이 밝혀졌습니다. 이 $w$을 가져와서 행렬에 쌓으면 $w_1^{[1]}$ 전치가 있으므로 행 벡터이거나 이 열 벡터 전치는 행 벡터를 제공하는 다음 $w_1^{[2]}$, $w_1^{[3]}$, $w_1^{[4]}$ 따라서 이 4개의 $w$ 벡터를 겹쳐 쌓으면 행렬이 됩니다.

또 다른 방법은 4개의 로지스틱 회귀 단위가 있고 각각의 로지스틱 회귀 단위에는 해당 매개변수 벡터 $w$가 있다는 것입니다. 
이 4개의 벡터를 함께 쌓으면, $4 \times 3$ 행렬이 됩니다. 
따라서 이 행렬을 사용하여 입력 특성 $x_1,x_2,x_3$을 곱하면 행렬 곱셉이 어떻게 작동하는지 알 수 있습니다.

결과적으로 $w_1^{[1]T}, w_2^{[1]T}, w_3^{[1]T}, w_4^{[1]T}$가 됩니다. 그런 다음 $b$는 계산하지 맙시다. 이제 여기에 벡터 $b_1^{[1]},b_2^{[1]},b_3^{[1]},b_4^{[1]}$를 더합니다. 그래서 이결과의 4개의 행 각각이 위에서 가지고 있던 식과 정확히 일치한다는 것을 알 수 있습니다.

즉, 정의한대로 $z_1^{[1]},z_2^{[1]},z_3^{[1]},z_4^{[1]}$와 같다는 것을 보여주었습니다. 놀랍지 않게도, 이 모든 것을 벡터 $z^{[1]}$라고 부를 것입니다. 그것은 $z$의 각각을 열 벡터로 쌓음으로써 얻어집니다.

벡터화를 할 때 이를 탐색하는 데 도움이 될 수 있는 경험 법칙 중 하나는 레이어내의 노드가 다른 경우 그것들을 수직으로 쌓는다는 것입니다. 그래서 $z_1^{[1]}$에서 $z_4^{[1]}$를 가지고 있는 이유이며 은닉층의 4개의 서로 다른 노드에 대응하여 이 4개의 숫자를 수직으로 쌓아 올려 벡터 $z^{[1]}$를 형성했습니다.

표기법을 하나 더 사용하기 위해 소문자 $w_1^{[1]},w_2^{[1]}$ 등을 쌓아서 얻은 이 $4 \times 3$ 행렬을 더 사용하기 위해 이 행렬을 $W^{[1]}$이라고 부를 것입니다. 마찬가지로, $b$를 담은 벡터는 $b^{[1]}$라고 부를 것입니다. 이것은 $4 \times 1$의 벡터입니다. 이 벡터 행렬 표기법을 사용하여 $z$를 계산했습니다.

마지막으로 해야할 일은 $a$의 이러한 값도 계산해야 합니다. 따라서 먼저 $a^{[1]}$을 단순히 쌓는 것으로 정의해도 이상하지 않습니다. 이러한 활성화 값 $a^{[1]}$은 $a_1^{[1]}$에서 $a_4^{[1]}$입니다. 그래서 이 4개의 값을 가지고 $a^{[1]}$라는 벡터와 함께 쌓으세요.

이것은 $z^{[1]}$의 시그모이드가 될 것이며, $z$의 4가지 요소를 받아들여 시그모이드 함수를 요소별로 적용하는 시그모이드 함수의 구현이 되었습니다.

요약하자면 $z^{[1]}$는 $w^{[1]} \times x + b^{[1]}$이고 
$a^{[1]}$는 $\sigma{(z^{[1]})}$임을 알아냈습니다. 다음 그림에서 이어 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175210998-ea1be8c2-2e69-4481-a25d-fef594f9fd56.png)

입력 $x$가 주어진 신경망의 첫 번째 층에 대해, $z^{[1]} = W^{[1]}x + b^{[1]}$와 같고 $a^{[1]} = \sigma{z^{[1]}}$라는 것입니다. 이것의 차원은 $4 \times 1$ 이고 $4 \times 3$ 행렬 곱하기 $3 \times 1$ 벡터 더하기 $4 \times 1$ 벡터 $b$였으며 이것은 끝과 같은 $4 \times 1$ 차원입니다.

$x$는 $a^{[0]}$와 같다고 말한 것을 기억하십시오. 
$\hat{y}$도 $a^{[2]}$와 같다고 해주세요. 원하는 경우 $x$를 $a^{[0]}$로 대체할 수 있습니다. $a^{[0]}$은 입력 특성의 벡터 $x$에 대한 별칭으로 사용하려는 경우이기 때문입니다.

이제 유사한 도출을 통해 출력층이 무엇을 하는지 그것이 그것과 연관되어 있는지 따라서 매개변수 $w^{[2]}$와 $b^{[2]}$도 마찬가지로 다음 레이어에 대한 표현을 작성할 수 있음을 알 수 있습니다.

따라서 이 경우 $w^{[2]}$는 $1 \times 4$ 행렬이 되고 $b^{[2]}$는 $1 \times 4$ 행렬이 됩니다. 그렇기 때문에 $z^{[2]}$가 (1,1) 행렬로 쓸 수 있는 실수입니다. 

이 마지막 상위 단위를 매개 변수 $W$와 $b$가 있는 로지스틱 회귀와 유사하다고 생각하다면 $W^{[2]}$는 실제로 $w^T$이고 $b^{[2]}$는 $b$와 같습니다. 네트워크의 이 왼쪽에 있는 부분은 일단 무시하겠습니다.

이 마지막 상위 단위는 로지스틱 회귀와 매우 유사한데요. 매개 변수를 $w$와 $b$를 쓰는 대신 $W^{[2]}$와 $b^{[2]}$로 쓰고 있다는 점을 제외하고는 차원을 (1,4), (1,1)로 쓰고 있습니다.

---

요약해보겠습니다. 로지스틱 회귀의 경우 출력을 구현하거나 예측을 구현하려면, $z = w^Tx + b$와 같고 $\hat{y} = a = \sigma{(z)}$와 같습니다. 하나의 은닉층이 있는 신경망을 가지고 있을 때 여러분이 구현해야 하는 것은 컴퓨터에서 이 출력이 이 4가지 방정식일 뿐이라는 것입니다.

이것을 은닉층의 로지스틱 회귀 단위에 대한 첫 번째 출력을 계산하는 벡터화된 구현으로 생각할 수 있습니다. 그게 바로 이 일이고 그런 다음 출력층에서 로지스틱 회귀를 수행할 수 있습니다. 이 설명이 이해가 되었기를 바랍니다. 중요한 건 신경망의 출력을 계산하는 것입니다. 필요한 것은 4줄의 코드입니다.

이제 단일 입력 특성인 벡터 $a$가 주어졌을 때 4줄의 코드로 이 신경망의 출력을 계산할 수 있는 방법을 보았습니다. 로지스틱 회귀 분석과 마찬가지로 여러 훈련 예제를 벡터화하려고 합니다.

행렬의 다른 열에서 훈련 예제를 쌓음으로써 이에 대한 약간의 수정만으로도 이 회귀에서 본 것과 마찬가지로 이 신경망의 출력을 계산할 수 있다는 것을 알게 될 것입니다. 한 번에 하나의 예제게 아니라 한 번에 전체 훈련 세트를 연장하십시오 그럼 자세한 내용은 다음에 살펴보도록 하겠습니다.
