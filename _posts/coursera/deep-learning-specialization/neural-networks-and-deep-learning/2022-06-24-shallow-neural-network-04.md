---
title: "[Ⅰ. Neural Networks and Deep Learning] Shallow Neural Networks (4)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Shallow Neural Networks (4)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Shallow Neural Networks

### Explanation for Vectorized Implementation
이전에 학습 예제를 행렬 $X$에 수평으로 쌓아올려 신경망으로 이어지는 벡터화된 구현을 어떻게 도출할 수 있는지 확인했습니다. 이제 조금 더 구체적으로 우리가 쓴 방정식이 여러 예제를 걸쳐 벡터화를 올바르게 구현한 이유를 좀 더 정의해 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175442014-32f51f5f-482b-4cce-82ca-183886035c92.png)

따라서 몇 가지 예를 들어 순방향 전파 계산의 일부를 살펴보겠습니다.

학습 예제에 대한 계산식이 있습니다. 내용을 단순화하기 위해 $b$는 무시하겠습니다. 정당성을 단순화하기 위해 $b$가 $0$과 같다고 가정해봅시다. 하지만 우리가 제시할 주장은 $b$가 $0$이 아니더라도 약간의 변화로 작용한다는 것입니다. 이렇게 내용을 단순한게 만들겠습니다.

그러면 $W^{[1]}$은 행렬이 될겁니다. 이 행렬에는 몇 개의 행이 있습니다. 따라서 $x^{[1]}$을 보면 $W^{[1]}w^{(1)}$은 그림에 보이는 것처럼 열 벡터를 제공합니다. 이는 $Z^{{[1]}(1)}$와 같습니다. $x^{(2)},x^{(3)}$도 동일합니다.

이제 모든 학습 예제를 함께 쌓아서 형성하는 학습 세트 대문자 $X$를 생각해 봅시다. 따라서 행렬 대문자 $X$는 벡터 $x^{(1)}$을 가져와서 수직으로 쌓고 $x^{(2)}, x^{(3)}$도 쌓음으로써 형성됩니다. 이것은 3개의 학습 예제만 가지고 있는 경우입니다.

만약 더 있다고 하면 계속 수평으로 쌓이게 됩니다. 하지만 이 행렬 $X$에 $W$를 곱하면 행렬 곱셈이 어떻게 작용하는지 생각해보면 첫 번째 열은 보라색으로 그린 것과 같은 값이 됩니다. 두 번째 열은 초록색으로 그려진 4개의 값이 됩니다. 그리고 세 번째 열은 오렌지색 값이 될 것입니다.

그러나 이는 당연히 $z^{{[1]}(1)}$이 열 벡터로 표현되고 그 다음으로 $z^{{[1]}(2)}, z^{{[1]}(3)}$도 열 벡터로 표현됩니다. 그리고 이것은 세 가지 학습 예제가 있는 경우입니다. 더 많은 예제를 얻으면 더 많은 열이 생깁니다. 그러면 이것은 행렬 대문자 $Z^{[1]}$입니다.

이 내용은 우리가 당시 단일 학습 예제를 볼 때 이전에 $w^{[1]}x^{(i)}$ = z^{{{[1]}(i)}$와 같은 이유에 대한 정당성을 제공하기를 바랍니다. 다른 학습 예제를 가져와서 다른 열에 쌓으면 해당 열에 $z$도 쌓이게 됩니다.

여기서 다루진 않겠지만 파이썬 브로드캐스팅을 통해서도 확인이 가능합니다. 여기 값들은 다시 더하면, 이 값에 대한 $b$에 값은 여전히 정확합니다. 그리고 실제로 일어나는 일은 결국 파이썬 브로드캐스팅이 되고 결국 이 행렬의 각 열에 대해 개별적으로 $b^{[i]}$가 생기게 되는 것입니다.

그래서 이 그림에서는 $Z^{[1]} = W^{[1]}X + b^{[1]}$의 식을 정당화했을 뿐입니다. 이전에 다룬 네 단계 중 첫 번째 단계의 올바른 벡터화이지만 비슷한 분석을 통해 다른 단계도 매우 유사한 논리를 사용하여 작동한다는 것을 보여줄 수 있습니다.

마지막으로, 전체적으로 복습해보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175442344-e6408798-c006-40cc-bd5b-b763f51ce630.png)




















