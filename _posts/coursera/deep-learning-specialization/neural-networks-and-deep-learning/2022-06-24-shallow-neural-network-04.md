---
title: "[Ⅰ. Neural Networks and Deep Learning] Shallow Neural Networks (4)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Shallow Neural Networks (4)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Shallow Neural Networks

### Activation Function
신경망을 생성할 때, 선택할 수 있는 한 가지 방법은 신경망의 출력 단위와 은닉층에서 사용할 활성화 함수입니다. 지금까지 시그모이드 활성화 함수를 사용했지만 때로는 다른 선택이 훨씬 더 잘 작동할 수 있습니다. 몇가지 옵션을 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175467328-d097040c-d8ec-4104-870a-21b436cea82a.png)

신경망의 순방향 전파단계에서 시그모이드 함수를 사용하는 이 두 단계를 가졌습니다. 그래서 시그모이드를 활성화 함수라고 합니다. 그리고 여기 익숙한 시그모이드 함수 $a = \cfrac{1}{1+e^{-z}}$가 있습니다.

그래서 더 일반적인 경우에 다른 함수 $g(z)$를 가질 수 있습니다. 제가 여기에 적을 것은 $g$는 시그모이드 함수가 아닌 비선형 함수일 수 있습니다. 예를 들어 시그모이드 함수는 0과 1사이에 있습니다.

거의 항상 시그모이드 함수보다 더 잘 작동하는 활성화 함수는 탄젠트 함수 또는 쌍곡선 탄젠트 함수입니다. 이것은 $a = tanh(z)$ 입니다. 그리고 이것은 +1과 -1 사이가 됩니다. $tanh$의 공식은 $\cfran{e^z-e^{-z}}{e^z+e^{-z}}$입니다. 그리고 그것은 사실 수학적으로 시그모이드 함수의 이동 버전입니다.

그래서 시그모이드 함수는 그와 비슷하지만 이제 스케일의 0점을 넘어서 이동했습니다. 따라서 이것은 -1과 +1 사이가 됩니다. 그리고 숨겨진 단위의 경우 함수 $g(z)$를 $tanh(z)$와 같게 하면 됩니다.

---

탄젠트 함수는 항상 시그모이드 함수보다 더 잘 작동합니다. 왜냐하면 +1과 -1사이의 값으로 은닉층에서 나오는 활성화의 평균은 0에 더 가깝기 때문입니다. 때때로 학습 알고리즘을 훈련할 때와 마찬가지로 데이터를 중심에 위치시키고 시그모이드 함수 대신 $tanh$를 이용하여 데이터의 평균이 0이 되도록 할 수 있습니다.

데이터의 평균이 0.5가 아닌 0에 가깝도록 데이터를 중심으로 하는 효과가 있습니다. 이렇게 하면 다음 레이어에 대한 학습이 조금 더 쉬워집니다. 최적화 알고리즘에 대해서도 다음에 자세히 설명하겠습니다.

그러나 한 가지 유의할 점은 더 이상 시그모이드 활성화 함수를 거의 사용하지 않는다는 것입니다. $tanh$ 함수는 거의 항상 우수합니다. 한 가지 예외는 출력 레이어에 대한 것입니다. $y$가 0이거나 1이면 $\hat{y}$이 -1과 1사이가 아니라 0과 1사이에 있는 출력하려는 숫자가 되는 것이 합리적입니다.

시그모이드 활성화 함수를 사용하는 한 가지 예외는 이진 분류를 사용할 때입니다. 이 경우 상위 레이어의 시그모이드 활성화 함수를 사용할 수 있습니다. 따라서 $g^{[2]}(z^{[2]}) = \sigma{(z^{[2]})}$와 같습니다. 그래서 이 예제에서는 은닉층에 대한 $tanh$ 활성화 함수와 출력층에 대한 시그모이드가 있는 곳입니다.

따라서 활성화 함수는 레이어마다 다를 수 있습니다. 그리고 때때로 활성화 함수가 레이어마다 다르다는 것을 나타내기 위해 대괄호 윗첨자를 사용하여 $g^{[1]}$과 $g^{[2]}$이 다를 수 있음을 나타낼 수도 있습니다. 다시, [1] 윗첨자는 은닉층을 나타내고 윗첨자 [2]이 출력층을 나타냅니다.

시그모이드 함수와 $tanh$함수의 단점 중 하나는 $z$가 매우 크거나 매우 작으면 이 함수의 기울기의 도함수가 매우 작아진다는 것입니다. 따라서 $z$가 매우 크거나 $z$가 매우 작으면 함수의 기울기가 0에 가까우므로 기울기 하강 속도가 느려질 수 있습니다.

---

머신 러닝에서 매우 인기 있는 또 다른 선택은 정류 선형 유닛(ReLU)라고 불리는 것입니다. 공식은 $a = max(0,z)$입니다. 따라서 $z$가 양수이고 도함수이거나 $z$가 음수일 때 기울기가 0이면 도함수는 1입니다. 이것을 구현하는 경우 $z$가 정확히 0일 때의 도함수는 기술적으로 잘 정의되지 않습니다. 하지만 컴퓨터에서 이것을 구현할 때 정화히 $z$가 0.000000000000이 될 확률은 매우 낮습니다. 그래서 걱정할 필요는 없습니다.

실제로 $z$가 0일 때 도함수를 1 또는 0으로 가정할 수 있습니다. 여기 활성화 기능을 선택하기 위한 몇 가지 규칙이 있습니다. 출력이 0, 1 값이고 이진 분류를 사용하는 경우 시그모이드 활성화 함수는 출력층에 대해 매우 자연스러운 선택입니다. 그런 다음 다른 모든 단위 값 또는 정류된 선형 유닛은 점차 활성화 함수의 기본 선택이 됩니다.

은닉층에 무엇을 사용해야 할지 잘 모르겠다면 요즘 대부분의 사람들이 사용하고 있는 값 활성화 함수를 사용합니다. 비록 사람들이 가끔 $tanh$ 활성화 함수를 사용하기도 하지만 말입니다.

값의 한 가지 단점은 $z$가 음수일 때 도함수가 0과 같다는 것입니다. 실제로 이것은 잘 작동합니다. 그러나 Leaky ReLU라는 값의 또 다른 버전이 있습니다. 이어서 공식을 알려드리겠지만 $z$가 음수일 때 0이 되는 대신 약간의 기울기가 필요합니다. 이를 Leaky ReLU라고 합니다.

일반적으로 값 활성화 함수보다 더 잘 작동합니다. 하지만 실제로는 잘 사용하지는 않습니다. 둘 중 어느 하나라도 괜찮습니다. 하지만, 하나를 골라야 한다면 저는 보통 그 값을 사용합니다. 그리고 값과 Leaky ReLU의 장점은 활성화 함수의 도함수인 $z$의 많은 공간에 대해 활성화 함수의 기울기가 0과 매우 다르다는 것입니다.

그래서 실제로 값 활성화 기능을 사용하여 신경망은 종종 $tanh$ 또는 시그모이드 활성화 함수를 사용할 때 보다 훨씬 빠르게 학습할 것입니다. 주된 이유는 함수의 기울기가 0이 되어 학습 속도가 느려지는 효과가 적기 때문입니다. 그리고 $z$ 범위의 절반에 대해 값의 기울기가 0이라는 것을 알고 있습니다.

따라서 대부분의 훈련 예제에서 학습은 여전히 매우 빠를 수 있습니다. 다음으로 다른 활성화 함수들의 장단점을 간단히 요약해 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175467445-71c07acf-534d-47ec-bc94-2d79ce57c5b5.png)

여기 시그모이드 활성화 함수가 있습니다. 저는 이항 분류를 하거나 거의 사용하지 않는 경우 출력층을 제외하고는 이것을 사용하지 말라고 말하고 싶습니다. 그리고 제가 이것을 거의 사용하지 않는 이유는 $tanh$가 훨씬 더 완벽하기 때문입니다.

그리고 기복적으로 가장 일반적으로 사용되는 활성화 함수 ReLU입니다. 그래서 무엇을 더 사용해야 할지 모르겠다면 이것을 사용하세요. 그리고 자유롭게 Leaky ReLU도 사용해 보세요.

---

그래서 신경망에서 사용할 수 있는 활성화 함수의 선택에 대한 감을 잡기를 바랍니다. 딥 러닝에서 보게될 것 중 하나는 신경망을 구축하는 방법에 있어 다양한 선택이 종종 있다는 것입니다. 숨겨진 단위의 수에서 선택 활성화 기능 나중에 볼 방법을 초기화하는 방법까지 다양합니다. 그런 많은 선택지들이 있습니다.

그리고 어떤 것이 당신의 문제에 가장 적합한지에 대한 좋은 지침을 얻는 것이 때때로 어렵다는 것이 밝혀졌습니다. 그래서 이 과정을 통해 업계에서 무엇이 더 인기 있는지 또는 덜 인기 있는지에 대해 계속해서 알려드리겠습니다.

하지만 애플리케이션의 경우 어떤 것이 가장 잘 동작하는지 미리 알기란 매우 어렵습니다. 일반적인 조언은 어떤 활성화 함수가 가장 잘 작동하는지 잘 모르겠으면 모두 시도해 보는 것입니다. 홀드아웃 검증 세트나 개발 세트처럼 평가해 주십시오. 이것에 대해서는 나중에 설명하겠습니다.

어떤 것이 가장 잘 작동하는지 살펴보고 그것을 사용하라는 것입니다. 이러한 다양한 선택들을 애플리케이션에 맞게 테스트함으로써 향후에 신경망 아키텍처에 대한 특이성 문제를 보다 효과적으로 입증할 수 있을 것으로 생각합니다.
 
알고리즘의 진화뿐만 아니라 항상 값 활성화를 사용하고 다른 것은 사용하지 말라고 말한다면 결국 작업하게 되는 모든 문제에 적용될 수도 있고 적용되지 않을 수도 있습니다. 가까운 미래에 또는 먼 미래에 말이죠.

그것은 활성화 함수의 선택이었고 가장 인기 있는 활성화 함수를 볼 수 있습니다. 가끔 물어볼 수 있는 또 다른 질문이 있습니다. 활성화 함수를 사용해야 하는 이유는 무엇입니까? 왜 그걸 그냥 없애버리지 않는 거죠? 다음에는 신경망에 일종의 비선형 활성화 함수가 필요한 이유에 대해 다뤄보겠습니다.
