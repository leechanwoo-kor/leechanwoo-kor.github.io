---
title: "[Ⅰ. Neural Networks and Deep Learning] Shallow Neural Networks (4)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Shallow Neural Networks (4)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Shallow Neural Networks

### Explanation for Vectorized Implementation
이전에 학습 예제를 행렬 $X$에 수평으로 쌓아올려 신경망으로 이어지는 벡터화된 구현을 어떻게 도출할 수 있는지 확인했습니다. 이제 조금 더 구체적으로 우리가 쓴 방정식이 여러 예제를 걸쳐 벡터화를 올바르게 구현한 이유를 좀 더 정의해 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175442014-32f51f5f-482b-4cce-82ca-183886035c92.png)

따라서 몇 가지 예를 들어 순방향 전파 계산의 일부를 살펴보겠습니다.

학습 예제에 대한 계산식이 있습니다. 내용을 단순화하기 위해 $b$는 무시하겠습니다. 정당성을 단순화하기 위해 $b$가 $0$과 같다고 가정해봅시다. 하지만 우리가 제시할 주장은 $b$가 $0$이 아니더라도 약간의 변화로 작용한다는 것입니다. 이렇게 내용을 단순한게 만들겠습니다.

그러면 $W^{[1]}$은 행렬이 될겁니다. 이 행렬에는 몇 개의 행이 있습니다. 따라서 $x^{[1]}$을 보면 $W^{[1]}w^{(1)}$은 그림에 보이는 것처럼 열 벡터를 제공합니다. 이는 $Z^{{[1]}(1)}$와 같습니다. $x^{(2)},x^{(3)}$도 동일합니다.

이제 모든 학습 예제를 함께 쌓아서 형성하는 학습 세트 대문자 $X$를 생각해 봅시다. 따라서 행렬 대문자 $X$는 벡터 $x^{(1)}$을 가져와서 수직으로 쌓고 $x^{(2)}, x^{(3)}$도 쌓음으로써 형성됩니다. 이것은 3개의 학습 예제만 가지고 있는 경우입니다.

만약 더 있다고 하면 계속 수평으로 쌓이게 됩니다. 하지만 이 행렬 $X$에 $W$를 곱하면 행렬 곱셈이 어떻게 작용하는지 생각해보면 첫 번째 열은 보라색으로 그린 것과 같은 값이 됩니다. 두 번째 열은 초록색으로 그려진 4개의 값이 됩니다. 그리고 세 번째 열은 오렌지색 값이 될 것입니다.

그러나 이는 당연히 $z^{{[1]}(1)}$이 열 벡터로 표현되고 그 다음으로 $z^{{[1]}(2)}, z^{{[1]}(3)}$도 열 벡터로 표현됩니다. 그리고 이것은 세 가지 학습 예제가 있는 경우입니다. 더 많은 예제를 얻으면 더 많은 열이 생깁니다. 그러면 이것은 행렬 대문자 $Z^{[1]}$입니다.

이 내용은 우리가 당시 단일 학습 예제를 볼 때 이전에 $w^{[1]}x^{(i)}$ = z^{{{[1]}(i)}$와 같은 이유에 대한 정당성을 제공하기를 바랍니다. 다른 학습 예제를 가져와서 다른 열에 쌓으면 해당 열에 $z$도 쌓이게 됩니다.

여기서 다루진 않겠지만 파이썬 브로드캐스팅을 통해서도 확인이 가능합니다. 여기 값들은 다시 더하면, 이 값에 대한 $b$에 값은 여전히 정확합니다. 그리고 실제로 일어나는 일은 결국 파이썬 브로드캐스팅이 되고 결국 이 행렬의 각 열에 대해 개별적으로 $b^{[i]}$가 생기게 되는 것입니다.

그래서 이 그림에서는 $Z^{[1]} = W^{[1]}X + b^{[1]}$의 식을 정당화했을 뿐입니다. 이전에 다룬 네 단계 중 첫 번째 단계의 올바른 벡터화이지만 비슷한 분석을 통해 다른 단계도 매우 유사한 논리를 사용하여 작동한다는 것을 보여줄 수 있습니다.

마지막으로, 전체적으로 복습해보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/175442344-e6408798-c006-40cc-bd5b-b763f51ce630.png)

이것이 당신의 신경망이라면 우리는 이것이 전파를 위해 구현해야 할 일이라고 말했고 $1$에서 $m$까지 하나 하나의 학습예제는 학습 예제 1과 같습니다. 그리고 나서 우리는 열에 학습 예제를 쌓고 각 $z$와 $a$값에 대해 해당 열을 쌓습니다. 따라서 그림에는 $A^{[1]}$의 예이지만 나머지도 마찬가지입니다.

이전에 그 다음으로 보여준 것은 $Z^{[1]} = W^{[1]}X + b^{[1]}$이 모든 $m$ 예제에서 동시에 벡터화 시킬 수 있다는 것입니다. 그리고 비슷한 추론으로 밝혀지면 다른 모든 줄이 이 네 줄의 코드에서 모두 올바른 벡터화라는 것을 보여줄 수 있습니다.

다시 말씀드리자면, $x = a^{[0]}$이기 때문에 $x^{(i)} = a^{{[0]}(i)}$로 표현할 수 있었습니다. 그리고 첫 번째 방정식에도 대칭성이 있습니다. $z,a$ 각 쌍의 방정식이 실제로 매우 유사해 보이지만 모든 지수가 1씩 진행된다는 것을 알 수 있습니다.

이것은 신경망의 다른 층들이 대략 같은 일을 하고 있거나 같은 계산을 반복하고 있따는 것을 보여줍니다. 여기 두 층의 신경망이 있습니다. 다음에는 더 깊은 신경망에 대해 배우겠습니다. 보다 깊은 신경망이 기본적으로 이 두 단계를 수행하고 있습니다. 여기에서 보는 것보다 훨씬 더 많은 시간을 수행한다는 것을 알 수 있습니다.

이렇게 하면 여러 학습 예제에서 신경망을 벡터화할 수 있습니다. 다음으로, 지금까지 우리는 신경망 전반에 걸쳐 시그모이드 함수를 사용해왔습니다. 그것은 사실 최선의 선택이 아니라는 것이 밝혀졌습니다. 다음엔 시그모이드 함수가 하나의 가능한 선택인 다른 활성화 함수를 어떻게 사용할 수 있는지 조금 더 자세히 알아보겠습니다.
