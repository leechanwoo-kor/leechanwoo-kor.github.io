---
title: "[Ⅰ. Neural Networks and Deep Learning] Shallow Neural Networks (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Shallow Neural Networks (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Shallow Neural Networks

### Neural Network Overview
이번에는 신경망을 구현하는 방법을 다뤄보겠습니다. 기술적 디테일로 넘어가기 전에, 볼 내용에 대한 간략한 개요를 보여드리고자 합니다. 따라서 모든 디테일을 따라가지 못하더라도 걱정하지 마십시오. 다음에 기술적인 디테일에 대해 자세히 알아볼 것입니다. 지금은 신경망을 구현하는 방법에 대해 자세히 알아볼 것입니다.

![image](https://user-images.githubusercontent.com/55765292/175183139-b882c426-6d0e-4f01-be06-2d52da9fe14d.png)

지난 번에 로지스틱 회귀에서 모델이 계산 초안에 어떻게 대응하는지 확인했습니다. 그런 다음 특성 $x$ 및 매개변수를 배치합니다. $w$ 및 $b$를 사용하면 $z$를 계산할 수 있으며 이는 $a$를 계산하는 데 사용됩니다. 그리고 출력 $\hat{y}$과 상호 교환 가능하게 사용하면 손실함수 $L$을 계산할 수 있습니다.

신경망은 그림 아래와 같이 생겼습니다. 전에 말했듯이, 많은 작은 시그모이드 유닛을 함께 쌓아서 신경망을 형성할 수 있습니다. 이전에 이 노드가 두 단계를 계산하는 데 해당합니다. 첫 번째는 $z$ 값을 계산하는 것이고 두 번째는 이것을 $a$의 값으로 계산한다는 것이죠.

이 신경망에서, 처음 노드 스택은 $z$와 $a$를 계산합니다. 그런 다음 다음 노드에서는 다른 $z$과의 계산을 합니다.

따라서 나중에 소개할 표기법은 다음과 같습니다.

먼저 특성 $x$를 일부 매개변수 $w$ 및 $b$와 함께 입력하면 이를 통해 $z^{[1]}$를 계산할 수 있습니다. 그래서 우리는 소개할 새로운 표기법은 윗첨자 [1]을 사용하여 노드 스택과 관련된 양을 나타내는 것입니다.

이를 레이어라고 합니다.

그런다음 윗첨자 [2]를 사용하여 해당 노드와 관련된 수량을 나타냅니다. 이를 신경망의 또 다른 레이어라고 합니다. 윗첨자 대괄호는 개별 훈련 예제를 참조할 때 사용하는 윗첨자 괄호와 혼동하지 마십시오.

따라서 $x$개의 윗첨자 (i)가  i번째 훈련 예제를 참조하는 반면, 윗첨자 [1]과 [2]는 이러한 서로 다른 레이어 즉 신경망의 레이어 1과 레이어 2를 나타냅니다.

하지만 로지스틱 회귀와 유사하게 $z^{[1]}$를 계산한 후, $a^{[1]}$을 계산하기 위한 연산이 있을 것입니다. 그것은 $z^{[1]}$의 시그모이드일 뿐이고 다른 선형 방정식을 사용하여 $z^{[2]}$를 계산한 다음 $a^{[2]}$를 계산합니다. $a^{[2]}$는 신경망의 최종 출력이며 $\hat{y}$와 상호 교환하여 사용할 수 있습니다.

많은 디테일인건 알지만 중요한 직관은 로지스틱 회귀의 경우 $z$ 뒤에 계산이 뒤따른다는 것입니다. 이 신경망에서는 여러번 반복하는 데요. $z$ 뒤에 계산을 하고 $z$ 뒤에 계산이 뒤따른다는 것이며, 마지막에 손실을 계산하게 됩니다.

로지스틱 회귀의 경우, 도함수를 계산하기 위해 또는 $da$, $dz$ 등을 계산할 때 역방향 계산을 수행했습니다. 따라서 같은 방식으로 신경망은 $da^{[2]}$, $dz^{[2]}$를 계산하고 $dw^{[2]}$, $db^{[2]}$를 계산하여 역방향 계산을 수행하게 됩니다.

빨간색 화살표로 표시하는 오른쪽에서 왼쪽으로 역방향 계산입니다. 이것은 신경망이 어떻게 생겼는지 간략하게 알 수 있습니다. 이것은 기본적으로 로지스틱 회귀를 취하고 두 번 반복합니다.

다음은 신경망 표현에 대해서 알아보겠습니다.





