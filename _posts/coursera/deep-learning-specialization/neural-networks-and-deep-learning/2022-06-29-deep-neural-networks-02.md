---
title: "[Ⅰ. Neural Networks and Deep Learning] Deep Neural Networks (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Deep Neural Networks (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Deep Neural Networks

## Deep Neural Network

### Getting your Matrix Dimensions Right
심층 신경망을 구현할 때, 코드의 정확성을 확인하기 위해 자주 사용하는 디버깅 도구 중 하나는 종이 한 장을 꺼내서 작업 중인 행렬의 차원을 살펴보는 것입니다. 이렇게 하면 심층 네트워크도 간단하게 구현할 수 있을 것 같으니 그 방법을 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/176329821-f03d254c-366f-4595-8d59-9eb0d042cafa.png)

따라서 $L = 5$입니다. 입력층을 세지 않기 때문에, 여기에는 5개의 층이 있고 4개의 은닉층과 1개의 출력층이 있습니다. 순방향 전파를 구현하는 경우 첫번째 단계는 $Z^{[1]} = W^{[1]}X + b^{[1]}$입니다.

우선 $b$는 무시하고 매개변수 $W$에 집중합시다. 첫 번째 은닉층은 3개의 은닉 유닛이 있습니다. 이전에 얻은 표기법을 사용하면 레이어 1의 은닉 유닛 수인 $n^{[1]}$이 3이 됩니다. 여기서 $n^{[2]} = 5, n^{[3]} = 4, n^{[5]} = 1$입니다.

지금까지 우는 단일 출력 유닛을 가진 신경망만을 보았지만 이후 과정에서는 다중 출력 유닛을 가진 신경망에 대해서도 설명하겠습니다. 마지막으로 입력층의 경우 또한 $n^{[0]} = n_x = 2$입니다.

이제 $Z,W,X$의 차원에 대해 생각해 봅시다. $Z$는 이 첫 번쨰로 은닉층에 대한 활성화 벡터입니다. 그러면 $Z$는 $3 \times 1$이 될 것이고, 3차원 벡터가 될 것입니다. 1차원 행렬로 $n^{[1]$을 이 경우에 $3 \times 1$이라고 쓰겠습니다.

이제 입력 특성 $X$를 보겠습니다. $X$는 2개의 입력 특성이 있습니다. 따라서 $X$는 이 예제에서 $2 \times 1$이지만 더 일반적으로는 $n^{[0]} \times 1$이 됩니다. 우리가 필요한 것은 행렬 $W^{[1]$이 $n^{[0]}$에 1벡터를 곱하면 $n^{[1]} \times 1$ 벡터가 되는 것입니다.

따라서 3차원 벡터는 2차원 벡터에 뭔가를 곱한 것입니다. 행렬 곱셈의 규칙에 따르면 $3 \times 2$행렬에 $2 \times 1$ 행렬을 곱하거나 $2 \times 1$ 벡터를 곱하면, $3 \times 1$벡터를 얻을 수 있습니다. 더 일반적으로, 이것은 $n^{[1]}$에 의한 $n^{[0]}$ 차원 행렬이 될 것입니다.

그래서 여기서 우리가 알아낸 것은 $W^{[1]}$의 차원이 $n^{[1]} \times n^{[0]}$이어야 하고, 더 일반적으로 $w^{[l]}$의 차원이 $n^{[l]} \times n^{[l-1]}$이어야 한다는 것입니다. 예를 들어, $w^{[2]}$의 차원은 $5 \times 3$이거나 $n^{[2]} \times n^{[1]}$이어야 합니다. 왜냐하면 $Z^{[2]}$를 $W^{[2]} \times a^{[1]}$로 계산할 것이기 때문입니다.

다시 말하지만, 이번에도 바이어스를 무시할 겁니다. 그러면 그것은 $3 \times 1$이 될 것입니다. 이것은 $5 \times 1$이 되어야 합니다. 따라서 $5 \times 3$이 더 좋습니다.

마찬가지로 $W^{[3]}$은 다음 레이어의 차원 이전 레이어의 차원입니다. 따라서 $4 \times 5$가 됩니다. $W^[4]$는 $2 \times 4$가 되며, $W^{[5]}$는 $1 \times 2$가 됩니다. 확인해야할 일반적인 공식은 레이어 $L$에 대한 행렬을 구현할 때 해당 행렬의 차원이 $n^{[l]} \times n^{[l-1]}$이라는 것입니다.

이제 이 벡터 $b$의 차원에 대해 생각해 봅시다. $b^{[1]}$은 $3 \times 1$ 벡터가 될 것이고, $3 \times 1$ 벡터를 출력으로 얻기 위해 다른 $3 \times 1$ 벡터에 더해야 합니다. $b^{[2]}$은 $5 \times 1$이 될 것입니다.

좀 더 일반적인 규칙은 왼쪽에 있는 예제에서, $b^{[1]}$는 $n^{[1]} \times 1$이라는 것이죠. 두 번째 예제로는 $n^{[2]} \times 1$이므로 보다 일반적인 경우 $b^{[l]}$은 $n^{[l]} \times 1$차원이어야 합니다.

역전파를 구현한다면 dw의 차원은 w의 차원과 같아야 합니다. 따라서 dw는 w와 같은 차원이어야 하고, db는 b와 같은 차원이어야 합니다.

이제 차원을 확인할 수 있는 또 다른 중요한 수량은 $Z, X$ 및 $A^{[l]}$인데요. 여기서 많이 이야기하지 않았던 것들입니다. $Z^{[l]} = g^{[l](a^{[1]})$이기 때문에, 요소별로 적용한 다음 $Z$와 $A$는 이러한 유형의 네트워크에서 동일한 차원을 가져야합니다.

이제 한 번에 여러 예제를 살펴보는 벡터화된 구현이 있는 경우 어떤 일이 발생하는지 보겠습니다. 물론 벡터화의 구현의 경우에도 $w, b, dw ,db$의 차원은 동일하게 유지됩니다. 그러나 벡터화된 구현에서는 $x$와 마찬가지로 $za$의 차원도 약간 변경됩니다.

![image](https://user-images.githubusercontent.com/55765292/176329873-45d9d194-16bd-41a2-b4a4-d11d649796e9.png)

