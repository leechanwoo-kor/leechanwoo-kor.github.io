---
title: "[Ⅰ. Neural Networks and Deep Learning] Deep Neural Networks (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Deep Neural Networks (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Deep Neural Networks

## Deep Neural Network

### Deep L-layer Neural Network

지금까지 여러분은 단일 은닉층과 로지스틱 회귀를 사용하는 신경망에 맥락에서 순방향 전파와 역방향 전파를 보았고 벡터화에 대해 배웠고 방법을 무작위로 초기화하는 것이 언제 중요한지에 대해서도 배웠습니다. 지난 연습에서 이러한 아이디어 중 일부가 스스로 작동하는 것을 구현하고 보았습니다. 지금쯤이면 심층 신경망을 구현하는 데 필요한 대부분의 아이디어를 실제로 보았을 것입니다.

이제 할 일은 그 아이디어를 모아서 자신만의 심층 신경망을 구현할 수 있도록 하는 것입니다. 내용은 짧게 다룰 것이고 연습을 하면서 신경망에 대한 깊은 생각을 가질 수 있기를 바랍니다.

![image](https://user-images.githubusercontent.com/55765292/176098229-bdac3bd9-885d-44a6-a828-3cbb9df1efcf.png)

자 그러면 심층 신경망은 무엇일까요? 여러분은 로지스틱 회귀를 위해 이 그림을 보았고 하나의 은닉층이 있는 신경망도 보았습니다. 그래서 여기 2개의 은닉층이 있는 신경망과 5개의 은닉층이 있는 신경망의 예제가 있습니다.

로지스틱 회귀는 매우 "얕은" 모델인 반면 은닉층이 많은 훨씬 더 깊은 모델이며 얕음과 깊음은 정도의 문제라고 말합니다. 그래서 하나의 은닉층의 신경망은 2층 신경망이 될 것입니다. 신경망에서 레이어를 계산할 때 입력층을 계산하지 않고 출력층과 마찬가지로 은닉층만 계산한다는 것을 기억하십시오.

따라서 2층 신경망이 여전히 매우 얕지만 로지스틱 회귀 만큼 얕지는 않습니다. 기술적으로 로지스틱 회귀는 단층 신경망이므로 가능했지만 지난 몇 년 동안 AI는 머신 러닝 커뮤니티에서 심층 신경망이 학습할 수 있는 기능이 있다는 것을 깨달았습니다. 그것은 얕은 모델은 학습할 수 없는 것입니다.

주어진 문제에 대해 원하는 네트워크 깊이를 미리 예측하는 것은 어려울 수 있습니다. 로지스틱 회귀를 시도하고 1개 및 2개의 은닉층을 시도하고 은닉층의 수를 다양한 값을 시도할 수 있는 또 다른 하이퍼 매개변수로 표시하여 유효성 검증 데이터 또는 개발 세트에서 평가하는 것이 합리적입니다. 나중에 자세히 보도록 하겠습니다.

![image](https://user-images.githubusercontent.com/55765292/176098294-0040fe20-2836-481d-9029-e59857efd022.png)

이제 심층신경망을 나타내는 표기법에 대해 알아보겠습니다. 여기 4개의 레이어 신경망이 있는데 3개의 은닉층이 있고 이 은닉층의 단위 수는 5, 5, 3이고 그리고 하나의 상위 유닛이 있습니다. 그래서 우리가 사용할 표기법은 대문자 $L$을 사용하여 네트워크의 레이어 수를 나타냅니다.

이 경우, $L = 4$, 레이어의 수도 마찬가지이며 노드 수 또는 레이어 소문자 l의 단위 수를 나타내기 위해 N개의 윗첨자 [l]을 사용할 것입니다. 따라서 입력층 인덱싱하면 레이어 $0$으로 입력됩니다. 순서대로 레이어 1, 2, 3, 4입니다.

그러면 예를 들어 $n^{[2]}$이 있고 첫 번째는 5와 같을 것입니다. 왜냐하면 거기에 5개의 은닉 유닛이 있기 때문입니다. 이를 위해, 우리는 $n^{[2]}$을 가지고 있으며 2번째 은닉층의 단위 수는 5, $n^{[3]} = 3$과 같고 $n^{[4]} = n^{[L]} = 1$입니다. 대문자 $L$이 4와 같기 때문에 이 상위 단위 수는 1입니다.

그리고 여기에서는 입력 층으로 $n^{[0]} = n_x = 3$이라는 값이 됩니다. 그래서 다른 레이어에 있는 노드의 수를 설명하는 데 사용하는 표기법입니다.

각 레이어 $L$에 대해, 레이어 $l$의 활성화를 나타내기 위해 $a^{[l]}$을 사용할 것입니다. 그래서 우리는 나중에 전파를 위해 $a^{[l]}$을 활성화 $g(z^{[l]})$로 계산하고 활성화는 레이어 $l$에 의해 인덱싱될 것입니다.

그런 다음 $W^{[l]}$을 사용하여 레이어 $l$에서 $z^{[l]}$ 값을 계산하기 위한 가중치를 나타내고 마찬가지로 $b^{[l]}$은 $z^{[l]}$을 계산하는 데 사용됩니다.

마지막으로, 표기법을 마무리하기 위해 입력 기능은 $x$라고 불리지만 $x$는 레이어 0의 활성화이기도 하므로 $a^{[0]} = x$이고 최종 레이어의 활성화인 $a^{[L]} = \hat{y}$입니다. 따라서 $a^{[L]}$은 신경망의 예측 $\hat{y}$에 대한 예측된 출력과 같습니다.

심층신경망이 어떻게 생겼는지 보셨는데요. 심층 네트워크를 설명하고 계산하는 데 사용하는 표기법도 마찬가지입니다. 다음으로 이러한 유형의 네트워크에서 순방향 전파가 어떤 모습인지 설명하겠습니다.


