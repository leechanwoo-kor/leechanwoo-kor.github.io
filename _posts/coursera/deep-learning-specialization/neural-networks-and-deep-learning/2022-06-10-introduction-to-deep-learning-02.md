---
title: "[Ⅰ. Neural Networks and Deep Learning] Introduction to Deep Learning (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Introduction to Deep Learning (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png)

<br><br>

# Introduction to Deep Learning

## Introduction to Deep Learning

### Why is Deep Learning taking off?

![image](https://user-images.githubusercontent.com/55765292/172992154-84f7dad3-9e4c-452a-ad06-d5f75165a00f.png){: .align-center}

![image](https://user-images.githubusercontent.com/55765292/172992180-bfced034-f654-45a5-8445-efb41de1634c.png){: .align-center}

딥러닝의 기술적인 측면과 여러분의 딥러닝 네트워크 개념이 수십 년 동안 존재해왔다고 하면 왜 지금이 되어서야 뜨고 있는 것일까요? 이번 강의에서는 딥 러닝이 부상하게 된 주요 원인이 되는 내용을 다루어 볼 텐데요 이런 내용을 통해 여러분들이 종사하고 있는 기업 내에서 가장 좋은 기회를 누리시고 실제로
적용하는 계기가 되실 겁니다 지난 몇 년 동안 많은 사람들이 저에게 질문하여 왔습니다 "앤드류, 딥러닝이 갑자기
이렇게 잘되는 이유가 뭔가요" 라고 말이죠, 이런 질문을 받을 때면 이런 그림을 그려주곤 하죠 이런 형체를 가로줄에 그리고 수행 업무의 데이터 양을 나타내고, 세로줄에는 방금 설명한 교육 알고리즘의 성능을 구체적으로 spam classifier(스팸 차단기)나 ad click predictor 또는, 신경 예측기의 정확도 예측 변수 또는 결과 위치를 파악하기 위한 다른 차와 자율 주행차에 말이죠 신경망의 정확도는 지원 벡터 머신이나 support vector machine (지지 벡터 기계)이나 기존 학습 알고리즘의 성능을 보유한 데이터 양의 함수로 표시하면 곡선을 얻을 수 있습니다 데이터를 추가할수록 성능이 일시적으로 향상되어 보이는 효과가 있는데요 잠시 후 성능이 상당히 좋아진다고 가정해 보십시오 여러분이 알고 있는 성능의 개념은 만약 가로선이 이렇게 된다고 해봅시다 방대한 양의 데이터를 어떻게 활용할지 몰랐다고 생각하면 지난 10년 동안 우리 사회에서 벌어진 현상은 많은 문제들에 대하여 소량의 데이터를 보유하고 있다가 꽤 많은 양의 데이터를 보유하게 된 걸로 바뀌었고 이러한 모든 현상이 사회가 디지털화되면서 가능해진 것입니다 수많은 인간 활동에 있어 디지털 영역이 상당부분 차지하고 있는데요 컴퓨터, 웹사이트, 모바일앱에서 많은 시간을 소비하고 디지털 장치에서 활동하면서 데이터를 생성하고 핸드폰 안에 장착되는 저렴한 카메라 가속도계 등 IoT의 센서들을 통해 더 많은 양의 데이터를 수집하게 되었습니다 과거 20년 동안 수많은 양의
데이터를 다루게 되었는데요 많은 어플을 통해 더 많은 양의 데이터를 축적하고 전통적인 트레이닝 알고리즘과 비교하여 더 많은 양을 효과적으로 다루면서 새로운 네트워크의 장점을
누릴 수 있게 되었습니다 작은 신경망을 트레이닝 한다면 그 성능은 이렇게 보여질 수 있습니다 medium-sized 인터넷이라고
불리는 조금 더 큰 버전의 인터넷을 트레이닝 시키면 조금 더 나은 그래프가 이렇게 나오는데요 이것이 매우 큰 신경망을 트레이닝 한다면 그 형태 그대로 유지하면서 지속적으로 성능이 향상되는 경향이 있습니다 첫 번째로 종종 엄청난 양의 데이터를 활용하기 위해 2가지가 필요합니다 충분히 큰 신경망을 훈련할 수 있도록 매우 방대한 양의 데이터를
이용할 수 있으려면 말이죠 두 번째로 x축에서 여기에 위치해야 하는데요 그렇게 하기 위해선 많은 양의
데이터를 필요로 합니다 그렇기 때문에 규모가 딥 러닝
진전을 주도하고 있으며 규모면에서 오늘날 가장 신뢰할 수 있는 신경망의 크기를 이야기하기도 하고 새로운 네트워크, 숨겨진 다량의 유닛 많은 양의 변수, 연결 요소들 그리고 데이터의 scale을 이야기하기도 합니다 네트워크 방법 중 하나는 신경망의 성능을 향상 시키기 위해선 더 큰 네트워크를 트레이닝 시키거나 더 많은 데이터를 구현시키는 방법입니다 이러한 시도들은 특정 시점까지만 효과가 있는데요 특정 지점까지만 작동합니다 아니면 결국에는 여러분의 네트워크가 너무 커져서 트레이닝 시키는데 너무 많은 시간이 이 도표를 좀 더 기술적으로 정밀하게 만들기 위해 해당 도표를 조금 더 기술적으로 정확도를 높이고 학습의 세계에서 먼 길을 왔습니다 데이터의 양, 엄밀히 이야기하면 이건 레이블 되어 있는 데이터의 양인데요 입력 X와 레이블 Y가 모두 있는 훈련 예제를 의미합니다 추가로 표기법을 도입했습니다 나중에 학습할 비디오에서 사용할 것입니다 이제부터는 소문자 알파벳으로 사용하도록 하겠습니다 트레이닝 세트의 사이즈를 나타낼 때나 트레이닝 예시의 개수를 나타낼 때 소문자 m을 사용하고 가로축에 표기될 것입니다 해당 표 관련해서, 몇 개 더 이야기할 텐데요 더 작은 트레이닝 세트의 지도에서는 알고리즘의 상대적 순서가 잘 정의되어 있지 않기 때문에 트레이닝 세트가 많이 없다고 하면 본인 고유 스킬 엔지니어링 특성에 좌우할 수 있습니다 그렇기 때문에 누군가가 SVM 을 트레이닝 시킬 경우 hand engineering을 쓰기 원한다면 본인이 더 큰 트레이닝 세트를 다루고 지금 것은 작은 트레이닝이라고 여기면 SEM이 더 효과적일 수 있습니다 이 영역에서 왼쪽부분은 상대적 순서가 유전자 알고리즘끼리의 배열이 잘 정의되어 있지 않고, 그 성능은 개인의 엔진 특성 스킬에 따라 그리고 알고리즘의 모바일 정보에
따라 차이가 날 수 있습니다 그리고 오로지 많은 양의 트레이닝세트가 있는
빅데이터 체제에서만 대문자 M, 즉 오른쪽부분에서만 더 일관되게 큰 신경망을 보게 됩니다 우위적으로 점유하고 있는 것을 볼 수 있습니다 만약 여러분 친구가 신경망이 왜 뜨고 있는지 물어본다면, 저 같은 경우에 이 그림을 그릴 것 같습니다 저는 딥러닝이 급부상하던 초기 시점에는 scaled data와 scale computation이 중심적인 역할을 했다고 봅니다 아주 큰 신경망 네트워크를 매우 큰 신경망을 말이죠 CPU나 GPU에서 말이죠 이를 통해 딥러닝 분야는 최근 몇 년간 많은 발전을 이루었고 아주 큰 알고리즘 혁신을 이루었습니다 그래서 제가 과소평가하고 싶지 않은 부분은 알고리즘의 혁신 중 상당 부분은 신경망을 더 빨리 운영되게 만들고 이 중 가장 구체적인 예로 가장 큰 네트워크의 변화를 가지고 온 부분은 바로 시그모이드 함수에서 이렇게 보이는 함수인데요 이렇게 생긴 함수에서 ReLU(렐류) 함수로 변경되었다는 점입니다 이전 비디오에서 잠깐 다루었던 내용이죠 제가 이야기하고 있는 내용이 잘 이해 가지 않으시면 너무 걱정하지 마십시오 시그모이드 함수의 문제는 나중에 알고 보니 머신러닝에 적용시키는 경우 이 부분들이 함수의 기울기가 거의 0에 가까운 값이 되는데요 그러므로 러닝의 속도가 매우 느려집니다 그 이유는 gradient descent
(기울기 강하)를 도입하는 경우 기울기가 0이면 개체가 매우 느린 속도로 변하기 때문입니다 반면에, 액티베이션 함수로 바꾸는 경우엔 신경망이 여기 보이는 함수를 적용시켜 Relu(렐류)라고 하는 함수, rectified linear unit, R-E-L-U, 기울기는 모든 양수에 대해 1이되고 그렇기 때문에 기울기가 0으로 줄어드는 확률은 급격히 감소됩니다 그래서, 여기 이 부분에서 기울기는 값이 0이지만 시그모이드 함수에서 ReLU 함수로 변경하면서 gradient descent (기울기 강하)라는  알고리즘을 더 빨리 작동하는 알고리즘을 생성시켰습니다 이런 사례와 같이, 일종의 알고리즘 발명을 통해 결과적으로 알고리즘의 혁신을 일으키고 계산을 하는 과정에 큰 도움을 주었습니다 이제까지 이런 예시들이 꽤 많았는데요 알고리즘에 변형을 주어서 코딩이 더 빨리 작동할 수 있도록 환경을 제공해주고 결과적으로 더 큰 신경망을 트레이닝 시킬 수 있도록 해줍니다 진행하는 과정에 많은 양의 데이터로 이루어진 네트워크가 있어도 말이죠 빠른 계산이 중요한 또 다른 이유는 네트워크를 트레이닝 시키는 과정이 매우 반복적이고 자주 신경망 구조에 대한 발상이나 아이디어가 있는 경우 이러한 발상을 코드화시켜 그 발상을 바탕으로 실험을 하고 실험을 통해 신경망이 얼마나 성능을 발휘하는지 확인하고 다시 돌아가서 네트워크의 상세 내용을 바꾸고 이 원형 프로세스를 계속 반복합니다 새로운 네트워크가 트레이닝 시키는데 오래 시간이 소요되는 경우엔 이 원형 cycle이 진행되는 시간이 늘어나며 생산성은 큰 차이를 불러옵니다 효과적인 신경망을 만드는 환경이 변할 수 있는 것이죠.
아이디어를 기반으로 10분안에 작동하는 것을 볼 수도 있고,
하루가 걸릴 수도 있지만 반대로 한 달간 신경망을 트레이닝 시키는 경우도 발생할 수 있습니다 이유인 즉 슨, 시도해보고 10분안에 바로 결과가 나오거나,
하루 만에 결과가 나오거나 할 수 있기 때문입니다.
여러 가지 아이디어를 시도해보면서 본인의 네트워크를 통해 어플이 잘 작동하는지 확인할 수 있습니다 그러므로, 빠른 산출은 실험한 내용에 대한 결과를 빨리 확인하는데 큰 도움을 주었습니다 신경망 분야 종사자들이나 리서치 부문 담당자들에게도 도움을 주면서 반복업무를 더 빨리 가능케 하고 아이디어를 더 신속히 개선시킬 수 있게 되면서 전체적인 딥러닝 커뮤니티에게는 아주 요긴한 것임이 드러났습니다 새로운 알고리즘을 개발하는 것 멈추지 않고 지속적으로 발전을 시키는 부분에서 말이죠 이런 점들이 딥러닝을 급부상하게 만드는 것입니다 좋은 소식은 이런 강력한 요소들이 지속적으로 딥러닝을 더 좋게 만들고 있다는 것입니다 데이터를 예로 들겠습니다.
사회는 더 많은 데이터를 뱉어내고 있습니다 또는 산출 부분 특화된 하드웨어의 부상 GPU나 더 빠른 네트워킹 등의 다양한 하드웨어 말이죠 우리가 아주 큰 신경망을 만들 수 있는 능력이 산출하는 부분에 있어서 더욱 좋아질 것이라고 자신합니다 알고리즘을 말해보죠 리서치 커뮤니티는 계속해서 알고리즘 측면에서 봤을 때 혁신을 하는데 있어 환상적인 성과를 보이고 있고 긍정적인 시각을 유지할 수 있다고 봅니다 딥러닝은 계속해서 앞으로 몇 년 간 발전을 이룰 것 입니다 이제 마지막 비디오로 넘어가겠습니다 다음 비디오에선 이번 코스에서 어떤 것들을 배우는지 더 자세히 말씀 드리겠습니다
