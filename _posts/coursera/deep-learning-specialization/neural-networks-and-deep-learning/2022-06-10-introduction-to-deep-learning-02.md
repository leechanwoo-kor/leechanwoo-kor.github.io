---
title: "[Ⅰ. Neural Networks and Deep Learning] Introduction to Deep Learning (2)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Introduction to Deep Learning (2)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png)

<br><br>

# Introduction to Deep Learning

## Introduction to Deep Learning

### Why is Deep Learning taking off?

딥러닝의 기술적인 측면과 여러분의 딥러닝 네트워크 개념이 수십 년 동안 존재해왔다고 하면 왜 지금이 되어서야 뜨고 있는 것일까요? 이번 강의에서는 딥 러닝이 부상하게 된 주요 원인이 되는 내용을 다루어 볼 텐데요. 이런 내용을 통해 여러분들이 종사하고 있는 기업 내에서 가장 좋은 기회를 누리시고 실제로 적용하는 계기가 되실 겁니다. 지난 몇 년 동안 많은 사람들이 질문하여 왔습니다.

>"딥러닝이 갑자기 이렇게 잘되는 이유가 뭔가요?"

라고 말이죠, 이런 질문을 받을 때면 이런 그림을 그려주곤 하죠.


![image](https://user-images.githubusercontent.com/55765292/172992154-84f7dad3-9e4c-452a-ad06-d5f75165a00f.png){: .align-center}

가로 축에 작업에 대한 데이터 양을 표시하고, 세로 축에는 스팸 분류기 또는 광고 클릭 예측기의 정확성 또는 우리가 다른 자동차의 위치를 파악하기 위한 신경망의 정확성과 같은 관련 학습 알고리즘에 대한 성능을 표시하는 그림을 그렸다고 합시다.

서포트 벡터 머신이나 로지스틱 회귀 분석과 같은 전통적인 학습 알고리즘의 성능을 데이터 양의 함수로 표시하면 빨간색과 같은 곡선을 얻을 수 있습니다. 데이터를 더 많이 추가할수록 성능이 한동안 향상되지만 얼마 후 성능이 향상되지 않습니다. 엄청난 양의 데이터로 무엇을 해야 할지 몰랐다고 생각할 수도 있지만 지난 10년 동안 우리 사회에서 일어난 일은 우리가 상대적으로 적은 양의 데이터를 가지고 있는 것에서 꽤 많은 양의 데이터를 가지게 된 것으로 바뀌었다는 것입니다.

이러한 모든 현상이 사회가 디지털화되면서 가능해진 것입니다. 수많은 인간 활동에 있어 디지털 영역이 상당부분 차지하고 있는데요. 컴퓨터, 웹사이트, 모바일앱에서 많은 시간을 소비하고 디지털 장치에서 활동하면서 데이터를 생성하고 핸드폰 안에 장착되는 저렴한 카메라 가속도계 등 IoT의 센서들을 통해 더 많은 양의 데이터를 수집하게 되었습니다.  

우리는 계속해서 데이터를 수집해 왔습니다. 지난 20년 동안 많은 응용 프로그램에서 전통적인 학습 알고리즘이 효과적으로 활용할 수 있었던 것보다 훨씬더 많은 데이터를 축적했습니다. 그리고 새로운 네트워크가 밝혀낸 것은 작은 신경망을 훈련시키면 이 성능이 그렇게 보일 수 있다는 것입니다.

작은 신경망을 트레이닝 한다면 그 성능은 이렇게 보여질 수 있습니다. medium-sized 인터넷이라고 불리는 조금 더 큰 버전의 인터넷을 트레이닝 시키면 조금 더 나은 그래프가 파란색 곡선 처럼 나오는데요. 이것이 매우 큰 신경망을 트레이닝 한다면 그 형태 그대로 유지하면서 지속적으로 성능이 향상되는 경향이 있습니다.

자, 몇 가지 관찰을 해보죠. 먼저 매우 높은 수준의 성능을 얻으려면 두 가지가 필요합니다. 첫 번째는 방대한 양의 데이터를 활용하기 위해서는 충분히 큰 신경망을 훈련시킬 수 있어야 한다는 것입니다. 두 번째는 $x$축에 많은 데이터가 있어야 한다는 것입니다.

그래서 우리는 종종 규모가 딥러닝의 진보를 주도하고 있다고 말합니다. 규모란 단지 새로운 네트워크, 많은 숨겨진 단위, 많은 매개 변수, 많은 연결, 그리고 데이터의 규모를 의미합니다. 하지만 단지 규모를 개선시키는 것 만으로도, 이 도표를 좀 더 기술적으로 정확하게 만들기 위해, 그리고 제가 X축에 적은 양의 데이터를 몇 가지 더 추가하기위해, 우리는 실제로 학습의 세계에서 많은 길을 걸었습니다.

엄밀히 말하면, 이것은 라벨 데이터의 양입니다. 즉, 트레이닝 예제를 통해 입력 X와 라벨 Y를 모두 사용할 수 있습니다. 이 과정에서 나중에 사용할 약간의 표기법을 소개하겠습니다. 우리는 소문자 알파벳 m을 사용하여 내 훈련 세트의 크기나 훈련 예제의 수를 나타낼 것입니다. 이는 가로축에 표기될 것입니다.

해당 표 관련해서, 몇 개 더 이야기할 텐데요 더 작은 트레이닝 세트의 지도에서는 알고리즘의 상대적 순서가 잘 정의되어 있지 않기 때문에 트레이닝 세트가 많이 없다고 하면 본인 고유 스킬 엔지니어링 특성에 좌우할 수 있습니다. 그렇기 때문에 누군가가 SVM 을 트레이닝 시킬 경우 hand engineering을 쓰기 원한다면 본인이 더 큰 트레이닝 세트를 다루고 지금 것은 작은 트레이닝이라고 여기면 SEM이 더 효과적일 수 있습니다. 이 영역에서 왼쪽부분은 상대적 순서가 유전자 알고리즘끼리의 배열이 잘 정의되어 있지 않고, 그 성능은 개인의 엔진 특성 스킬에 따라 그리고 알고리즘의 모바일 정보에 따라 차이가 날 수 있습니다 그리고 매우 큰 훈련 세트, 오른쪽의 매우 큰 M 체제, 우리는 다른 접근 방식을 지배하는 큰 신경망을 보다 일관되게 볼 수 있다.


![image](https://user-images.githubusercontent.com/55765292/172992180-bfced034-f654-45a5-8445-efb41de1634c.png){: .align-center}


만약 여러분 친구가 신경망이 왜 뜨고 있는지 물어본다면, 저 같은 경우에 이 그림을 그릴 것 같습니다. 저는 딥러닝이 급부상하던 초기 시점에는 scaled data와 scale computation이 중심적인 역할을 했다고 봅니다. 아주 큰 신경망 네트워크를 매우 큰 신경망을 말이죠. CPU나 GPU에서 말이죠.

이를 통해 딥러닝 분야는 최근 몇 년간 많은 발전을 이루었고 아주 큰 알고리즘 혁신을 이루었습니다. 그래서 제가 과소평가하고 싶지 않은 부분은 알고리즘의 혁신 중 상당 부분은 신경망을 더 빨리 운영되게 만들고 이 중 가장 구체적인 예로 가장 큰 네트워크의 변화를 가지고 온 부분은 바로 시그모이드 함수에서 ReLU(렐류) 함수로 변경되었다는 점입니다.

이전 포스트에서 잠깐 다루었던 내용이죠. 제가 이야기하고 있는 내용이 잘 이해 가지 않으시면 너무 걱정하지 마십시오 시그모이드 함수의 문제는 나중에 알고 보니 머신러닝에 적용시키는 경우 이 부분들이 함수의 기울기가 거의 0에 가까운 값이 되는데요. 그러므로 학습의 속도가 매우 느려집니다. 그 이유는 gradient descent(기울기 강하)를 도입하는 경우 기울기가 0이면 개체가 매우 느린 속도로 변하기 때문입니다.

반면에, 활성화 함수로 바꾸는 경우엔 신경망이 여기 보이는 함수를 적용시켜 ReLU라고 하는 함수, rectified linear unit, R-E-L-U, 기울기는 모든 양수에 대해 1이되고 그렇기 때문에 기울기가 0으로 줄어드는 확률은 급격히 감소됩니다. 그래서, 여기 이 부분에서 기울기는 값이 0이지만 시그모이드 함수에서 ReLU 함수로 변경하면서 gradient descent (기울기 강하)라는 알고리즘을 더 빨리 작동하는 알고리즘을 생성시켰습니다.

이런 사례와 같이, 일종의 알고리즘 발명을 통해 결과적으로 알고리즘의 혁신을 일으키고 계산을 하는 과정에 큰 도움을 주었습니다. 이제까지 이런 예시들이 꽤 많았는데요 알고리즘에 변형을 주어서 코딩이 더 빨리 작동할 수 있도록 환경을 제공해주고 결과적으로 더 큰 신경망을 트레이닝 시킬 수 있도록 해줍니다. 진행하는 과정에 많은 양의 데이터로 이루어진 네트워크가 있어도 말이죠.

빠른 계산이 중요한 또 다른 이유는 네트워크를 트레이닝 시키는 과정이 매우 반복적이고 자주 신경망 구조에 대한 발상이나 아이디어가 있는 경우 이러한 발상을 코드화시켜 그 발상을 바탕으로 실험을 하고 실험을 통해 신경망이 얼마나 성능을 발휘하는지 확인하고 다시 돌아가서 네트워크의 상세 내용을 바꾸고 이 원형 프로세스를 계속 반복합니다.

새로운 네트워크가 트레이닝 시키는데 오래 시간이 소요되는 경우엔 이 원형 cycle이 진행되는 시간이 늘어나며 생산성은 큰 차이를 불러옵니다. 효과적인 신경망을 만드는 환경이 변할 수 있는 것이죠. 아이디어를 기반으로 10분안에 작동하는 것을 볼 수도 있고, 하루가 걸릴 수도 있지만 반대로 한 달간 신경망을 트레이닝 시키는 경우도 발생할 수 있습니다. 이유인 즉 슨, 시도해보고 10분안에 바로 결과가 나오거나, 하루 만에 결과가 나오거나 할 수 있기 때문입니다. 여러 가지 아이디어를 시도해보면서 본인의 네트워크를 통해 어플이 잘 작동하는지 확인할 수 있습니다. 그러므로, 빠른 산출은 실험한 내용에 대한 결과를 빨리 확인하는데 큰 도움을 주었습니다. 신경망 분야 종사자들이나 리서치 부문 담당자들에게도 도움을 주면서 반복업무를 더 빨리 가능케 하고 아이디어를 더 신속히 개선시킬 수 있게 되면서 전체적인 딥러닝 커뮤니티에게는 아주 요긴한 것임이 드러났습니다. 새로운 알고리즘을 개발하는 것 멈추지 않고 지속적으로 발전을 시키는 부분에서 말이죠. 이런 점들이 딥러닝을 급부상하게 만드는 것입니다.

좋은 소식은 이런 강력한 요소들이 지속적으로 딥러닝을 더 좋게 만들고 있다는 것입니다 데이터를 예로 들겠습니다. 사회는 더 많은 데이터를 뱉어내고 있습니다. 또는 산출 부분 특화된 하드웨어의 부상 GPU나 더 빠른 네트워킹 등의 다양한 하드웨어 말이죠. 우리가 아주 큰 신경망을 만들 수 있는 능력이 산출하는 부분에 있어서 더욱 좋아질 것이라고 자신합니다. 알고리즘을 말해보죠. 리서치 커뮤니티는 계속해서 알고리즘 측면에서 봤을 때 혁신을 하는데 있어 환상적인 성과를 보이고 있고 긍정적인 시각을 유지할 수 있다고 봅니다. 딥러닝은 계속해서 앞으로 몇 년 간 발전을 이룰 것 입니다.
