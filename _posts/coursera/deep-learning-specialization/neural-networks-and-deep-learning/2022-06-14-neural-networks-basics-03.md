---
title: "[Ⅰ. Neural Networks and Deep Learning] Neural Networks Basics (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Neural Networks Basics (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Logistic Regression as a Neural Network

### Gradient Descent
로지스틱 회귀 모델을 보았고, 단일 훈련 예제에서 얼마나 잘하고 있는지 측정하는 손실 함수를 보았습니다. 또한 매개변수 $w$와 $b$가 전체 학습 세트에서 얼마나 잘 수행되고 있는지 측정하는 비용 함수를 살펴보았습니다. 이제 기울기 하강 알고리즘을 사용하여 학습 세트의 매개변수 $w$를 학습하는 방법에 대해 살펴보겠습니다.
{: .notice--info}

![image](https://user-images.githubusercontent.com/55765292/173479957-73ddf003-0440-4d77-b0bf-3bf609935011.png){: .align-center}

정리하자면 첫 번째 줄에 익숙한 로지스틱 회귀 알고리즘이 있습니다. 두 번째 줄에는 비용 함수 $J$가 있는데 바로 매개변수 $w$와 $b$의 함수입니다. 그리고 평균은 m에 1번 이상 이 손실 함수가 있는 것으로 정의됩니다.

따라서 손실 함수는 알고리즘이 출력하는 정도를 측정합니다. 각 훈련 예제의 $\hat{y}^{(i)}$는 각 훈련 예제의 $y^{(i)}$와 비교해서 쌓아올립니다. 전체 공식은 오른쪽에 확장됩니다. 따라서 비용함수는 학습 세트에서 매개 변수 $w$와 $b$가 얼마나 잘 수행되는지 측정합니다.

따라서 매개변수 $w$와 $b$의 집합을 배우기 위해서는 $w$와 $b$를 찾고 싶어하는 것이 당연해 보입니다. 그것은 비용함수 $J(w,b)$를 가능한 작게 만드는 것입니다.

자, 기울기 하강의 예시를 보겠습니다. 이 도표에서 가로축은 실제 $w$에서 매개 변수 $w$와 $b$의 공간을 훨씬 더 높은 차원으로 나타낼 수 있지만, 도표화하여 $w$을 단일 수로, $b$를 단일 수로 설명하겠습니다.

$w,b$의 비용함수 $J$는 이 수평축 $w$와 $b$ 위의 어떤 표면 입니다. 따라서 표면의 높이는 특정 지점에서 $J,b$의 값을 나타냅니다. 그리고 우리가 정말로 원하는 것은 비용함수 $J$의 최소값에 해당하는 $w$와 $b$의 값을 찾는 것입니다.

이 특정 비용함수 $J$는 볼록함수라는 것이 밝혀졌습니다. 그래서 그것은 하나의 그릇 모양이라고 할 수 있는데요. 이것은 볼록 함수이며 이것은 볼록하지 않고 국소 최적을 갖는 이와 같은 함수와는 반대됩니다.

따라서 여기서 정의한 $w,b$의 비용함수 $J$가 볼록하다는 사실은 로지스틱 회귀에 이 특정 비용함수 $J$를 사용하는 큰 이유 중 하나입니다.

매개변수에 대한 좋은 값을 찾기 위해 $w$와 $b$를 초기화하면 그 초기값은 작은 빨간점으로 표시될 수 있습니다. 그리고 로지스틱 회귀의 경우 거의 모든 초기화 방법이 작동합니다. 일반적으로 0으로 초기화시키는데요. 랜덤 초기화도 가능하지만 사람들은 일반적으로 로지스틱 회귀에 대해 그렇게 하지 않습니다. 이 함수가 볼록함수이기 때문에 어느 지점에서 초기화하더라도 똑같은 지점에 도달하거나 거의 비슷한 지점에 도달할 것입니다.

그리고 기울기 하강은 초기 지점에서 시작하여 가장 가파른 내리막 방향으로 한 걸음 내딛는 것입니다. 그래서 기울기 하강 한 단계 후에, 여러분은 아마 거기에 도착할 겁니다. 왜냐하면 그것은 가장 가파른 하강 방향에서 한 단계 내리막길을 시도하거나 가능한 한 빨리 내려가는 방향으로 내리막길을 시도하기 때문입니다.

이것이 기울기 하강의 반복 중 하나입니다. 그리고 기울기 하강 과정을 반복한 후에 거기서 멈출 수도 있고 세 번 반복하는 등의 작업을 수행할 수도 있습니다.

저는 이것이 도표 뒤에 숨겨져 있지 않다고 생각합니다. 결국 여러분이 전역 최적점으로 모이거나 근접한 지점에 도달하기를 바랍니다. 따라서 이 그림은 기울기 하강 알고리즘을 나타내고 있습니다. 설명을 위해 조금 더 자세한 내용을 작성해 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/173480066-8d8361b0-982f-4ad7-8fae-4577cd3b390b.png){: .align-center}

최소화하고 싶은 함수 $J$가 있다고 가정해보죠. 이 함수는 그리기 쉽도록 이렇게 생겼을 수도 있습니다. 고차원 도표 대신 1차원 도표를 작성하기 위해 지금은 b를 무시하겠습니다. 기울기 하강을 이렇게 해서, 해당 업데이트를 반복적으로 진행할 예정입니다.

```
Repeat {
  w := w - α·dw
}
```

우리는 $w$값을 가지고 업데이트할 것입니다. 업데이트 $w$를 나타내기 위해 콜론표시를 사용합니다. $w$를 $w-\alpha\frac{dJ(w)}{dw}$. 그리고 우리는  알고리즘이 수렴될 때까지 그것을 반복합니다. 그래서 여기 표기법 알파의 몇 가지 요점은 학습 속도이며 한번에 얼마만큼 기울기 하강을 진행할 수 있는지 조절해줍니다. 학습 속도, 알파 및 두 번째 양을 선택하는 몇 가지 방법에 대해 나중에 이야기 할 것입니다.

$\frac{dJ(w)}{dw}$는 도함수입니다. 기본적으로 매개 변수 $w$에 대한 변경 사항의 업데이트이며 기울기 하강을 구현하기 위해 코드를 작성하기 시작할 때 코드에서 변수 이름 $dw$가 이 도함수를 나타내는 데 사용된다는 규칙을 사용할 것입니다. 그래서 이 코드를 작성할 때 `w := w - α·dw`로 적어볼 텐데요.

따라서 $dw$를 이 도함수로 나타내는 변수 이름으로 사용합니다.

자 이제 기울기 하강 업데이트가 말이 되는지를 확인해보겠습니다. $w$가 매우 큰 값에 위치하고 가정해 봅시다. 그래서 여러분은 해당 시점에 $w$ 비용 함수 $J$에 있습니다. 기억할 것은, 도함수 정의가 특정 지점에서 함수의 기울기라는 것입니다. 따라서 함수의 기울기는 실제로 높이를 삼각형의 아래쪽 너비로 나눈 값입니다. 그렇게 해서 도함수는 양수가 나옵니다.

$w$는 $w$에서 도함수를 뺀 학습률로 업데이트되며, 도함수는 양수이므로 결국 $w$에서 뺄셈을 하게 됩니다. 따라서 결국 왼쪽으로 한 걸은 나아가고 기울기 하강을 사용하여 알고리즘이 매개변수를 천천히 감소시킵니다. $w$를 큰 값으로 시작한 것이라면 말이죠.

다른 예로, 만약 $w$가 작은 값에 위치해 있었다면 해당 지점에서 기울기 또는 $\frac{dJ(w)}{dw}$는 음수가 됩니다. 그래서 그들은 알파 곱하기 음수로 빼는 것으로 업데이트를 보내려고 합니다. 그리고 서서히 $w$가 상승하게 됩니다. 따라서 계속되는 기울기 하강으로 $w$를 점점 더 크게 만들게 됩니다.

따라서 왼족에서 초기화하는지 오른쪽에서 진행하는지 여기에서 이 글로벌 최소값을 향해 중앙으로 이동하게됩니다.

그래서 우리는 $J(w)$에 대한 기울기 하강을 썼습니다. 이 $w$가 로지스틱 회귀의 매개변수에만 있는 경우 입니다. 비용 함수는 $w$ 및 $b$위의 함수입니다. 그 경우 기울기 하강의 내부 루프는 $w:=w-\alpha\frac{dJ(w)}{dw}$와 $b:=b-\alpha\frac{dJ(w)}{db}$입니다. 

다음으로 도함수에 대해 조금 더 이야기해 보도록 하겠습니다.

### Derivatives

![image](https://user-images.githubusercontent.com/55765292/173499226-da311404-ccdb-48e3-b704-3a4b36863bec.png){: .align-center}

여기에서 $f(a)$ 함수는 $3a$와 같습니다. 이것은 직선입니다. 도함수에 대한 직관을 얻기 위해서, 이 함수에 대한 몇 가지 사항을 살펴보겠습니다. $a=2$라고 합시다. 그런 경우에 $f(a)$는 $3$ x $a$ = $6$과 같습니다. 그래서 $a=2$라면, $f(a)=6$입니다.

자 그럼 $a$의 값을 아주 조금만 더 올린다고 해봅시다. 이제 조금 더 $a$를 올려서 $2.001$이 되도록 하겠습니다. 그래서 오른쪽으로 작은 이동을 주려고 합니다. 이제 $f(a)$는 그것의 x $3$과 같습니다. 그래서 $6.003$이고 이것을 도표에 표시합니다.

녹색으로 강조하고 있는 작은 삼각형을 보시면, 우리가 보는 것은 $0.001$만큼 우측으로 이동시키면, $f(a)$는 $0.003$만큼 올라갑니다. $f(a)$가 올라간 양은, 제가 오른쪽에 있는 것보다 3배나 많은 양이 되었습니다. 그래서 $a$가 기울기와 같거나 $2$일 때, $f(a)$ 함수의 기울기 또는 도함수는 3입니다. 또한 도함수라는 용어는 기본적으로 기울기라는 뜻입니다.

단지 도함수는 무섭고 위협적인 단어처럼 들리는 반면, 기울기는 도함수의 개념을 설명하는데 더 친숙한 방법입니다. 그래서 도함수를 들을 때마다 함수의 기울기를 생각하십시오.

더 공식적으로 기울기는 여기 보이는 높이를 녹색으로 표시된 이 작은 삼각형의 너비로 나눈 값으로 정의됩니다. 이것은 $0.003/0.001$이고, 기울기가 $3$이거나 도함수가 $3$이라는 사실은 단지 $a$를 $0.001$만큼 오른쪽으로 이동시킬 때 $f(a)의 값이 올라가는 양이 수평 방향으로 $a$를 살짝 이동한 양의 3배가 된다는 사실을 나타냅니다. 이 모든 것이 선의 기울기가 되는 것입니다.

자 그럼 이 함수를 조금 다른 관점에서 한번 보겠습니다. $a$가 이제 $5$라고 해봅시다. 그런 경우에는, $f(a)$는 $3$ x $a$ = $15$와 같습니다. 그럼 다시 한번 보죠. $a$를 오른쪽으로 약간 이동시킵니다. 이제 $a$의 값이 $5.001$만큼 이동 됐다고 해보겠습니다. $f(a)$는 $3$배 가까이 그 값이 증가하는 것을 볼 수있습니다. 따라서, $f(a) = 15.003$입니다.

이와 같이 다시 한 번 $a$를 오른쪽으로, $0.001$만큼 이동시키면, $f(a)$는 $3$배 가까이 그 값이 증가하는 것을 볼 수있습니다. 그래서 기울기는 다시 $a = 5$의 경우에도 $3$이 됩니다.

그래서 우리가 이걸 쓰는 방식은, 함수 $f$의 기울기는 $3$과 동일한거죠. 우리는 $\frac{df(a)}{da}$라고 하고 이것이 뜻하는 것은 함수 $f(a)$의 기울기를 의미합니다. 변수 $a$를 약간 움직일 때 아주 적은 양으로 이것은 3과 같습니다. 다른 방법으로 미분 공식을 쓰는 방법은 이와 같습니다. $\frac{d}{da}$ x $f(a)$. $f(a)$를 맨 위에 놓든 옆에 쓰던 상관없습니다.

하지만 모든 방정식이 의미하는 것은 $a$를 오른쪽으로 약간 이동시키면 $f(a)$가 작은 $a$의 값을 조금 움직인 것의 3배만큼 올라갈 것으로 예상한다는 겁니다.

신경망과 딥 러닝을 매우 효과적으로 적용하기 위해 변수 $a$를 $0.001$만큼 이동하면 무슨 일이 일어나는지에 대해 이야기해 보겠습니다. 여러분이 만약 도함수의 공식적인 수학적 정의를 알고 싶으시면 도함수는 $a$를 오른쪽으로 얼마나 움직이느냐에 따라 훨씬 더 작은 값으로 정의됩니다.

그래서 그것은 $0.001$이나 $0.00001$ 같은 값이 아닙니다. 그것은 훨씬 더 작은 값입니다. 그리고 미분의 공식적인 정의는 아주 작은 양만큼 오른쪽으로 움직일 때마다 기본적으로 무한히 작은 양이라고 말합니다. 만약 그렇게 하면 이 $f(a)$는 여러분이 오른쪽으로 살짝 밀었던 아주 작고 작은 양보다 3배나 더 올라갑니다.

이게 사실 도함수의 공식적인 정의입니다. 그러나 직관적인 이해를 위해 $a$를 소량의 $0.001$만큼 오른쪽으로 이동시키는 것에 대해 설명하겠습니다. 비록 $0.001$이 정확히 작은 것은 아니지만 아주 작은 극미량입니다.

이제 도함수의 한 가지 특성은 이 함수의 기울기를 어디에서 취하든 상관없이 $a$가 $2$이든 $a$가 $5$이든 $3$과 같다는 것입니다. 이 함수의 기울기는 $3$이며 $a$의 값이 무엇이든 간에 그것을 $0.001$으로 증가시키면 $f(a)$의 값은 3배로 올라간다는 것입니다.

그러므로 이 함수는 모든 곳에서 동일한 기울기를 갖는 것입니다. 그것을 볼 수 있는 한 가지 방법은 어디에서나 작은 삼각형을 그리는 것입니다. 높이를 너비로 나눈 것은, 항상 3:1의 비율을 가지고 있습니다.

함수의 기울기나 도함수가 직선에 대해 무엇을 의미하는지 알 수 있기를 바랍니다. 이 예에서 함수의 기울기는 모든 곳에서 3이었습니다. 다음 포스팅에서는 함수의 대한 기울기가 함수의 여러 지점에서 다를 수 있다는 조금 더 복잡한 예를 살펴보겠습니다.
