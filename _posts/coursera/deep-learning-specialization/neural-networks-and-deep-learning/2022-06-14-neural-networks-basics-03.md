---
title: "[Ⅰ. Neural Networks and Deep Learning] Neural Networks Basics (3)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
toc: true
toc_sticky: true
toc_label: "Neural Networks Basics (3)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/172768350-41a6b2f0-9468-4b13-bc94-4a38f89ce5e6.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Neural Networks Basics

## Logistic Regression as a Neural Network

### Gradient Descent
로지스틱 회귀 모델을 보았고, 단일 훈련 예제에서 얼마나 잘하고 있는지 측정하는 손실 함수를 보았습니다. 또한 매개변수 $w$와 $b$가 전체 학습 세트에서 얼마나 잘 수행되고 있는지 측정하는 비용 함수를 살펴보았습니다. 이제 기울기 하강 알고리즘을 사용하여 학습 세트의 매개변수 $w$를 학습하는 방법에 대해 살펴보겠습니다.
{: .notice--info}

![image](https://user-images.githubusercontent.com/55765292/173479957-73ddf003-0440-4d77-b0bf-3bf609935011.png){: .align-center}

정리하자면 첫 번째 줄에 익숙한 로지스틱 회귀 알고리즘이 있습니다. 두 번째 줄에는 비용 함수 $J$가 있는데 바로 매개변수 $w$와 $b$의 함수입니다. 그리고 평균은 m에 1번 이상 이 손실 함수가 있는 것으로 정의됩니다.

따라서 손실 함수는 알고리즘이 출력하는 정도를 측정합니다. 각 훈련 예제의 $\hat{y}^{(i)}$는 각 훈련 예제의 $y^{(i)}$와 비교해서 쌓아올립니다. 전체 공식은 오른쪽에 확장됩니다. 따라서 비용함수는 학습 세트에서 매개 변수 $w$와 $b$가 얼마나 잘 수행되는지 측정합니다.

따라서 매개변수 $w$와 $b$의 집합을 배우기 위해서는 $w$와 $b$를 찾고 싶어하는 것이 당연해 보입니다. 그것은 비용함수 $J(w,b)$를 가능한 작게 만드는 것입니다.

자, 기울기 하강의 예시를 보겠습니다. 이 도표에서 가로축은 실제 $w$에서 매개 변수 $w$와 $b$의 공간을 훨씬 더 높은 차원으로 나타낼 수 있지만, 도표화하여 $w$을 단일 수로, $b$를 단일 수로 설명하겠습니다.

$w,b$의 비용함수 $J$는 이 수평축 $w$와 $b$ 위의 어떤 표면 입니다. 따라서 표면의 높이는 특정 지점에서 $J,b$의 값을 나타냅니다. 그리고 우리가 정말로 원하는 것은 비용함수 $J$의 최소값에 해당하는 $w$와 $b$의 값을 찾는 것입니다.

이 특정 비용함수 $J$는 볼록함수라는 것이 밝혀졌습니다. 그래서 그것은 하나의 그릇 모양이라고 할 수 있는데요. 이것은 볼록 함수이며 이것은 볼록하지 않고 국소 최적을 갖는 이와 같은 함수와는 반대됩니다.

따라서 여기서 정의한 $w,b$의 비용함수 $J$가 볼록하다는 사실은 로지스틱 회귀에 이 특정 비용함수 $J$를 사용하는 큰 이유 중 하나입니다.

매개변수에 대한 좋은 값을 찾기 위해 $w$와 $b$를 초기화하면 그 초기값은 작은 빨간점으로 표시될 수 있습니다. 그리고 로지스틱 회귀의 경우 거의 모든 초기화 방법이 작동합니다. 일반적으로 0으로 초기화시키는데요. 랜덤 초기화도 가능하지만 사람들은 일반적으로 로지스틱 회귀에 대해 그렇게 하지 않습니다. 이 함수가 볼록함수이기 때문에 어느 지점에서 초기화하더라도 똑같은 지점에 도달하거나 거의 비슷한 지점에 도달할 것입니다.

그리고 기울기 하강은 초기 지점에서 시작하여 가장 가파른 내리막 방향으로 한 걸음 내딛는 것입니다. 그래서 기울기 하강 한 단계 후에, 여러분은 아마 거기에 도착할 겁니다. 왜냐하면 그것은 가장 가파른 하강 방향에서 한 단계 내리막길을 시도하거나 가능한 한 빨리 내려가는 방향으로 내리막길을 시도하기 때문입니다.

이것이 기울기 하강의 반복 중 하나입니다. 그리고 기울기 하강 과정을 반복한 후에 거기서 멈출 수도 있고 세 번 반복하는 등의 작업을 수행할 수도 있습니다.

저는 이것이 도표 뒤에 숨겨져 있지 않다고 생각합니다. 결국 여러분이 전역 최적점으로 모이거나 근접한 지점에 도달하기를 바랍니다. 따라서 이 그림은 기울기 하강 알고리즘을 나타내고 있습니다. 설명을 위해 조금 더 자세한 내용을 작성해 보겠습니다.

![image](https://user-images.githubusercontent.com/55765292/173480066-8d8361b0-982f-4ad7-8fae-4577cd3b390b.png){: .align-center}

최소화하고 싶은 함수 $J$가 있다고 가정해보죠. 이 함수는 그리기 쉽도록 이렇게 생겼을 수도 있습니다. 고차원 도표 대신 1차원 도표를 작성하기 위해 지금은 b를 무시하겠습니다. 기울기 하강을 이렇게 해서, 해당 업데이트를 반복적으로 진행할 예정입니다.

```
Repeat {
  w := w - α·dw
}
```

우리는 $w$값을 가지고 업데이트할 것입니다. 업데이트 $w$를 나타내기 위해 콜론표시를 사용합니다. $w$를 $w-\alpha\frac{dJ(w)}{dw}$. 그리고 우리는  알고리즘이 수렴될 때까지 그것을 반복합니다. 그래서 여기 표기법 알파의 몇 가지 요점은 학습 속도이며 한번에 얼마만큼 기울기 하강을 진행할 수 있는지 조절해줍니다. 학습 속도, 알파 및 두 번째 양을 선택하는 몇 가지 방법에 대해 나중에 이야기 할 것입니다.

$\frac{dJ(w)}{dw}$는 도함수입니다. 기본적으로 매개 변수 $w$에 대한 변경 사항의 업데이트이며 기울기 하강을 구현하기 위해 코드를 작성하기 시작할 때 코드에서 변수 이름 $dw$가 이 도함수를 나타내는 데 사용된다는 규칙을 사용할 것입니다. 그래서 이 코드를 작성할 때 `w := w - α·dw`로 적어볼 텐데요.

따라서 $dw$를 이 도함수로 나타내는 변수 이름으로 사용합니다.

자 이제 기울기 하강 업데이트가 말이 되는지를 확인해보겠습니다. $w$가 매우 큰 값에 위치하고 가정해 봅시다. 그래서 여러분은 해당 시점에 $w$ 비용 함수 $J$에 있습니다. 기억할 것은, 도함수 정의가 특정 지점에서 함수의 기울기라는 것입니다. 따라서 함수의 기울기는 실제로 높이를 삼각형의 아래쪽 너비로 나눈 값입니다. 그렇게 해서 도함수는 양수가 나옵니다.

$w$는 $w$에서 도함수를 뺀 학습률로 업데이트되며, 도함수는 양수이므로 결국 $w$에서 뺄셈을 하게 됩니다. 따라서 결국 왼쪽으로 한 걸은 나아가고 기울기 하강을 사용하여 알고리즘이 매개변수를 천천히 감소시킵니다. $w$를 큰 값으로 시작한 것이라면 말이죠.

다른 예로, 만약 $w$가 작은 값에 위치해 있었다면 해당 지점에서 기울기 또는 $\frac{dJ(w)}{dw}$는 음수가 됩니다. 그래서 그들은 알파 곱하기 음수로 빼는 것으로 업데이트를 보내려고 합니다. 그리고 서서히 $w$가 상승하게 됩니다. 따라서 계속되는 기울기 하강으로 $w$를 점점 더 크게 만들게 됩니다.

따라서 왼족에서 초기화하는지 오른쪽에서 진행하는지 여기에서 이 글로벌 최소값을 향해 중앙으로 이동하게됩니다.

그래서 우리는 $J(w)$에 대한 기울기 하강을 썼습니다. 이 $w$가 로지스틱 회귀의 매개변수에만 있는 경우 입니다. 비용 함수는 $w$ 및 $b$위의 함수입니다. 그 경우 기울기 하강의 내부 루프는 $w:=w-\alpha\frac{dJ(w)}{dw}$와 $b:=b-\alpha\frac{dJ(w)}{db}$입니다. 

다음으로 도함수에 대해 조금 더 이야기해 보도록 하겠습니다.

### Derivatives

![image](https://user-images.githubusercontent.com/55765292/173499226-da311404-ccdb-48e3-b704-3a4b36863bec.png){: .align-center}
