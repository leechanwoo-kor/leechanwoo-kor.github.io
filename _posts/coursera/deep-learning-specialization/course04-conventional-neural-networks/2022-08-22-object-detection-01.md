---
title: "[Ⅳ. Convolutional Neural Networks] Object Detection (1)"
categories:
  - Deep Learning Specialization
tags:
  - Coursera
  - Deep Learning Specialization
  - Deep Learning
  - Facial Recognition System
  - Convolutional Neural Network
  - Tensorflow
  - Object Detection and Segmentation
toc: true
toc_sticky: true
toc_label: "Object Detection (1)"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/183551502-3482e2d7-efb0-4815-9c94-b662606b4842.png){: width="50%" height="50%"}{: .align-center}

<br><br>

# Object Detection

## Detection Algorithms

### Object Localization

이번에는 객체 현지화에 대해 배우도록 하겠습니다. 객체 현지화란 컴퓨터 시각 분야 중 하나로서 불과 몇 년 전에 비해 급격히 증가하고 있으며, 훨씬 효과적으로 사용되고 있습니다. 객체 감지까지 작성하려면 먼저 객체 현지화에 대해 학습합니다. 우선 그 의미를 한 번 정의하며 시작해 봅시다.

#### What are localization and detection?

![image](https://user-images.githubusercontent.com/55765292/185896942-9ed2e872-87b9-481c-be5c-3cad82eb6ad5.png)

알고리즘이 이 그림을 보고 이것이 자동차라고 말할 수 있는 이미지 분류 작업은 이미 잘 알고 있습니다. 이것이 바로 분류입니다. 이 비디오에서 나중에 다룰 네트워크 구축에 대해 배우는 문제는 현지화에 의한 분류입니다.

즉, 자동차라고 라벨을 붙여야 할 뿐만 아니라 알고리즘은 경계 상자를 넣거나 이미지에서 자동차 위치 주위에 빨간색 직사각형을 그립니다. 이를 현지화 문제 분류라고 합니다.

여기서 현지화라는 용어는 사진 속에서 당신이 감지하고 있는 차가 어디에 있는지 알아내는 것을 의미합니다. 나중에 감지 문제에 대해 알아볼 텐데요. 그때에는 사진 속에 여러 개의 객체가 등장할 수도 있고 모두 감지하여 현지화해야 합니다.

만약 자율주행 애플리케이션에서 이 작업을 수행하는 경우 다른 차뿐만 아니라 다른 보행자, 오토바이 심지어 다른 물체도 감지해야 할 수 있습니다. 나중에 이것을 살펴보도록 하겠습니다.

이번에 사용하게 될 용어들 중 분류법과 위치측정문제 분류법은 대체적으로 하나의 객체를 다루고 있습니다. 보통 이미지 중앙에 있는 하나의 큰 객체를 인식하거나 또는 인식하고 현지화하려고 합니다.

이와는 반대로, 감지 문제에서는 다수의 객체가 존재할 수 있습니다. 실제로 하나의 이미지 내에 여러 카테고리들의 여러 객체 일 수도 있습니다.

따라서 이미지 분류에 대해 학습한 아이디어는 현지화 분류에 도움이 됩니다. 그리고 현지화를 위해 배운 아이디어가 감지에 도움이 될 것입니다. 우선 현지화와 분류에 대해 설명하겠습니다.

#### Classification with localization

![image](https://user-images.githubusercontent.com/55765292/185897534-8790cfea-173f-4ce6-a908-b4b82a072359.png)

여러분은 이미 이미지분류 문제에 익숙하실 겁니다. 여러 레이어를 가진 ConvNet에 사진을 입력 할 수 있고, 이것이 ConvNet입니다. 그리고 이것은 예측된 클래스를 출력하는 softmax 유닛 공급의 벡터 특성 생성이라는 결과를 가져옵니다.

그래서 여러분이 만약 자율주행 자동차를 만들고 있다면 객체 카테고리들은 다음과 같을 수 있습니다. 보행자, 자동차, 오토바이 또는 배경 등이죠. 이것은 위의 어느 것도 의미하지 않습니다.

예를 들어, 보행자도 없고, 차, 오토바이도 없다면, 여러분은 해당 출력 배경만 갖게 될 것입니다. 이 클래스는 4가지 출력이 가능한 softmax입니다. 이것은 표준 분류 파이프라인입니다. 이미지에서도 차를 현지화하고 싶다면 어떨까요?

이를 위해 신경망을 변경하여 경계 상자를 출력하는 출력 장치를 몇 개 더 가질 수 있습니다. 특히 신경망에서 4개의 숫자를 더 출력할 수 있고, 이것들을 $b_x, b_y, b_h, b_w$라고 부릅니다. 그리고 이 4개의 숫자는 감지 된 객체의 경계 상자를 파라미터화 했습니다.

그래서 이미지의 왼쪽 상단에 있는 표기법을 사용하려고 합니다. 좌표 (0,0)로 표시하겠습니다. 그리고 오른쪽 아래에는 (1,1)을 나타냅니다. 경계 상자를 지정하면 빨간색 직사각형에 중간 점을 지정해야 합니다. $b_x$는 이 경계상자의 $b_h$와 폭, $b_w$가 되는 높이와 함께 포인트 $b_x$는 높이와 함께 $b_x$와 높이입니다.

따라서 훈련 세트에 객체 클래스 라벨만이 포함되어 있지 않은 경우 $b_h$는 신경망이 예측하려고 하지만 4개의 추가 수치도 포함되어 있습니다.

경계 상자를 지정하면 지도 학습을 사용하여 알고리즘이 클래스 라벨뿐만 아니라 감지한 객체의 경계 상자 위치를 알려주는 4개의 파라미터도 출력할 수 있습니다.

따라서 이 예에서는 이상적인 $b_x$는 0.5 정도일 수 있는데요. 왜냐하면 이 $b_x$는 이미지의 오른쪽 절반 정도이기 때문입니다. $b_y$는 약 0.7이 될 수 있는데 이미지까지 70% 정도 내려갈 수 있기 때문입니다.

$b_h$는 약 0.3이 될 수 있는데요. 왜냐하면 이 빨간 사각형의 높이는 이미지 전체 높이의 약 30%이기 때문입니다. 또한 빨간색 상자의 폭은 전체 이미지 폭의 0.4 정도이기 때문에 $b_w$는 약 0.4 정도일 수 있습니다. 이제 이 문제의 타깃라벨 y를 지도 학습 태스크로 정의하는 방법에 대해 좀 더 구체적으로 설명하겠습니다.

#### Defining the target label y

![image](https://user-images.githubusercontent.com/55765292/185897710-55ba8c0c-59e3-43b6-b940-7efed58f7ec9.png)

상기시키기 위해 4개의 클래스가 있고 그리고 신경망은 분류라벨, 즉 분류라벨의 가능성뿐만 아니라 이 4개의 숫자를 출력합니다. 따라서 타겟 라벨 $y$를 다음과 같이 정의합니다.

첫 번째 컴포넌트 PC가 위치하는 벡터가 됩니다 객체가 있나요? 따라서 객체가 클래스 1, 2 또는 3 이 되면 $p_c$는 1이 될 것입니다. 이것이 배경 분류라면, 감지하고자 하는 객체가 아무것도 없다면, $p_c$는 0이 될 것입니다. 객체가 있을 가능성을 나타내는 것으로 $p_c$를 간주할 수도 있습니다.

즉, 여러분이 감지하려고 하는 분류 중 하나가 거기에 있다는 가능성입니다. 따라서 이것은 배경 분류 이외의 것을 가리키는 것이죠.

다음으로는, 만약 객체가 있다면 여러분은 $b_x, b_y, b_h$와 $b_w$, 즉 탐지했던 객체를 위해서 경계 상자를 출력하고 싶어할 것입니다. 그리고 최종적으로 객체가 있다면, 만약 $p_c$가 1이라면, $c_1, c_2$ 그리고 $c_3$를 출력값으로 입력해야 하며, 이들은 클래스 1, 클래스 2, 클래스 3을 의미합니다. 따라서 이는 보행자, 자동차 또는 오토바이가 되겠네요.

그리고 우리가 다루고 있는 문제에서 여러분의 이미지는 단지 하나의 객체만을 가지고 있다고 가정합시다. 따라서 대부분의 객체 중 하나가 그림에 나타납니다. 현지화 문제와 함께 이 분류에서 말이죠. 몇 가지 예제를 좀 더 살펴봅시다.

이것이 훈련 세트 이미지이고, 이것이 x이고, y는 $p_c$가 1이 되는 첫 번째 구성요소가 될 것입니다. 왜냐하면 객체 $b_x, b_y, b_h$와 $b_w$가 경계 상자를 지정할 테니까요. 따라서 라벨이 지정된 훈련 세트에는 라벨에 있는 경계 상자가 필요할 것입니다.

그리고 마지막으로 이건 자동차인데요. 클래스 2이죠. $c_1$은 보행자가 아니기 때문에 0일 것입니다. $c_2$는 자동차이므로 1이 되고, $c_3$는 오토바이가 아니므로 0이 될 것입니다. $c_1, c_2,$ 그리고 $c_3$ 중에서, 기껏해야 그 중 하나는 1이 될 것입니다. 이것은 이미지 안에 객체가 있다면 거죠. 이미지 안에 객체가 없다면 어떻게 될까요?

x가 두번째 그림과 같은 훈련 예시가 있다면 어떻게 될까요? 이런 경우에는, $p_c$는 0이 될 것이고, 이 나머지 요소들은 상관없음이 되며, 이 안에 물음표를 다 써보겠습니다. 이것은 상관없는 인데요. 왜냐하면 이 이미지 안에 객체가 없다면 세 개의 객체 $c_1, c_2, c_3$뿐만 아니라 네트워크 출력 값에 있는 경계 상자가 무엇인지 상관없기 때문입니다.

그래서 일련의 라벨 훈련 예제를 통해 객체가 있는 이미지와 없는 이미지 둘 다를 위해서 어떻게 비용 라벨 y뿐만 아니라 입력 이미지 x를 구성할지를 생각해 보는 것입니다. 그러면 이 세트가 여러분의 훈련 세트를 결정할 것입니다.

마지막으로 신경망을 훈련시키기 위해 사용하는 손실함수를 설명하겠습니다. 기본 참 레이블는 y였고, 신경망은 ŷ을 입니다. 그럼 손실은 무엇이 될까요? 만약 제곱오차를 사용하게 되면 손실은 ${(\hat{y}_1 - y_1)}^2$ 더하기 ${(\hat{y}_2 - y_2)}^2$ 더하기 이렇게 쭉 이어지다가 ${(\hat{y}_8 - y_8)}^2$의 제곱이 될 것입니다.

이처럼 y는 8개 구성 요소를 가지고 있습니다. 따라서 다른 요소들의 차이의 제곱의 총액이 되는 것입니다. 그리고 이것은 $y_1 = 1$이면 손실입니다. 그리고 객체가 있는 경우에 그렇습니다. 따라서, $y_1 = p_c$ 이미지에 객체가 있으면 $p_c = 1$ 이죠.

따라서 손실은 모든 차이나는 요소들의 제곱의 합이 되는 것입니다. 또 다른 경우는, 만약 $y_1 = 0$이라면 다시 말해, $p_c = 0$ 이죠. 이 경우, 손실은 ${(\hat{y}_1 - y_1)}^2$ 입니다. 왜냐하면 이 두 번째 경우에, 나머지 요소들 전부 상관없음인 불필요한 것이기 때문이죠.

신경 써야 하는 것은 $p_c$를 출력하는 데에 얼마나 정확하게 신경망이 작동하는지 입니다. 요약하자면, $y_1 = 1$, 바로 이 경우를 말하는 건데요, 이런 다음 제곱 오차를 사용하여 예측값과 8개 성분의 실제 출력에 대한 제곱 미분을 벌칙할 수 있습니다.

반면 $y_1 = 0$이면 두 번째에서 여덟 번째 성분은 상관없음 입니다. 따라서 여러분이 신경 써야 하는 것은 신경망이 $y_1$, 즉 $p_c$를 얼마나 정확하게 예측하는가 입니다. 자세한 내용을 알고 싶은 분들을 위해 설명을 간략화하기 위해 제곱 오류를 사용했습니다.

실제로는 softmax 출력에 대한 $c_1, c_2, c_3$의 특성 손실과 같은 로그를 사용할 수 있습니다. 이러한 요소 중 하나는 일반적으로 경계 상자 좌표에 제곱 오차 또는 제곱 오차 같은 것을 사용할 수 있으며, $p_c$의 경우 로지스틱 회귀 손실과 같은 것을 사용할 수 있습니다. 제곱 오차를 사용해도 정상적으로 동작할 수 있습니다.

---

이렇게 해서 신경망은 단순히 객체를 분류할 뿐만 아니라 현지화 할 수 있습니다. 신경망이 실수의 많은 수를 출력해 사진의 어디에 사물이 있는지 알려 주는 것은 매우 강력한 아이디어로 판명되었습니다. 다음에는 신경망을 통해 실수의 집합을 출력하는 아이디어를 컴퓨터 시각에서도 활용할 수 있습니다.


### Landmark Detection

신경망이 $b_x, b_y, b_h, b_w$의 4개의 번호를 출력하여 신경망이 현지화하는 객체의 경계 상자를 지정하는 방법을 설명했습니다. 좀 더 일반적인 경우에는 신경망은 중요한 점이나 이미지의 x와 y좌표를 출력할 수 있습니다. 이것들은 때때로 랜드마크라고 불리며 신경망이 인식하기를 바랍니다. 몇 가지 예제를 보시죠.

![image](https://user-images.githubusercontent.com/55765292/185921427-47eb4fbb-3895-46e6-ad85-ec32af4e1bd4.png)

얼굴인식 어플리케이션을 만들고 있는데 어떤 이유에서인지 알고리즘이 다른 사람의 눈꼬리를 알려주고 싶다고 가정해 봅시다. 이 지점이 x, y 좌표를 가지게 되죠. 신경망이 최종 레이어를 가지고 있고 2개의 숫자를 더 출력할 수 있는데요. 사람 눈꼬리 부분의 좌표를 알려주는 것을 $l_x$ 와 $l_y$ 라고 부르겠습니다.

자, 두 눈의 네꼬리를 전부를 알고 싶다면 어떻게 해야 할까요? 이 지점들을 왼쪽부터 오른쪽으로, 첫 번째, 두 번째, 세 번째, 네 번째 라고 한다면, 첫 번째 지점은 $l_{1x}$와 $l_{1y}$ 로, 두 번째 지점은 $l_{2x}$와 $l_{2y}$ 등으로 출력하도록 신경망을 수정할 수 있을 것입니다. 그러면 신경망은 사람 얼굴의 이 네 지점의 예상되는 위치를 출력 할 수 있겠죠. 하지만 네 가지 지점만 원하는 게 아니라면 어떨까요?

만약 이 눈을 따라서 여기, 여기, 여기, 여기 이 모든 지점을 출력하고 싶다면 어떻게 해야 할까요? 제가 입을 따라서 주요 지점들을 표시하면 여러분은 입 모양을 찾아내서 이 사람이 웃고 있는지 인상 쓰고 있는지 알수 있으며 아니면 코의 가장자리를 따라서 주요 지점들을 찾아낼 수 있겠지만 하지만 몇 가지 숫자를 정의할 수 있는데요.

우리의 논의를 위해서, 64개 지점, 즉 64개 랜드마크가 얼굴에 있다고 가정해보죠 얼굴의 가장자리를 정의하는 데 도움이 되는 일부 포인트도 턱선을 정의하지만, 여러 개의 랜드마크를 선택하고 이러한 랜드마크를 모두 포함하는 라벨 훈련 세트를 생성함으로써 신경망을 통해 얼굴의 주요 위치 또는 주요 랜드마크가 어디에 있는지 알 수 있습니다.

그래서 여러분이 하는 일은 이 이미지, 사람의 얼굴을 입력으로 만들고 컨볼네트를 거쳐서 이 컨볼네트가 특성 세트를 갖도록 하는겁니다. 아마 얼굴에 수정사항이 있는지 없는지에 따라 출력은 0이나 1이 될 것이고, 그것은 $l_{1x}, l_{1y}$ 등을 $l_{64x}, l_{64y}$까지 출력합니다. 랜드마크라고 표시하려고 저는 l을 사용하고 있습니다.

이 예시는 129개의 출력 유닛을 가지게 될 것이며 하나는 얼굴인지 아닌지에 대한 것이고 그리고 만약 64개 랜드마크가 있으면 64 x 2가 되므로 128 + 1 출력 유닛이 됩니다. 이것은 얼굴 자체뿐만 아니라 얼굴의 주요 랜드마크를 알려줄 수 있는 것이죠.

이것은 얼굴에서 감정을 인식하는 기본 구성 요소이며 만약 스냅챗이나 다른 엔터테인먼트를 가지고 놀면 Snapchat 사진 같은 AR 증강현실 필터도 얼굴에 왕관을 그리거나 다른 특수 효과를 낼 수 있습니다. 얼굴에 이러한 랜드마크를 감지할 수 있으려면 얼굴이 휘어지게 한다던가 사람에게 왕관이나 모자를 씌운다던 지 다양한 특수 효과를 그려낼 수 있는 컴퓨터 그래픽 효과의 핵심 구성 요소도 있습니다.

물론, 이렇게 네트워크를 다루기 위해서는 라벨 훈련 세트가 필요할 것입니다. 우리는 일련의 이미지들과 라벨 y를 가지고 있습니다. 여기엔 누군가 이 모든 랜드마크에 수고스럽게 주석을 달아 줄 것입니다.

마지막 예시인데요, 여러분이 사람의 포즈 감지에 관심이 있다면 가슴의 중간 지점, 왼쪽 어깨, 왼쪽 팔꿈치, 팔목 이런 부분 같은 몇 가지 핵심 위치들을 정의 내릴 수 있을 것입니다. 사람의 자세에서 중요한 위치에 주석을 달 수 있는 신경망만 있으면 되고 그리고 제가 주석을 달고 있는 이 지점들을 신경망이 출력하도록 시키세요. 신경망이 사람의 자세를 출력하도록 할 수도 있습니다.

그리고 물론 그렇게 하기 위해서는 여러분은 이 랜드마크를 지정할 필요가 있고, $l_1x$ 와 $l_1y$ 가슴의 중간 지점이고 이런 식으로 아래로 내려가면 32 좌표를 사용해서 사람의 포즈를 지정한다면, $l_{32x}$와 $l_{32y}$가 되겠지요.

이 아이디어는 단순히 출력 유닛을 여러 개 추가하여 인식하고자 하는 다른 랜드마크의 x,y 좌표를 출력하는 것으로 매우 간단해 보일 수 있습니다. 명확하게 말하자면, 랜드마크 1의 정체는 다른 이미지를 통틀어 일관성이 있어야만 합니다. 랜드마크 1은 언제나 눈꼬리 라던지 랜드마크 2는 언제나 이쪽 눈꼬리 라던지 랜드마크 3, 랜드마크 4, 이런 식으로 말이죠.

따라서 라벨은 여러 이미지 간에 일관성이 있어야 합니다. 하지만 레이블링 기계를 쓰거나 여러분 스스로 충분히 큰 데이터 세트를 레이블 한다면 신경망은 이러한 랜드마크를 모두 출력할 수 있습니다. 인물의 포즈나 사진 속 누군가의 감정을 인식하거나 하는 등의 흥미로운 효과를 얻을 수 있습니다. 랜드마크 감지는 여기까지입니다. 다음으로 이 구성 요소를 사용하여 객체 인식을 위해 쌓기 시작합시다.


### Object Detection

랜드마크 감지는 물론 객체 위치 측정에 대해서도 배웠습니다. 이제 다른 객체 감지 알고리즘을 구축해 봅시다. 이번에는 Sliding Windows Detection Algorithm(슬라이딩 윈도우 감지 알고리즘)을 사용하여 슬라이딩 윈도우 감지 알고리즘이라고 하는 것을 사용하여 객체 검출을 실시합니다.

#### Car detection example

![image](https://user-images.githubusercontent.com/55765292/185923547-89ac99de-2b86-4d12-a082-9df7d98ea4c9.png)

예를 들어 자동차 감지 알고리즘을 구축한다고 가정해 보겠습니다. 여러분이 할 수 있는 일은 이것입니다. 먼저 x와 y로 레이블 된 훈련 세트를 만들 수 있습니다. 그래서 x와 y는 자동차의 예를 잘라낸 것입니다. 이미지 x에는 긍정적인 예가 있습니다. 자동차가 있습니다. 여기도 자동차, 저기도 자동차기 있죠 여기엔 자동차가 없고, 여기도 자동차가 없습니다.

이 훈련 세트에서는 차의 이미지가 잘린 것부터 시작할 수 있습니다. 이는 x가 그냥 거의 자동차 라는 뜻이죠. 그래서 사진을 찍어서 자르고 자동차의 일부가 아닌 다른 것들은 오려낼 수 있습니다. 결국 전체 이미지의 중심에 차가 놓이게 됩니다.

이 라벨 트레이닝 세트를 지정하면 이 비디오에서는 ConvNet을 사용하여 수행하는 방법을 학습한 후 이 잘린 이미지 중 하나와 같습니다. 이미지를 입력하는 ConvNet을 훈련할 수 있습니다. 0 이나 1, 이것은 자동차 인가 아닌가를 말해주는 것이죠. ConvNet을 학습한 후에는 슬라이딩 윈도우 감지에서 사용할 수 있습니다.

#### Sliding window detection

![image](https://user-images.githubusercontent.com/55765292/185923586-fa4f481e-c1fd-49ca-b905-f9845b44a541.png)

그래서 그렇게 하는 방법은 이와 같은 테스트 이미지가 있는 경우 아래 표시된 특정 크기의 윈도우를 선택하는 것부터 시작합니다. 그리고 이 ConvNet에 작은 직사각형 영역을 입력합니다. 그러니 이 아래에 있는 빨간색 사각형을 가져다가 그것을 ConvNet에 입력합니다. ConvNet에 예측을 의뢰합니다.

추측컨대, 빨간색 사각형 안에 있는 작은 영역에는 차가 없는 작은 빨간 사각형이라고 적혀 있을 겁니다. 슬라이딩 윈도우 감지 알고리즘에서 여러분이 하는 일은 입력으로 전달되는 두 번째 이미지입니다. 이 빨간 사각형이 약간 위로 이동해서 ConvNet에 공급됩니다.

즉, 이미지의 영역만 입력합니다. ConvNet의 빨간색 사각형에 표시하고 ConvNet을 다시 실행합니다. 그리고 세 번째 이미지 등을 사용하여 이 작업을 수행합니다. 그리고 이미지 내의 모든 위치에서 윈도우를 슬라이드할 때까지 계속 진행합니다.

이 예에서는 애니메이션을 빠르게 진행하기 위해 큰 스트라이드를 사용하고 있습니다. 하지만 기본적으로는 이 크기의 모든 영역을 살펴보고 많은 작은 자른 이미지를 전송하여 각 위치에서 스트라이드 0 또는 1로 분류되도록 하는 것입니다.

이것을 실행해서 일단 완료하는 것을 이미지를 통과하는 슬라이딩 윈도우라고 부릅니다. 그럼 여러분은 이를 반복하면 되는데 이번에는 더 큰 윈도우을 사용하세요. 그래서 조금 더 큰 영역을 취해서 그 영역을 실행시키세요.

따라서 이 영역의 크기를 ConvNet이 예상하는 입력 크기로 조정하고 ConvNet에 공급하여 0 또는 1로 출력합니다. 그리고 스트라이드를 사용해서 다시 이 윈도우를 옆으로 밀어내시면 됩니다. 그리고 당신은 그것을 끝까지 이미지 전체에 걸쳐 실행합니다. 그리고 더 큰 윈도우를 사용하여 세 번째 작업을 수행할 수도 있습니다.

이렇게 하시면, 이미지 어딘가에 차가 있기만 하면 창문도 있고 예를 들어, 이 창을 ConvNet으로 전달하면 ConvNet이 해당 입력 영역에 대한 출력을 가질 수 있습니다. 그러면 여러분은 저기에 자동차가 있다는 것을 감지하게 되는 것이죠. 이 알고리즘은 **슬라이딩 윈도우 감지**라고 불려 집니다.

왜냐하면 이런 윈도우 이 정사각형들을 이 전체 이미지를 가로지르게 하고 자동차를 포함하고 있는지 아닌지 스트라이드를 가진 모든 사각 영역을 분류하기 때문입니다.

슬라이딩 윈도우 감지에는 컴퓨팅 비용이라는 큰 단점이 있습니다. 이미지에 있는 많은 사각 영역들을 잘라내고 ConvNet을 통해 각각 독립적으로 실행할 수 있습니다. 여러분이 아주 거친 스트라이드, 즉 아주 큰 스트라이드나 아주 큰 사이즈를 사용한다면, ConvNet을 통과하는 데 필요한 윈도우 수를 줄일 수 있습니다. 하지만 더 거친 입상도는 성능을 해칠 수 있습니다.

반면, 아주 고운 입상도, 즉 아주 작은 스트라이드를 사용한다면, 컨볼네트를 지나게 하는 많은 양의 이 모든 작은 영역은 ConvNet을 통과하는 것은 계산 비용이 매우 높다는 것을 의미합니다.

따라서, 신경망이 등장하기 전에는, 객체 감지를 수행하기 위해 핸드 엔지니어 특성을 통해 단순한 선형 분류기 같은 훨씬 단순한 분류기를 사용했습니다. 그 당시에는 각각의 분류기가 상대적으로 계산 비용이 저렴했기 때문에, 그것은 단지 선형 함수였습니다. 슬라이딩 윈도우 감지는 잘 작동했었습니다. 그리 나쁜 방법은 아니지만, 그러나 ConvNet을 통해 현재 하나의 분류 태스크가 실행 중이기 때문에 이 방법으로 하는 슬라이딩 윈도우는 실행불가능 하리만큼 느립니다. 매우 고운 입성도 즉 매우 작은 스트라이드를 사용하지 않는 한, 이미지 내에서 정확하게 객체 위치를 감지하는 것은 불가능합니다.

하지만 다행히도 계산 비용 문제는 좋은 해결책을 가지고 있습니다. 특히나 슬라이딩 윈도우 객체 감지는 컨볼루션적으로 훨씬 효과적으로 실행될 수 있습니다. 어떻게 하면 될지 다음에 봅시다.
