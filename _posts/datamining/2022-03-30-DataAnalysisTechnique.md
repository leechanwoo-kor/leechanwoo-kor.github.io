# 대표적인 데이터 분석 테크닉

## 1. Linear Regression
종속변수(Dependent variable) Y와 한 개 이상의 설명변수(Independent Variable) X(들)과의 선형 상관 관계를 모델링하는 회귀분석 기법이며 종속 변수가 수치형 변수일 때 사용합니다.

가장 기본적인 방법론이고 종속변수와 설명변수간의 관계를 함수로 설명하기 때문에 이들 간의 관계를 확인하기에 용이하지만, 종속변수와 설명변수들 간에 선형 관계만을 가정하기 때문에 정확성을 높이는 데는 한계가 존재합니다.

![image](https://user-images.githubusercontent.com/55765292/160788247-6bd70f0c-134e-4f51-8dc1-712bd0f8bbb8.png){: .align-center}

## 2. Logistic Regression
종속변수와 설명변수 간의 관계를 함수로 설명하려는 측면은 동일하나 종속변수가 범주형 변수일때 사용합니다. 분류 및 예측에 주로 사용되는 모델입니다. 너무 유명한 회귀분석이라 더이상의 설명은 생략합니다.

## 3. Ridge Regression
회귀분석의 설명변수의 개수가 증가할 경우 설명변수들 사이의 강한 상관관계로 인한 다중공선성이 존재하거나,  모델 자체가 지나치게 복잡해지는 문제가 발생합니다. 이럴 때 회귀계수 축소를 통해 보다 적은 변수를 사용해 모델을 간결하면서도 Robust하게 만들어 주는 리지 회귀분석을 사용할 수 있습니다. 리지 회귀분석은 Overfitting을 방지하면서 예측 정확도를 높이고 모델 해석력을 올려주는 장점이 있습니다.

## 4. Lasso Regression
변수 선택과 표준화 기능이 있는 회귀분석 방법입니다. Ridge Regression과 유사한 개념이지만, 영향력이 적은 회귀계수 값을 쉽게 0으로 만드는 변수 선택 기능이 있어 보다 Ridge Regression의 예측 정확도를 취하면서도 변수 선택에 있어서의 해석력을 확보한 분석 방법입니다.

## 5. Jackknife Regression
전통적인 회귀분석의 단점을 해결하기 위해 최근에 등장한 분석 방법입니다. 결과 변수가 서로 연관되어 있거나, 정규 분포가 아닐 때도 (예: Mode가 여러개 있다거나) 비교적 결과 예측력이 좋은 편입니다. 회귀분석의 기본 가정 (다중 공선성, 분포 등)이 위배되었을 때에도 사용할 수 있습니다.
(주의: Bradley Efron의 Jackknife resampling과는 다른 개념입니다.)

## 6. Decision Tree
대표적인 지도 학습 (Supervised Learning) 방법론이며, 수치형/범주형 종속 변수에 대한 예측/분류를 위해 사용합니다.  White box식 모델이기 때문에 결과를 보다 직관적으로 이해할 수 있다는 점, 계산 방법이 간단하다는 장점이 있지만 과적합화 등으로 인해 다른 모델에 비해 정확도가 낮은 편입니다.

![image](https://user-images.githubusercontent.com/55765292/160788521-f2ccae5a-bb98-4c20-8881-507dceb213c5.png){: .align-center}

## 7. Random Forest
이름에서 짐작할 수 있듯이 다수의 의사결정나무(Decision Tree)를 만든 후 최빈값을 기준으로 예측/분류하는 알고리즘입니다. Bagging/Bootstrap Aggregating 방법을 사용해 의사결정나무 노드 생성의 Bias를 줄이므로 의사결정나무의 과적합화 문제를 해결할 수 있는 대안으로 사용할 수 있습니다.

## 8. K-means Clustering
비지도 학습의 대표적인 분석 방법으로 주어진 데이터를 유사한 K개의 군집으로 묶는 알고리즘입니다. 군집을 나누는 방법에 따라 여러 종류로 구분됩니다. 데이터에 대한 이해 단계인 (EDA, exploratory data analysis)단계에서 부터 고객 세그멘테이션, 이미지 분할 등에 광범위하게 적용 가능합니다.

단, K값을 사전에 지정해 주어야 한다는 점, 이상치에 민감하게 반응하는 점 및 구형이 아닌 군집을 찾는데는 적절하지 않다는 단점이 있습니다.

![image](https://user-images.githubusercontent.com/55765292/160788641-9b77fbf4-4930-4688-a8ef-a0fc84f9c115.png){: .align-center}

## 9. Cross-Validation
예측/분류 모델의 과적합화 (Overfitting)를 방지하고, 보다 일반적인 Population에 적용 가능한지 여부를 확인하기 위해 사용되는 Sampling 방법의 한 유형입니다. 데이터를 여러 개의 구간으로 쪼개어 샘플링을 하며, 쪼개는 방법/샘플링 하는 방법에 따라 여러 종류로 구분됩니다. (k-fold , 2-fold, leave-p-out 등)

확보된 데이터(관측치)가 충분하지 않아 일반적으로 사용하는 Split (데이터의 70%는 train에, 30%는 test에) 방법을 적용하기 힘든 경우에 특히 유용합니다.

## 10. Artificial neural network
생물체의 신경망(중추 신경계, 뇌 등)의 뉴런이 시냅스를 통해 결합되는 것 처럼, 여러개의 함수의 집합과 각 집합에 대한 가중치를 조정하여 분류/예측하는 알고리즘입니다. 인공신경망 분석의 가장 큰 특징은 학습이 가능하다는 점입니다. 인공 신경망에서의 학습 기본 원리는 새로운 관측치가 발견될 경우 Cost function을 최소화 하는 함수를 찾아나가는 식으로 작동합니다.

신경망의 복잡도에 따라 결과를 얻기까지 걸리는 속도가 증가한다는 단점이 있지만, 변수 종류에 구애받지 않고, 비선형 조합이 가능하기 때문에 예측력이 우수하다는 점 (알파고도 인공신경망 기반의 알고리즘을 채택했습니다.) 때문에 주목받고 있습니다.

## 11. Ensemble Learning
같은 데이터 셋으로 여러개의 예측/분류 알고리즘을 만들고, 그 결과를 종합해서 보다 나은 결과값을 얻고자 하는 방법입니다. 주로 연산 속도가 빠른 알고리즘의 종합에 사용됩니다. (예: Decision Tree- Random Forest)

Netflix Prize에서 수상한 팀도 이 방법을 사용했다고 하죠? 여러 개의 알고리즘을 참고하는 만큼 정확도가 높아지지만, 연산 속도가 느린 단점이 있습니다. 실제로 Netflix에서도 1위 팀의 알고리즘이 추천 정확도는 더 높았지만 속도 문제로 알고리즘을 채택하지 않았다고 하죠.

## 12. Naive Bayesian Classifier
특성들 사이의 독립을 가정한 베이즈 정리를 적용한 분류 알고리즘입니다. 예를 들어 복면 가수가 아이돌일 확률 (능숙한 댄스,  적절한 노래 실력, 몸매/비율 등)이 각각 연관이 없다고 가정하고 각각의 특성이 복면 가수가 아이돌일 확률에 독립적으로 기여하는 것으로 봅니다.

나이브 베이즈는 전통적인 확률론에 기반한 알고리즘은 아니지만, 다른 진보된 분류 알고리즘과 비교했을 때도 매우 예측력이 높은 것으로 알려져 있습니다. 주로 문서의 텍스트를 확인하여 어떤 문서가 어떤 카테고리(스팸/비스팸)에 속하는지, 어떤 뉘앙스(지지/비판/중립 등)를 가지고 있는지 등을 판단하는 문제에 가장 자주 등장하는 알고리즘 입니다.

## 13. Collaborative filtering
사용자의 선호도 및 아이템의 특성 등을 사용해서 사용자가 선호할 아이템을 예측하는 추천 시스템에 주로 사용되는 방법입니다. 사용자, 아이템 중 어떤 것에 기반하느냐에 따라 여러 종류로 나뉩니다.

기반 사용자/아이템에 따라 매트릭스를 만들고 이에 따라 사용자의 선호를 추측하는 방법이기 때문에 사용자나 아이템이 늘어날 수록 계산이 복잡해지고, 속도도 느려지는 단점이 있습니다.

## 14. Principal Component Analysis (PCA)
데이터가 복잡해 질 수록 노이즈 및 계산 비용도 증가합니다. 주성분 분석 방법은 변수가 많은 고차원의 데이터를 정보 손실을 최소화 하면서 저차원의 데이터, 즉 변수 숫자가 작은 데이터로 압축시키는 방법입니다.

Regression이나 Decision Tree에서 설명 변수들 간 다중공선성 문제나, 군집 분석의 속도 개선 등 데이터 전처리에 주로 활용됩니다. 다만 주의해야 할 점은 분산을 기준으로 계산하기 때문에 변수의 단위에 영향을 받는 다는 점 (Z-score나 평준화 필요)과 데이터가 PCA의 주요 가정 (선형성, 직교성, 분산 차)를 만족하는 지 체크해야 한다는 점 정도가 있겠네요.

## 15. Support Vector Machine
두개의 주어진 Class 간 가장 가까운 거리를 가지는 Support Vector사이의 초평면을 찾아, 새로운 데이터에 Class를 부여하는 분류 방법입니다. 아래 그림을 보시면 이해가 쉽겠네요.

![image](https://user-images.githubusercontent.com/55765292/160788781-dfdd20a1-0e6c-41e2-a897-24c13b22583e.png){: .align-center}

주어진 데이터를 이진 분류해 내는 데에 있어 다른 알고리즘 보다 정확도가 뛰어나지만, 데이터셋의 크기나 복잡도에 따라 연산 속도가 느려진다는 단점이 있습니다. 주로 텍스트 분석(스팸/비스팸, 긍/부정 등), 손글씨 등의 이미지 인식 등에 주로 사용됩니다.

 

데이터 분석에 대해 공부하고 싶은데 어디부터 시작해야 할 지 모르거나, 어느 정도 공부 했지만 어떤 부분을 더 공부해야 할지 막혔던 분들은 본 포스팅의 데이터 분석 방법론 리스트를 보시면서 스터디 해 보시는 것도 도움이 되리라 생각합니다.



참고자료
- http://www.dodomira.com/2016/08/19/frequently_used_analyitic_method/
