---
title: "[Artificial Intelligence] ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptiron, MLP)"
categories:
  - Artificial Intelligence
tags:
  - Multi-Layer Perceptiron
  - MLP
toc: true
toc_sticky: true
toc_label: "ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptiron, MLP)"
toc_icon: "sticky-note"
---

# Multi-Layer Perceptiron, MLP

![image](https://user-images.githubusercontent.com/55765292/222048422-de682065-987e-4da1-8f63-8dc25552fa27.png)

<br>

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237057845-8ac3bb01-96bd-4534-b3bc-89f3e66dc8bf.png">

<br>

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237058048-42bf5672-3628-48f3-8a15-430e10ebce7d.png">

<br>

- MLP and backpropagation algorithm which is used to train it
- MLP used to describe any general feedforward (no recurrent connections) network

<br>

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237058979-4e59748a-2985-4689-b987-6776b2ede4d3.png">

- 3 layer (no. of layers of adaptive weights), 3ì¸µ ì‹ ê²½ë§
- 1st question:
  - ì€ë‹‰ì¸µì˜ ì—­í• ?
    - íŠ¹ì§•ì¶”ì¶œê¸°
  - ë‹¨ì¸µ ì‹ ê²½ë§ì˜ ë¬¸ì œì ?

<br>

## XOR problem

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237060353-2c54d785-e388-43c0-9b60-3cdddd6875e9.png">

- XOR (exclusive OR) problem
  - 0+0=0
  - 1+1=2=0 mod 2
  - 1+0=1
  - 0+1=1
- Perceptron does not work here
- Single layer generates a linear decision boundary

<br>

- Minsky & Papert (1969) offered solution to XOR problem by combining perceptron unit responses using a second layer of units

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237060588-5d222c35-3f7b-49db-b55d-333fcfc517e4.png">

- $out = hard lim(w_1 x_1 + w_2 x_2 - \theta)$

<br>

## Three Layer Networks

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237062453-122e32c5-0c69-4bcc-9a5b-0ff2a5a6d597.png">

<br>

## Properties of architecture

- ì¸µë‚´ì—ì„œ ì—°ê²°ì´ ì—†ìŒ
- ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì˜ ì§ì ‘ì—°ê²°ì´ ì—†ìŒ
- ì¸ì ‘ì¸µê°„ì—ëŠ” ì™„ì „ì—°ê²°
- ì¶œë ¥ unitê°œìˆ˜, ì…ë ¥ unit ê°œìˆ˜
- ì€ë‹‰ì¸µì˜ ìœ ë‹›ì˜ ê°œìˆ˜ëŠ” ì…ë ¥ìœ ë‹› í˜¹ì€ ì¶œë ¥ ìœ ë‹›ì˜ ê°œìˆ˜ë³´ë‹¤ ì‘ê±°ë‚˜, ë§ì„ìˆ˜ ìˆë‹¤. 

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237062944-6252bf04-c682-49d2-a755-c151bf0545fb.png">

<br>

## Decision Boundary

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237063010-653abad5-076f-46b8-852b-014ce3c51596.png">

- 2ì¸µì€ ì§€ì—­ì  ì§€ì‹ì„ ì¶”ì¶œí•˜ê³ , 3ì¸µì€ ì „ì—­ì ì¸ ì§€ì‹ì„ ì¶”ì¶œ
- ì€ë‹‰ì¸µì— sigmoidal êµ¬ë™í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´, 2ì¸µ ì‹ ê²½ë§ì€ ì–´ë–¤ í•¨ìˆ˜ë¼ë„ ê·¼ì‚¬í™” í•  ìˆ˜ ìˆìŒ. : Universal Approximation

<br>

# Backpropagation Learning 

- ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ê¸° ìœ„í•´, ì†ì‹¤í•¨ìˆ˜ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ì„ ì ìš© :
  - $\Delta w_{ji} = (t_j - y_j) x_i$
- ì…ë ¥ìœ ë‹› $i$ ì—ì„œ ì¶œë ¥ ìœ ë‹› $j$ë¡œì˜ ê°€ì¤‘ì¹˜ ($w_{ji}$) ì˜ ìˆ˜ì •ëŸ‰ì€ ì…ë ¥ê³¼ $j$ ì¶œë ¥ ìœ ë‹›ì˜ ì—ëŸ¬ì—ë§Œ ì˜ì¡´í•˜ëŠ” ì§€ì—­ì  íŠ¹ì„±

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/cb6948a6-6cb8-4c5b-b318-2f17a9416db7">

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/2596149a-df26-406e-aadd-47405690de82">

- ì—ëŸ¬ëŠ” 3ì§¸ì¸µì—ì„œë§Œ ê³„ì‚°ë¨. ì²«ì§¸ì¸µê³¼ 2ì§¸ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ”?
- ì²˜ìŒ 2ê°œì¸µì—ëŠ” ì§ì ‘ì ì¸ ì—ëŸ¬ê°€ ì—†ìŒ.

<br>

- Credit assignment problem
  - Problem of assigning â€˜creditâ€™ or â€˜blameâ€™ to individual elements involved in forming overall response of a learning system (hidden units)
  - In neural networks, problem relates to deciding which weights should be altered, by how much and in which direction

<br>

- ì´ˆê¸°ì¸µì˜ ê°€ì¤‘ì¹˜ê°€ ì¶œë ¥, ë”°ë¼ì„œ ì—ëŸ¬ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•´ì•¼í•¨
- ì¦‰, ê°€ì¤‘ì¹˜ wij ê°€ ì—ëŸ¬ì— ì–´ë–¤ ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ êµ¬í•˜ê³ ì í•¨. ë‹¤ìŒ ê°’ì„ êµ¬í•˜ê³ ì í•¨.

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/213981d8-5bb3-4a08-a99e-8530eda3d282">

<br>

- Backpropagation learning algorithm â€˜BPâ€™
- Solution to credit assignment problem in MLP. Rumelhart, Hinton and Williams (1986)

- BP ëŠ” ë‘ ë‹¨ê³„:
  - ìˆœë°©í–¥ íŒ¨ìŠ¤: ìˆœë°©í–¥ìœ¼ë¡œ ê° ìœ ë‹›ì˜ ì…ë ¥ê°’ ê³„ì‚°, ì¶œë ¥ ê³„ì‚°ì„ ë°˜ë³µí•˜ì—¬ ìµœì¢… ì¶œë ¥ê°’ êµ¬í•¨.
  - ì—­ë°©í–¥ íŒ¨ìŠ¤: ì¶œë ¥ ìœ ë‹›ì˜ ì—ëŸ¬ì‹ í˜¸ë¥¼ ê³„ì‚°í•œí›„ ì—­ë°©í–¥ìœ¼ë¡œ ì—ëŸ¬ë¥¼ ì „íŒŒí•¨. (ì—ëŸ¬ëŠ” ì‹¤ì œê°’ê³¼ ëª©í‘œì¹˜ì™€ì˜ ì°¨ì´)

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/cd3dbe2c-853d-4cfd-a46b-a4ac9b27b83c">

<br>

- 2ì¸µìœ¼ë¡œ ì„¤ëª…í•¨. ì‰½ê²Œ ë‹¤ì¸µìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•¨.

- $z_i(t) = g( \sum_j v_{ij}(t) x_j(t) )$ at time t
  - $= g ( u_i(t) )$
- $y_i (t) = g( \sum_j w_{ij}(t)z_j(t) )$ at time t
  - $= g ( a_i(t) )$
- a/u activation(total ì…ë ¥)
- g êµ¬ë™í•¨ìˆ˜(activation function)
- ë°”ì´ì–´ìŠ¤ëŠ” ì¶”ê°€ ê°€ì¤‘ì¹˜ë¡œ ë‘ 

## Forward pass

- Weights are fixed during forward and backward pass at time t

<img width="612" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/3a0a840f-fe2e-422c-a9cb-df24744db337">

<br>

<img width="205" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/88bd6623-5efa-49ea-a6fe-4d2d59a33962">

<br>

## Backward Pass

Will use a sum of squares error measure. For each training pattern we have:

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/5e7cd7be-127d-49c7-9ed1-16ecc13199ad">

<br>

where dk
is the target value for dimension k. We want to know how to modify weights in order to decrease E. **Use gradient descent** ie

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/0562782b-ee56-467f-926a-90caa085806f">

<br>

**both for hidden units and output units**

<br>

- The partial derivative can be rewritten as product of two terms using chain rule for partial differentiation

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/a2f0c134-7438-44b5-b135-efd25eb8def5">

**both for hidden units and output units**

- Term A
  - i ìœ ë‹›ì˜ ì „ì²´ì…ë ¥ ë³€í™”ì— ë”°ë¼ ì—ëŸ¬ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€? $\Delta i$
- Term B
  - ê°€ì¤‘ì¹˜ w ì˜ ë³€í™”ì— ë”°ë¼ i ìœ ë‹›ì˜ ì…ë ¥ì€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€? $z_j$

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c969982d-9c19-4cc8-aa47-fa421f5d0dfb">

<br>

# Activation Functions

- How does the activation function affect the changes?

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/b242d6fc-302f-4d36-bf80-907b1ab969a9">

- we need to compute the derivative of activation function g
- to find derivative the activation function must be smooth (differentiable)

<br>

- Sigmoidal (logistic) function-common in MLP

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/aea1c4c1-b446-40c5-9c16-85626cabf5d5">

<br>

- Derivative of sigmoidal function is

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/14d63025-096c-4023-b36d-8565bd64d79c">

- Derivative of sigmoidal function has max at ğ‘ğ‘–(ğ‘¡)= 0, is symmetric about this point falling to zero as sigmoid approaches extreme values

<br>

- ê°€ì¤‘ì¹˜ì˜ ìˆ˜ì •ê°’ì€ êµ¬ë™í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— ë¹„ë¡€í•¨.

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/32a7eb28-dde4-4de6-9534-adf91ed7e48e">

- ê°€ì¤‘ì¹˜ ìˆ˜ì •ì€ ìœ ë‹›ì˜ ì „ì²´ì…ë ¥ì´ ì¤‘ê°„ê°’(0ë¶€ê·¼)ì¼ ë•Œ ê°€ì¥ í¬ê²Œ ë°œìƒ. ì…ë ¥ì´ ì•„ì£¼ í¬ê±°ë‚˜,ì•„ì£¼ ì‘ì„ ë•Œì—ëŠ” ìˆ˜ì •ì¹˜ëŠ” ê±°ì˜ 0ì„. ìœ ë‹›ì´ í¬í™”ìƒíƒœ. â€¦ë‹¤ì¸µì¼ ë•Œ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œë©¸(gradient vanish)ë¬¸ì œ

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c1f198a6-14e5-4672-825c-bfc79d9e00a5">

<br>

# Batch Learning, Online learning

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/3d2b8a63-1b10-440b-ad57-930ab51b9903">

- Batch learning :
  - ê° í›ˆë ¨ë°ì´í„°ì— ëŒ€í•œ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ìˆ˜ì •ì¹˜ë¥¼ ëˆ„ì í•¨
  - ì „ì²´ í›ˆë ¨ë°ì´í„°ì— ëŒ€í•´ ëˆ„ì ëœ ê°’ì„ í•œë²ˆì— ìˆ˜ì •í•¨- â€“epoch
  - ìˆ˜ì • íšŸìˆ˜ ì ìŒ
- Online learning :
  - ê°€ì¤‘ì¹˜ë¥¼ ê° í›ˆë ¨ë°ì´í„°ë§ˆë‹¤ ìˆ˜ì •í•¨
  - ì†Œìš” ë©”ëª¨ë¦¬ ê³µê°„ ì ìŒ
  - í•™ìŠµë¥ ì€ ì‘ì€ ê°’ìœ¼ë¡œ ë‘ 
  - ë°ì´í„°ëŠ” ëœë¤í•œ ìˆœì„œë¡œ ì…ë ¥í•¨ - stochastic gradient descent
  - ëœë¤ìˆœì„œâ€¦ì§€ì—­ì  ìµœì†Œê°’ì„ íšŒí”¼í•˜ëŠ”ë° ë„ì›€

<br>

# Activation function of output unit

<img width="677" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/8bc4cb35-1e0e-434f-aba6-bcac58142699">

<br>

# Learning Example

- Once weight changes are computed for all units, weights are updated at the same time (bias included as weights here). An example:

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/602a2d06-3aef-4bb0-b3be-92f7c73783ae">

<br>

- Use identity activation function (ie g(a) = a)

<br>

- All biases set to 1. Will not draw them for clarity.
- Learning rate ï¨ = 0.1

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/82fb7ff4-7d60-4db3-8cf6-28ba9b3d7e73">

Have input [0 1] with target [1 0]. 

<br>
