---
title: "[Artificial Intelligence] ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptiron, MLP)"
categories:
  - Artificial Intelligence
tags:
  - Multi-Layer Perceptiron
  - MLP
toc: true
toc_sticky: true
toc_label: "ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptiron, MLP)"
toc_icon: "sticky-note"
---

# Multi-Layer Perceptiron, MLP

![image](https://user-images.githubusercontent.com/55765292/222048422-de682065-987e-4da1-8f63-8dc25552fa27.png)

<br>

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237057845-8ac3bb01-96bd-4534-b3bc-89f3e66dc8bf.png">

<br>

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237058048-42bf5672-3628-48f3-8a15-430e10ebce7d.png">

<br>

- MLP and backpropagation algorithm which is used to train it
- MLP used to describe any general feedforward (no recurrent connections) network

<br>

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237058979-4e59748a-2985-4689-b987-6776b2ede4d3.png">

- 3 layer (no. of layers of adaptive weights), 3ì¸µ ì‹ ê²½ë§
- 1st question:
  - ì€ë‹‰ì¸µì˜ ì—­í• ?
    - íŠ¹ì§•ì¶”ì¶œê¸°
  - ë‹¨ì¸µ ì‹ ê²½ë§ì˜ ë¬¸ì œì ?

<br>

## XOR problem

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237060353-2c54d785-e388-43c0-9b60-3cdddd6875e9.png">

- XOR (exclusive OR) problem
  - 0+0=0
  - 1+1=2=0 mod 2
  - 1+0=1
  - 0+1=1
- Perceptron does not work here
- Single layer generates a linear decision boundary

<br>

- Minsky & Papert (1969) offered solution to XOR problem by combining perceptron unit responses using a second layer of units

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237060588-5d222c35-3f7b-49db-b55d-333fcfc517e4.png">

- $out = hard lim(w_1 x_1 + w_2 x_2 - \theta)$

<br>

## Three Layer Networks

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237062453-122e32c5-0c69-4bcc-9a5b-0ff2a5a6d597.png">

<br>

## Properties of architecture

- ì¸µë‚´ì—ì„œ ì—°ê²°ì´ ì—†ìŒ
- ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì˜ ì§ì ‘ì—°ê²°ì´ ì—†ìŒ
- ì¸ì ‘ì¸µê°„ì—ëŠ” ì™„ì „ì—°ê²°
- ì¶œë ¥ unitê°œìˆ˜, ì…ë ¥ unit ê°œìˆ˜
- ì€ë‹‰ì¸µì˜ ìœ ë‹›ì˜ ê°œìˆ˜ëŠ” ì…ë ¥ìœ ë‹› í˜¹ì€ ì¶œë ¥ ìœ ë‹›ì˜ ê°œìˆ˜ë³´ë‹¤ ì‘ê±°ë‚˜, ë§ì„ìˆ˜ ìˆë‹¤. 

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237062944-6252bf04-c682-49d2-a755-c151bf0545fb.png">

<br>

## Decision Boundary

<img width="950" alt="image" src="https://user-images.githubusercontent.com/55765292/237063010-653abad5-076f-46b8-852b-014ce3c51596.png">

- 2ì¸µì€ ì§€ì—­ì  ì§€ì‹ì„ ì¶”ì¶œí•˜ê³ , 3ì¸µì€ ì „ì—­ì ì¸ ì§€ì‹ì„ ì¶”ì¶œ
- ì€ë‹‰ì¸µì— sigmoidal êµ¬ë™í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´, 2ì¸µ ì‹ ê²½ë§ì€ ì–´ë–¤ í•¨ìˆ˜ë¼ë„ ê·¼ì‚¬í™” í•  ìˆ˜ ìˆìŒ. : Universal Approximation

<br>

# Backpropagation Learning 

- ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ê¸° ìœ„í•´, ì†ì‹¤í•¨ìˆ˜ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ì„ ì ìš© :
  - $\Delta w_{ji} = (t_j - y_j) x_i$
- ì…ë ¥ìœ ë‹› $i$ ì—ì„œ ì¶œë ¥ ìœ ë‹› $j$ë¡œì˜ ê°€ì¤‘ì¹˜ ($w_{ji}$) ì˜ ìˆ˜ì •ëŸ‰ì€ ì…ë ¥ê³¼ $j$ ì¶œë ¥ ìœ ë‹›ì˜ ì—ëŸ¬ì—ë§Œ ì˜ì¡´í•˜ëŠ” ì§€ì—­ì  íŠ¹ì„±

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/cb6948a6-6cb8-4c5b-b318-2f17a9416db7">

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/2596149a-df26-406e-aadd-47405690de82">

- ì—ëŸ¬ëŠ” 3ì§¸ì¸µì—ì„œë§Œ ê³„ì‚°ë¨. ì²«ì§¸ì¸µê³¼ 2ì§¸ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ”?
- ì²˜ìŒ 2ê°œì¸µì—ëŠ” ì§ì ‘ì ì¸ ì—ëŸ¬ê°€ ì—†ìŒ.

<br>

- Credit assignment problem
  - Problem of assigning â€˜creditâ€™ or â€˜blameâ€™ to individual elements involved in forming overall response of a learning system (hidden units)
  - In neural networks, problem relates to deciding which weights should be altered, by how much and in which direction

<br>

- ì´ˆê¸°ì¸µì˜ ê°€ì¤‘ì¹˜ê°€ ì¶œë ¥, ë”°ë¼ì„œ ì—ëŸ¬ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•´ì•¼í•¨
- ì¦‰, ê°€ì¤‘ì¹˜ wij ê°€ ì—ëŸ¬ì— ì–´ë–¤ ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ êµ¬í•˜ê³ ì í•¨. ë‹¤ìŒ ê°’ì„ êµ¬í•˜ê³ ì í•¨.

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/213981d8-5bb3-4a08-a99e-8530eda3d282">

<br>

- Backpropagation learning algorithm â€˜BPâ€™
- Solution to credit assignment problem in MLP. Rumelhart, Hinton and Williams (1986)

- BP ëŠ” ë‘ ë‹¨ê³„:
  - ìˆœë°©í–¥ íŒ¨ìŠ¤: ìˆœë°©í–¥ìœ¼ë¡œ ê° ìœ ë‹›ì˜ ì…ë ¥ê°’ ê³„ì‚°, ì¶œë ¥ ê³„ì‚°ì„ ë°˜ë³µí•˜ì—¬ ìµœì¢… ì¶œë ¥ê°’ êµ¬í•¨.
  - ì—­ë°©í–¥ íŒ¨ìŠ¤: ì¶œë ¥ ìœ ë‹›ì˜ ì—ëŸ¬ì‹ í˜¸ë¥¼ ê³„ì‚°í•œí›„ ì—­ë°©í–¥ìœ¼ë¡œ ì—ëŸ¬ë¥¼ ì „íŒŒí•¨. (ì—ëŸ¬ëŠ” ì‹¤ì œê°’ê³¼ ëª©í‘œì¹˜ì™€ì˜ ì°¨ì´)

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/cd3dbe2c-853d-4cfd-a46b-a4ac9b27b83c">

<br>

- 2ì¸µìœ¼ë¡œ ì„¤ëª…í•¨. ì‰½ê²Œ ë‹¤ì¸µìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•¨.

- $z_i(t) = g( \sum_j v_{ij}(t) x_j(t) )$ at time t
  - $= g ( u_i(t) )$
- $y_i (t) = g( \sum_j w_{ij}(t)z_j(t) )$ at time t
  - $= g ( a_i(t) )$
- a/u activation(total ì…ë ¥)
- g êµ¬ë™í•¨ìˆ˜(activation function)
- ë°”ì´ì–´ìŠ¤ëŠ” ì¶”ê°€ ê°€ì¤‘ì¹˜ë¡œ ë‘ 

## Forward pass

- Weights are fixed during forward and backward pass at time t

<img width="612" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/3a0a840f-fe2e-422c-a9cb-df24744db337">

<br>

<img width="205" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/88bd6623-5efa-49ea-a6fe-4d2d59a33962">

<br>

## Backward Pass

Will use a sum of squares error measure. For each training pattern we have:

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/5e7cd7be-127d-49c7-9ed1-16ecc13199ad">

<br>

where dk
is the target value for dimension k. We want to know how to modify weights in order to decrease E. **Use gradient descent** ie

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/0562782b-ee56-467f-926a-90caa085806f">

<br>

**both for hidden units and output units**

<br>

- The partial derivative can be rewritten as product of two terms using chain rule for partial differentiation

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/a2f0c134-7438-44b5-b135-efd25eb8def5">

**both for hidden units and output units**

- Term A
  - i ìœ ë‹›ì˜ ì „ì²´ì…ë ¥ ë³€í™”ì— ë”°ë¼ ì—ëŸ¬ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€? $\Delta i$
- Term B
  - ê°€ì¤‘ì¹˜ w ì˜ ë³€í™”ì— ë”°ë¼ i ìœ ë‹›ì˜ ì…ë ¥ì€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€? $z_j$

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c969982d-9c19-4cc8-aa47-fa421f5d0dfb">

<br>

# Activation Functions

- How does the activation function affect the changes?

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/b242d6fc-302f-4d36-bf80-907b1ab969a9">

- we need to compute the derivative of activation function g
- to find derivative the activation function must be smooth (differentiable)

<br>

- Sigmoidal (logistic) function-common in MLP

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/aea1c4c1-b446-40c5-9c16-85626cabf5d5">

<br>

- Derivative of sigmoidal function is

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/14d63025-096c-4023-b36d-8565bd64d79c">

- Derivative of sigmoidal function has max at ğ‘ğ‘–(ğ‘¡)= 0, is symmetric about this point falling to zero as sigmoid approaches extreme values

<br>

- ê°€ì¤‘ì¹˜ì˜ ìˆ˜ì •ê°’ì€ êµ¬ë™í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— ë¹„ë¡€í•¨.

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/32a7eb28-dde4-4de6-9534-adf91ed7e48e">

- ê°€ì¤‘ì¹˜ ìˆ˜ì •ì€ ìœ ë‹›ì˜ ì „ì²´ì…ë ¥ì´ ì¤‘ê°„ê°’(0ë¶€ê·¼)ì¼ ë•Œ ê°€ì¥ í¬ê²Œ ë°œìƒ. ì…ë ¥ì´ ì•„ì£¼ í¬ê±°ë‚˜,ì•„ì£¼ ì‘ì„ ë•Œì—ëŠ” ìˆ˜ì •ì¹˜ëŠ” ê±°ì˜ 0ì„. ìœ ë‹›ì´ í¬í™”ìƒíƒœ. â€¦ë‹¤ì¸µì¼ ë•Œ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œë©¸(gradient vanish)ë¬¸ì œ

<br>

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c1f198a6-14e5-4672-825c-bfc79d9e00a5">

<br>

# Batch Learning, Online learning

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/3d2b8a63-1b10-440b-ad57-930ab51b9903">

- Batch learning :
  - ê° í›ˆë ¨ë°ì´í„°ì— ëŒ€í•œ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ìˆ˜ì •ì¹˜ë¥¼ ëˆ„ì í•¨
  - ì „ì²´ í›ˆë ¨ë°ì´í„°ì— ëŒ€í•´ ëˆ„ì ëœ ê°’ì„ í•œë²ˆì— ìˆ˜ì •í•¨- â€“epoch
  - ìˆ˜ì • íšŸìˆ˜ ì ìŒ
- Online learning :
  - ê°€ì¤‘ì¹˜ë¥¼ ê° í›ˆë ¨ë°ì´í„°ë§ˆë‹¤ ìˆ˜ì •í•¨
  - ì†Œìš” ë©”ëª¨ë¦¬ ê³µê°„ ì ìŒ
  - í•™ìŠµë¥ ì€ ì‘ì€ ê°’ìœ¼ë¡œ ë‘ 
  - ë°ì´í„°ëŠ” ëœë¤í•œ ìˆœì„œë¡œ ì…ë ¥í•¨ - stochastic gradient descent
  - ëœë¤ìˆœì„œâ€¦ì§€ì—­ì  ìµœì†Œê°’ì„ íšŒí”¼í•˜ëŠ”ë° ë„ì›€

<br>

# Activation function of output unit

<img width="677" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/8bc4cb35-1e0e-434f-aba6-bcac58142699">

<br>

# Learning Example

- Once weight changes are computed for all units, weights are updated at the same time (bias included as weights here). An example:

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/602a2d06-3aef-4bb0-b3be-92f7c73783ae">

<br>

- Use identity activation function (ie g(a) = a)

<br>

- All biases set to 1. Will not draw them for clarity.
- Learning rate $\eta$ = 0.1

<img width="955" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/82fb7ff4-7d60-4db3-8cf6-28ba9b3d7e73">

Have input [0 1] with target [1 0]. 

<br>

- Forward pass. Calculate $1^{st}$ layer activations:

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/fd5b5b92-3362-4cb4-9284-6242a104c4eb">

- $u_1 = -1 \times 0 + 0 \times 1 + 1 = 1$
- $u_2 = 0 \times 0 + 1 \times 1 + 1 = 2$

<br>

- Calculate first layer outputs by passing activations thru activation functions

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c8a0d84b-6887-44c2-93b2-a7cf19092d52">

- $z_1 = g(u_1) = 1$
- $z_2 = g(u_2) = 2$

<br>

- Calculate $2^{nd}$ layer outputs (weighted sum thru activation functions):

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/00a8d85b-3370-49d2-bf47-c063614c964a">

- $y_1 = a_1 = 1 \times 1 + 0 \times 2 + 1 = 2$
- $y_2 = a_2 = -1 \times 1 + 1 \times 2 + 1 = 2$

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/169fab27-830f-4f60-9027-1c311305dfc3">

- So
  - $\Delta_1 = (d_1 - y_1) = 1 â€“ 2 = -1$
  - $\Delta_2 = (d_2 - y_2) = 0 â€“ 2 = -2$

<br>

- Calculate weight changes for $1^{st}$ layer (cf perceptron learning):

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/fad5188d-33c6-48d3-946f-7e041ba23a74">

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/a4770f37-092e-4e4c-8f55-1a420dcb5b9c">

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c69a17df-3293-4e51-9bab-8e26af3205a0">

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/125d70b2-3238-4e75-a02d-89b2535074fd">

- $\delta_1 = - 1 + 2 = 1$
- $\delta_2 = 0 â€“ 2 = -2$

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/47c5e7c8-74b6-4d57-a05d-33284595b755">

<br>

Finally change weights:

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/26403f23-2092-417b-8067-79618db509f5">

- Note that the weights multiplied by the zero input are unchanged as they do not contribute to the error We have also changed biases (not shown)

<br>

- Now go forward again (would normally use a new input vector):

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/8d13dab1-3081-4e4d-af17-ffff4c4e8f9f">

<br>

- Now go forward again (would normally use a new input vector):

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/d03a084b-3323-4402-9c9e-d8b903859ffb">

- Outputs now closer to target value [1, 0]

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/f1a2f172-f575-4ffb-9b0a-7c353767132d">

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/e7c48cad-f97f-40be-8267-13e6b8c72a13">

<br>

## Selecting initial weight values 

- Choice of initial weight values is important as this decides starting position in weight space. That is, how far away from global minimum
- Select weight values randomly from uniform probability distribution

<br>

## Momentum

- ìˆ˜ë ´ì†ë„ë¥¼ ë†’ì´ë©´ì„œ ë¶ˆì•ˆì •ì„±ì„ ì¤„ì´ëŠ” ê¸°ë²•
- ê°€ì¤‘ì¹˜ ê°±ì‹ ì‹ì— ì´ì „ ê°€ì¤‘ì¹˜ ìˆ˜ì •ì¹˜ì˜ ë¹„ë¡€ê°’ì„ ë”í•¨
- ìˆ˜ì •ëœ ê°€ì¤‘ì¹˜ ê°±ì‹ ì‹

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/d5741d07-60f7-4b0d-8c93-828f159d33bb">

<br>

- $\alpha$ is momentum constant and controls how much notice is taken of recent history

- Effect of momentum term
  - ê°€ì¤‘ì¹˜ì˜ ìˆ˜ì •ì¹˜ê°€ ì´ì „ ìˆ˜ì •ì¹˜ì™€ ê°™ì€ ë¶€í˜¸ë¥¼ ê°€ì§€ë©´, ê´€ì„±í•­ì€ ìˆ˜ì •ì„ ë§ì´ í•˜ì—¬ ìˆ˜ë ´ì†ë„ë¥¼ ë†’ì„.
  - ë§Œì•½ ìˆ˜ì •ì¹˜ê°€ ì´ì „ì˜ ìˆ˜ì •ì¹˜ì™€ ë°˜ëŒ€ë¶€í˜¸ë¥¼ ê°€ì§€ë©´, ê´€ì„±í•­ì€ ìˆ˜ì •ì¹˜ë¥¼ ì¤„ì—¬ì„œ ì†ë„ë¥¼ ëŠ¦ì¶”ì–´ ì§„ë™ì„ ë°©ì§€í•œë‹¤. (ì•ˆì •í™”)
  - ì§€ì—­ì  ìµœì†Œì¹˜ë¥¼ íšŒí”¼í•˜ë„ë¡ ë„ì›€

<br>

## Adaptive Learning Rate

- Learning rate $\eta$
  - mostly less than or equal to 0.2
  - can be made adaptive for faster convergence
  - kept large when learning takes place and decreased when learning slows down

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/c4570d9b-392c-40c4-8c03-6aa461d5cb40">

<br>

## Overtraining

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/7114d7d0-a2ff-4e84-8434-59b16294711f">

- where
  -  n = number of training patterns,
  -  m = number of output units

- Could stop training when rate of change of E is small, suggesting convergence

- However, aim is for new patterns to be classified correctly 

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/b0e09844-dcd6-4205-9a6b-8f2fb2d3256e">

<br>

- Typically, though error on training set will decrease as training continues generalisation error (error on unseen data) hitts a minimum then increases (cf model complexity etc)
- Therefore want more complex stopping criterion

<br>

- Cross-validation
  - Method for evaluating generalisation performance of networks in order to determine which is best using of available data
- Hold-out method
  - Simplest method when data is not scare
- Divide available data into sets
  - Training data set
    - used to obtain weight and bias values during network training
  - Validation data
    - used to periodically test ability of network to generalize
    - > suggest â€˜bestâ€™ network based on smallest error

- Cf . Test data

<br>

<img width="950" alt="image" src="https://github.com/leechanwoo-kor/leechanwoo-kor.github.io/assets/55765292/6c935a90-5076-4d0e-ac2a-fcadec171c78">

<br>

## Weka

Choose classify-functions-multilayer perceptron
1. hidden layer ì˜ ë…¸ë“œ ìˆ˜ì˜ ì„¤ì •ë°©ë²•
popup ì°½ì˜ choose ë‚œì˜ multilayer perceptronì„ í´ë¦­
hiddenLayers ì— ì€ë‹‰ì¸µì˜ ë…¸ë“œìˆ˜ ì…ë ¥
ì˜ˆ 2,4 ì…ë ¥
: 1ì§¸ ì€ë‹‰ì¸µì˜ ë…¸ë“œìˆ˜ 2ê°œ
2ì§¸ ì€ë‹‰ì¸µì˜ ë…¸ë“œìˆ˜ 4ê°œ
2. choose ë‚œì˜ multilayer perceptronì„ í´ë¦­
popup ì°½ì˜ GUI ë‚œì„ true ë¡œ í•˜ë©´ MLP êµ¬ì¡°ê°€ ë³´ì„

## MLP regressor (cpu performance)

## MLP classifier(iris)



