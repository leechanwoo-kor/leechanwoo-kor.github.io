---
title: "[NLP] Word2Vec"
categories:
  - NLP
tags:
  - NLP
  - Word2Vec
toc: true
toc_sticky: true
toc_label: "Word2Vec"
toc_icon: "sticky-note"
---

![image](https://user-images.githubusercontent.com/55765292/196322694-37aa45dd-c985-4514-8ba4-dd447a6486a6.png)

<br>

# Word2Vec

<br>

##  Word2Vec
- Efficient & effective tool for learning word representations from corpus

![image](https://user-images.githubusercontent.com/55765292/199245997-ca700ca2-164f-4bb8-afe6-ef437c5f19fc.png)

![image](https://user-images.githubusercontent.com/55765292/199246056-d303b1f1-91a4-4688-a141-9f2bbed7f682.png)

<br>

---

- 텍스트 말뭉치를 받아들이고 각 단어에 대한 벡터 표현을 출력하는 딥러닝 알고리즘

![image](https://user-images.githubusercontent.com/55765292/199246227-2445da2c-3e67-4c4a-a083-f56e0fc2c048.png)

- 이론적 근거 --> 분포 가설(Distributed Hypothesis) (by John Firth, 1880-1960)
  - 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다!!
    - 어떤 글에서 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단하는 방법
    - '나는 식사를 하기 전에 반드시 ___을 씻는다'

- 유형
  - CBOW(Continuous Bag of Words) 모델
  - Skip-gram 모델

<br>

---

- CBOW --> 주변 단어들을 가지고 중간에 있는 단어를 예측하는 방법
- Skipgram --> 중간에 있는 단어를 가지고 주변 단어들을 예측하는 방법

![image](https://user-images.githubusercontent.com/55765292/199247003-44b24b29-fd37-4199-8494-abab08793d6c.png)

<br>

## CBOW 모델

![image](https://user-images.githubusercontent.com/55765292/199247119-d88c48fc-791e-4e3b-bb28-c0616f4e3872.png)

- V 차원의 one-hot 벡터로 표현된 단어를 N 차원의 실수 벡터로 바꿈
  - V : 사전에 포함된 전체 단어 개수
  - N : 단어를 표현하는 실수 벡터의 차원
- 학습방법
  - 입력에 주변 단어들이 주어질 때 , 출력에서는 타겟 단어가 나타날 확률이 높아지도록 학습
  - C개의 단어가 입력으로 들어가서 하나의 타겟 단어가 출력으로 나옴 (C 는 보통 10 개정도
  - 은닉층 --> 출력 단어를 표현하는 실수 벡터 를 학습하는 역할을 함
- 학습 데이터
  - {주변단어 1, 주변단어 2, ... 주변단어 C, 타겟 단어}

<br>

---

- 입력층과 은닉층의 가중치 : W
- 은닉층과 출력층의 가중치 : U
- 은닉층의 값

$h=\dfrac{1}{C}\sum\limits^{C}_{i=1}Wx_i$

- 출력층의 값
  - 가중치 U와 h를 곱한 다음 softmax를 하여 계산함
  - j번째 단어에 속할 확률값

$y_j=\dfrac{exp(u_j\cdot h)}{\sum\limits^V_{i=1}exp(u_j\cdot h)}$

<br>

---

- 학습목표
  - 주변 단어들이 입력으로 주어질 때, 타겟 단어로 출력될 확률이 최대가 되도록 학습
- 학습을 위한 목적함수를 음의 로그가능도로 정의할 수 있음

$E(W,U)=-\log p(t=h\mid x_1,\cdots,x_{i-1},x_{i+1},\cdots,x_C)$

- 학습시간
- 입력층~은닉층에서 $C \times N$ 에 비례하는 시간
- 은닉층~출력층에서 $N \times V$ 에 비례하는 시간
- 학습 데이터 하나에 대한 학습 비용 --> $C \times N + N \times V$에 비례함

- 단어 개수가 많은 경우 , 출력노드 개수가 단어 개수와 같으므로 softmax 계산하는데 시간이 오래 걸림

<br>

## Skip-gram 모델

- V 차원의 one hot 벡터로 표현된 단어를 N 차원의 실수 벡터로 바꿈
- 학습방법
  - 입력에 타겟 단어가 주어질 때 , 출력에서는 타겟 단어의 주변 단어들이 나타날 확률이 높아 지도록 학습
- 학습 데이터
  - {타겟 단어 , 주변단어 1}, { 타겟 단어 , 주변단어 2}, {타겟 단어 , 주변단어 3}, ..., { 타겟 단어 , 주변단어 C} 의 C 개의 쌍 --> 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW 보다 좋음

<br>

---

- 입력층과 은닉층의 가중치 : W
- 은닉층과 출력층의 가중치 : U
- 은닉층의 값

$h = Wt$

- 출력층의 값
  - 가중치 U 와 h 를 곱한 다음 softmax 를 하여 계산함
  - t가 입력으로 주어질 때 , $x_j$ 가 $j^* $ 번째 단어일 확률

$y_{j^* } = p(x_j = j^* 번째 단어 \mid t) = \dfrac{exp(u_{j^* } \cdot h)}{\sum\limits^V_{j=1}exp(u_j \cdot h)}$

<br>

---

- 학습목표
  - 전체 주변 단어의 발생 확률 $p(x_1,x_2,\cdots,x_C \mid t)$를 최대화하는 것
- 학습을 위한 목적함수를 음의 로그가능도로 정의할 수 있음

$E(W, U) = -\log p(x_1,x_2,\cdots,x_C \mid t)$

- 학습시간
  - 입력층~은닉층에서 $V \times N$ 에 비례하는 시간
  - 은닉층~출력층에서 $N \times V \times C$ 에 비례하는 시간
  - 학습 데이터 하나에 대한 학습 비용 --> $V \times N + N \times V \times C$에 비례함
- 단어 개수가 많은 경우 , 출력노드 개수가 단어 개수와 같으므로 softmax 계산하는데 시간이 오래 걸림

<br>

## Word2Vec 단점 및 보완 방법
단어 개수가 많은 사전인 경우 출력층에서 softmax 를 계산할 때 , 분모에서 모든 노드에 대한 계산 결과값이 필요 --> 하나의 데이터를 학습하는데 전체 단어 개수 V 만큼 계산하므로 높은 계산 비용이 듬

<br>

### Hierarchical Softmax
- softmax를 이진 트리로 근사하여 계산시간을 절감하자!! --> Hierarchical Softmax --> O(V)에서 O(logV)로 절감

![image](https://user-images.githubusercontent.com/55765292/199250281-53b572e6-0c93-416e-a3e0-5eebfc8160e3.png)

<br>

### Negative Sampling
- 표본을 추출해서 이들에 대해서만 계산
  - 표본 구성
    - 학습 데이터에 주어진 목표 출력 단어인 positive 단어들 $w_i$와 소수의 negative 단어들 $w_{nj}$로 구성
    - negative 단어는 무작위로 뽑을 수 있는 분포나 자주 접하는 단어를 가능하면 피할 수 있도록 하는 분포에서 선택함
  - 각 학습 데이터의 목표 출력 단어인 positive 단어들 $w_i$와 negative 단어들 $w_{nj}$에 대한 오차 함수 E를 최소화하도록 학습
    - 오차 함수가 소프트맥스의 값을 직접 사용하지 않기 때문에 시간 절감

$E = -\log \sigma (u_i \cdot h) - \sum\limits_{w_{nj}} \log \sigma (-u_{nj} \cdot h)$

![image](https://user-images.githubusercontent.com/55765292/199251008-f5c67934-dcc5-4753-af1c-4ff7ee4c53ba.png)

<br>

### Subsampling
- 자주 등장하는 단어는 학습에서 제외함
  - 고빈도 단어의 경우 등장 횟수만큼 모두 학습시키는 것이 비효율적
    - 예를들어, 조사 은/는 같은 것은 고빈도 단어로 학습기회가 많기 때문에 강제로 학습에서 어느정도 제외시킴
    - 반대로, 저빈도 단어인 경우는 해당 단어가 나올 때 마다 뺴놓지 않고 학습시킴

<br>

## 단어 임베딩(= 단어의 벡터표현)을 하면 무엇을 할 수 있는가?
- 유사한 의미의 단어가 벡터공간 상에서 가까운 곳에 위치

![image](https://user-images.githubusercontent.com/55765292/199251442-c26dfa59-e9aa-482a-816e-7f1b02529a5b.png)

- 단어 간의 유사도(similarity)를 벡터의 거리를 사용하여 구할 수 있다!!
  - 유의어 추출
  - 의미 분석
  - 관계 추출
  - 텍스트 분류
