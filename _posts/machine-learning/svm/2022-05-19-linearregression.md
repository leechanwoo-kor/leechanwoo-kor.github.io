---
title: "[Machine Learning] Linear Regression"
categories:
  - Machine Learning
tags:
  - Linear Regression
toc: true
toc_sticky: true
toc_label: "Linear Regression"
toc_icon: "sticky-note"
---

# 선형 회귀

## 단순 선형 회귀 (Simple linear regression)

### 단순 선형 회귀의 개요
- 회귀 목표는 연속형 반응 변수의 값을 예측하는 것
- 단순 선형 회귀란 설명 변수인 단일 특징과 단일 반응 변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링한 것
- 단순 회귀는 설명 변수의 차원과 반응 변수의 차원, 모두 2개의 차원을 가지며, 초평면은 한차원 낮은 1차원이 됨(1차원의 초평면은 선)

$y = α + βx$
{: .text-center}

- y : 반응 변수의 예측값
- 절편항 α와 계수 β: 알고리즘을 통해 학습하게 되는 모델의 파라미터
- x : 설명 변수

### 최소 제곱법
- OLS(Ordinary Least Squares) 또는 LLS(Linear Least Squares)로 부름
- 단순 선형 회귀에서 모델을 최적화하는 파라미터 값을 훈련 데이터로부터 학습하는 방법
- 비용 함수 : 손실 함수. 모델의 오차를 정의하고 측정하기 위해 사용
- 잔차 : residual. 훈련 오차로 훈련 데이터의 관측 값과 모델에 의한 예측 값의 차이
- 예측 오차 : prediction error. 테스트 오차로 테스트 데이터의 관측 값과 모델에 의한 예측 값의 차이
- 잔차의 합을 최소화 → 값을 예측할 수 있는 추정기를 만들 수 있음

### 비용함수를 사용한 모델의 적합화 척도: RSS
- 반응 변수의 예측 값이 훈련 데이터의 관측 값과 가까워지면 모델이 적합화되었다고 할 수 있음
- 모델의 적합화의 척도: 잔차 제곱합(RSS. Residual Sum of Squares)

$RSS = SS_res = \sum_{i = 1}^{n}{(y_i - f(x_i))^2}$
{: .text-center}

- $y_i$ : $i$번째 훈련 데이터에 대한 반응 변수의 관측 값
- $f(x_i)$ : $i$번째 훈련 데이터에 대한 반응 변수의 예측 값

### 단순 선형 회귀를 위한 OLS의 계산
- 단순 선형 회귀식: $y = α + βx$
- 비용 함수를 최소화하는 $α$ 값과 $β$ 값을 찾는 것이 목표
- $β$를 계산하기 위해 $x$의 분산과 $x$와 $y$의 공분산을 계산

$var(x) = \frac{\sum_{i = 1}^{n}{(x_i - \bar{x})^2}}{n-1}$
{: .text-center}

- $\bar{x}$ : $x$의 평균
- $x_i$ : $x$의 $i$번째 훈련 데이터의 값
- $\bar{y}$ : $y$의 평균
- $y_i$ : $y$의 $i$번째 관측 값
- $n$ : 훈련 데이터의 개수

---

- β의 값 계산 : $β = \frac{cov(x,y)}{var(x)}$
- α의 값 계산 : $α = \bar{y} - β\bar{x}$
  - $\bar{x}$ : $x$의 평균
  - $\bar{y}$ : $y$의 평균
  - $(\bar{x},\bar{y})$ : 모델이 반드시 지나가야 하는 센트로이드 좌표

### 단순 선형 회귀 모델의 평가
- 회귀 모델의 기본 예측 능력 평가 지표 : 결정계수 $R^2$
- 데이터가 회귀선에 얼마나 가깝게 분포하는지를 측정
- 모델의 의해 설명된 반응 변수 분산의 비율을 기술 → 0 ≤ $R^2$ ≤ 1
- $R^2$은 이상치에 민감하고, 모델에 새로운 특징이 추가되면 값이 증가하는 문제를 가짐
  - $R^2 = 1 - \frac{SS_res}{SS_tot}$
  - $SS_res = \sum_{i = 1}^{n}{(y_i - f(x_i))^2}$
  - $SS_tot = \sum_{i = 1}^{n}{(y_i - \bar{y})^2}$


## 다중 선형 회귀 (Multiple linear regression)

### 다중 선형 회귀의 개요
- 다중 선형 회귀란 설명 변수인 다수의 특징과 단일 반응 변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링한 것
- 다중 회귀는 설명 변수의 n차원과 반응 변수의 1차원, 모두 n + 1개의 차원을 가지며, 초평면은 한 차원 낮은 n차원이 됨

$y = α + β_1x_1 + β_2x_ + β_3x_ + \cdots + β_nx_n$
{: .text-center}

- y : 반응 변수의 예측 값
- 절편 항 α와 β_n : 알고리즘을 통해 학습하게 되는 모델의 파라미터
- x_n : 설명 변수

### 다중 선형 회귀의 벡터 정리
- 선형 회귀 모델의 벡터 표기법

$Y = Xβ$
$\left[\begin{array}{c} Y_1 \\ Y_2 \\ \vdots \\ Y_n \\ \end{array}\right] = \left[\begin{array}{c} α + βX_1 \\ α + βX_2 \\ \vdots \\ α + βX_n \\ \end{array}\right] = \left[\begin{array}{c} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \\ \end{array}\right] ⨉ \left[\begin{array}{c} α \\ β \\ \end{array}\right]$

- $Y$ : 반응 변수의 예측 값을 가진 열 벡터
- $β$ : 알고리즘을 통해 학습하게 되는 모델의 파라미터 값을 갖는 열 벡터
- $X$ : 설명 변수 값을 갖는 $m ⨉ n$차원 행렬로 $m$ 은 훈련 데이터의 개수, $n$ 은 특징의 개수

- 훈련 데이터로 부터 $X$ 와 $Y$ 의 값을 알고 있으므로 비용함수를 최소화하는 $β$ 를 계산

$Y = Xβ$
{: .text-center}

$β = (X^TX)^{-1}X^TY$
{: .text-center}

  - $Y$ : 반응 변수의 예측 값을 가진 열 벡터
  - $β$ : 알고리즘을 통해 학습하게 되는 모델의 파라미터 값을 갖는 열 벡터
  - $X$ : 설명 변수 값을 갖는 $m ⨉ n$차원 행렬로 $m$ 은 훈련 데이터의 개수, $n$ 은 특징의 개수
  - $X^T$ : X의 전치행렬


## 다항 회귀 (Polynomial Regression)

### 다항 회귀의 개요
- 다항 회귀란 반응 변수와 다항식으로 표현된 특징 사이의 선형 관계를 모델링 할 수 있는 다중 선형 회귀의 특수한 형태
- 특징을 변환한 후 다중 선형 회귀에서와 같은 방식으로 설명 변수와 반응 변수의 비선형 관계를 모델링함
- 단일 설명 변수의 단일 특징을 사용하고 있지만 모델의 항은 n개

$y = α + β_1x + β_2x^2 + \cdots + β_nx^n$
{: .text-center}

  - $y$ : 반응 변수의 예측 값
  - 절편항 $α$와 계수 $β_n$ : 알고리즘을 통해 학습하게 되는 모델의 파라미터
  - $x^n$ : 설명 변수

## 주요 정리

<div>
  1. 선형회귀 모델을 생성하기에 앞서 상관분석, 산점도행렬 등을 이용해 데이터에 대한 탐색적 분석을 실행합니다.<br>
  2. 단순 선형 회귀란 설명 변수인 단일 특징과 단일 반응 변수 간에 선형 관계가 있다.<br>
  3. 다중 선형 회귀는 설명 변수인 다수의 특징과 단일 반응 변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링한 것이다.<br>
  4. 다항 회귀는 설명 변수와 반응 변수의 비선형 관계를 모델링한다.<br>
  5. 회귀모델 성능의 기본 평가지표는 결정계수 $R^2$를 이용해 측정한다.<br>
</div>
{: .notice--warning}
