---
title: "[Machine Learning] Attention Is All You Need (1)"
categories:
  - Machine Learning
tags:
  - Support Vecrtor Machine
  - SVM
toc: true
toc_sticky: true
toc_label: "Attention Is All You Need (1)"
toc_icon: "sticky-note"
---

# Attention Is All You Need

<br>

## Transformer ì˜ì˜

<br>

### Transformer ê¸°ì—¬
- ê¸°ì¡´ì˜ Sequence Transduction(ë³€í™˜) ëª¨ë¸ì€ ì¸ì½”ë”(Encoder)ì™€ ë””ì½”ë”(Decoder)ë¥¼ í¬í•¨í•˜ëŠ” êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìˆœí™˜ ì‹ ê²½ë§(Recurrent)ë‚˜ Convolution Layerë¥¼ ì‚¬ìš©í•¨
- ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì˜ íŠ¹ì§• : Attention ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•´ì„œ, ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•œ ëª¨ë¸
- Attention ë©”ì»¤ë‹ˆì¦˜ë§Œì„ ì‚¬ìš©í•˜ëŠ” "Transformer"ë¼ëŠ” ìƒˆë¡œìš´ êµ¬ì¡°ë¥¼ ì œì•ˆ
  - ê¸°ê³„ë²ˆì—­(Machine Translation) Taskì—ì„œ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥
  - í•™ìŠµ ì‹œ, ìš°ìˆ˜í•œ ë³‘ë ¬í™”(Parallelizable) ë° í›¨ì”¬ ë” ì ì€ ì‹œê°„ ì†Œìš”
  - êµ¬ë¬¸ë¶„ì„(Constituency Parsing) ë¶„ì•¼ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ â†’ ì¼ë°˜í™”(Generalization)ë„ ì˜ë¨

**ğŸ“Œ Parsing?**
![image](https://user-images.githubusercontent.com/55765292/195633401-84291ae1-82bc-4fce-a8e7-ae98da87db93.png)

<br>

## Background

### Sequential ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ, ì´ì „ ì—°êµ¬
- LSTM, GRU ë“±ì„ í™œìš©í•œ, "Rucurrent(ìˆœí™˜) êµ¬ì¡°"ê°€ ì–¸ì–´ ëª¨ë¸ë§ ë° ê¸°ê³„ë²ˆì—­ ë“±ì˜ Taskì—ì„œ í™•ê³ í•œ ì…ì§€ë¥¼ ë‹¤ì ¸ì™”ìŒ
- Recurrent(ìˆœí™˜) êµ¬ì¡°ëŠ” Inputê³¼ Output Sequenceë¥¼ í™œìš©
  - $h_t$ : $t-1$ë¥¼ Inputê³¼ $h_{t-1}$ë¥¼ í†µí•´ ìƒì„± â†’ ì´ëŸ¬í•œ êµ¬ì¡°ë¡œ ì¸í•´ ì¼ê´„ì²˜ë¦¬ê°€ ì œí•œë¨.
  - ìˆœì°¨ì ìœ¼ë¡œ $t$ì´ì „ì˜ Outputì´ ë‹¤ ê³„ì‚°ë˜ì–´ì•¼ ìµœì¢… Outputì´ ìƒì„±
    - ë³‘ë ¬í™” ì œí•œë¨
    - Sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë” ì·¨ì•½í•´ì§ â†’ Factorization Trickì´ë‚˜ Condition Computationìœ¼ë¡œ ê³„ì‚° íš¨ìœ¨ì„±ì„ ì¦ëŒ€í–ˆìœ¼ë‚˜ í•œê³„ì  ì¡´ì¬

<br>

### Sequential Computation ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ, ì´ì „ ì—°êµ¬
- CNN êµ¬ì¡°ë¥¼ í†µí•œ ë³‘ë ¬í™” ê³ ë ¤ í–ˆì§€ë§Œ, ì—¬ì „íˆ í•œê³„ì  ì¡´ì¬
  - ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•œ ì¶”ê°€ ì—°ì‚° í•„ìš”
  - ì›ê±°ë¦¬ Position ê°„ì˜ Dependencies(ì¢…ì†ì„±)ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€

**ğŸ“Œ CNN**

![image](https://user-images.githubusercontent.com/55765292/195637481-1bc76d4b-544d-478b-be8e-8cf2b18122fa.png)

<br>

### Attention ë©”ì»¤ë‹ˆì¦˜
- Sequence ëª¨ë¸ë§ì—ì„œ í•„ìˆ˜ì ì¸ ìš”ì†Œ
- Inputì´ë‚˜, Output Sequenceì— ê´€ê³„ì—†ì´, Dependencies(ì¢…ì†ì„±)ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ
- í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ RNNì— ë¶™ì—¬ì„œ ì‚¬ìš©í–ˆìŒ

**ğŸ“Œ Attention**

![image](https://user-images.githubusercontent.com/55765292/195638421-3af14ad5-cf1d-4730-b4d0-aced4836f523.png)

<br>

### Self-Attention ë©”ì»¤ë‹ˆì¦˜
- **Infra-Attention**ì´ë¼ê³ ë„ ë¶ˆë¦¼
- ë‹¨ì¼ Sequence ì•ˆì—ì„œì˜ Positionë“¤ì„ ì—°ê²°
- Self-Attentionì€ ë…í•´, ìš”ì•½, Sentence Representationì—ì„œ íš¨ê³¼ì 
- **Recurrent Attention**ì´ **Sequence-ALigned Recurrent** ë³´ë‹¤ End-to-End í•™ìŠµì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥

<br>

### Transformerì˜ íŠ¹ì§•
- ì¼ê´„ì²˜ë¦¬ê°€ ë˜ì§€ ì•ŠëŠ” Recurrance êµ¬ì¡° í”¼í•¨ â†’ Self-Attentionë§Œìœ¼ë¡œ ì…âˆ™ì¶œë ¥ì˜ Representationì„ ê³„ì‚°
- Inputê³¼ Output ì‚¬ì´ì˜ **Global Dependency**ë¥¼ í•™ìŠµ
- ë³‘ë ¬í™”(Parallelizable) ìš°ìˆ˜ (P100-8 GPU â†’ 12ì‹œê°„ í•™ìŠµ)
- SOTA ë‹¬ì„±

<br>

## Model Architecture

<br>

### ì „ì²´ì ì¸ êµ¬ì¡°
- ê¸°ê³„ë²ˆì—­ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ **Encoder-Decoder** êµ¬ì¡°ë¥¼ ê°€ì§
- ì…ë ¥ $(x_1, \dots, x_n)$ì€ encoderë¥¼ í†µí•´ $z=(z_1, \dots, z_n)$ë¡œ í‘œí˜„ ë° ë§¤í•‘ë¨
- Encoderë¡œ í‘œí˜„ëœ $z$ë¥¼ í™œìš©í•˜ì—¬, í•œ ë²ˆì— í•œ Element ì”© Output Sequence $(y_1, \dots, y_m)ê°€ ìƒì„±
  -  Auto-Regressive : ìƒì„±ëœ Symbolì€ ë‹¤ìŒ ìƒì„± ê³¼ì •ì—ì„œ ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ ì‚¬ì—°

![image](https://user-images.githubusercontent.com/55765292/195642002-9f1f2438-6aff-480d-a50d-f41483d95794.png)

Encoder-Decoder êµ¬ì¡°{: .text-center}

<br>

### Encoder êµ¬ì¡°
- $N=6$ì˜ ë™ì¼í•œ Layer Stackìœ¼ë¡œ êµ¬ì„±
- ê° LayerëŠ” 2ê°œì˜ Sub-Layerë¡œ êµ¬ì„±
  - Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜
  - Position-wise Fully Connected Feed-Forward Network
    - 1x1 Conv Layerê°€ 2ê°œ ì´ì–´ì§„ ê²ƒê³¼ ê°™ìŒ
    - Position ë³„ë¡œ ë™ì¼í•œ Fully Connected Feed-Forward Networkê°€ ì ìš©(Dim=2048)
- ê° Sub-LayerëŠ” Residual Connection ë° Layer Normalization ì ìš©

$LayerNorm(x+SubLayer(x))${: .text-center}

- Residual Connection ì ìš©ì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´, Sub-layer, Embedding, Output Dimensionì„ 512ë¡œ í†µì¼
  - Residual Connection ì ìš©ì„ ìœ„í•´ì„ , Inputê³¼ ì—°ê²°ë˜ Outputì˜ Dimenstionì´ ë™ì¼í•´ì•¼í•¨

![image](https://user-images.githubusercontent.com/55765292/195643160-5d7b0ff2-1893-4a67-9e4a-27c60401bd0d.png)

<br>

### Decoder êµ¬ì¡°
- $N=6$ì˜ ë™ì¼í•œ Layer Stackìœ¼ë¡œ êµ¬ì„±
- ê° LayerëŠ” 3ê°œì˜ Sub-Layerë¡œ êµ¬ì„±
  - Masked Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜
    - Decoderì˜ Multi-Head Self-Attentionì€ Maskingì„ ì‚¬ìš©í•˜ì—¬, í›„ì†(Subsequent) Positionì€ Attention ì•ˆ í•˜ë„ë¡ ì¡°ì •
    - Position$(i)$ë¥¼ ì˜ˆì¸¡í•  ë•ŒëŠ”, $(i)$ ë³´ë‹¤ ì‘ì€ ìœ„ì¹˜ì— ì•Œë ¤ì§„ Outputì—ë§Œ ì˜ì¡´
    - Output Embeddingì€ One Position ì”© Offset(ì£¼ì†Œ ì°¸ì¡°)
  - Encoderì˜ Outputì— ëŒ€í•´ Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜ ìˆ˜í–‰
  - Position-wise Fully Connected Feed-Forward Network

<br>

### Encoder-Decoder êµ¬ì¡° ì •ë¦¬ ë° ìš”ì•½
- EncoderëŠ” Self-attention layers êµ¬ì¡°ë¡œ êµ¬ì„±
  - ì´ì „ Encoder Layerì—ì„œì˜ ì¶œë ¥ â†’ í˜„ì¬ Encoder Layerì—ì„œì˜ ë™ì¼ Positionì˜ ì…ë ¥
  - ê° Encoder LayerëŠ” ì´ì „ Layerë¡œë¶€í„° ëª¨ë“  ìœ„ì¹˜ë¥¼ ì²˜ë¦¬í•œ ì •ë³´ë¥¼ í™œìš©
- Decoder Layer íŠ¹ì§•
  - Sequence-to-Sequence ëª¨ë¸ì—ì„œì˜ ì¼ë°˜ì ì¸ ì¸ì½”ë”-ë””ì½”ë” Attention ë©”ì»¤ë‹ˆì¦˜ì„ ëª¨ë°©
  - Query : ì´ì „ Decoder Layerì—ì„œ ê°€ì§€ê³ ì˜´
  - Keyì™€ Value : encoderì—ì„œì˜ outputì—ì„œ ê°€ì§€ê³  ì˜´
  - ê° DecoderëŠ” Encoderì˜ ëª¨ë“  Positionì •ë³´ë¥¼ ì‚¬ìš©í•¨
- Decoderì—ì„œì˜ ëª¨ë“  Position ì •ë³´ ì‚¬ìš© ë°©ì§€
  - auto-regressive ì†ì„±(Property)ì„ ìœ„í•´ ì™¼ìª½ìœ¼ë¡œë¶€í„° ë„˜ì–´ì˜¤ëŠ” ì •ë³´íë¦„ì„ ë°©ì§€í•´ì•¼ í•¨
  - ì˜ëª»ëœ ì—°ê²°ì— í•´ë‹¹í•˜ëŠ” Softmax ê°’ì„ ë§ˆìŠ¤í‚¹(-âˆ)ìœ¼ë¡œ ì„¸íŒ…

<br>

### Attentionì— ëŒ€í•œ ê³ ì°°
- í¬ê²Œ ë‘ ê°€ì§€ ì¢…ë¥˜ê°€ ìˆìŒ

**ğŸ“Œ Attention ì¢…ë¥˜**

![image](https://user-images.githubusercontent.com/55765292/195646591-26a2dbd5-4bb3-4839-9475-58f8be4d00b3.png)

1) Additive Attention : Single Hidden Layerë¡œ êµ¬ì„±ëœ Feed-Forwad Networkë¥¼ í™œìš© â†’ Compatibility(ì¼ì¹˜ì„±) ê³„ì‚°

**ğŸ“Œ Additive Attention?**

![image](https://user-images.githubusercontent.com/55765292/195646908-2ce5ae6c-da0d-436b-8a0b-d4f7265ec4ce.png)

2) Dot-product(Multiplicative) Attention : 
- ì¥ì  : íš¨ìœ¨ì ì¸ í–‰ë ¬ê³± êµ¬ì„±ìœ¼ë¡œ, **ì‹¤ì œë¡œ ë” ë¹ ë¥´ê³  ê³µê°„ íš¨ìœ¨ì **
- ë‹¨ì  : ì‘ì€ Key ë²¡í„°ì˜ ì°¨ì›$(d_k)$ì—ì„œëŠ” ë‘ ë©”ì»¤ë‹ˆì¦˜ì´ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë” í° $d_k$ì—ì„œëŠ” Additive Attentionì´ ë” ì¢‹ìŒ

<br>

### Attention í•µì‹¬ ê³„ì‚° ê³¼ì •

### The Beast with Many Head

### ìµœì¢… Self-Attention ê³„ì‚° ê³¼ì • ìš”ì•½

### Position-wise Feed-Forward Networks

### Embedding ë° Softmax êµ¬ì„±
