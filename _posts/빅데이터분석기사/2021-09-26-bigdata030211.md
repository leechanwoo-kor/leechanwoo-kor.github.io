---
title: "[빅데이터분석기사 필기] Ⅲ.빅데이터 모델링 - 02. 분석기법 적용 (11) 앙상블 분석"
categories:
    - 빅데이터분석기사
tags:
    - 빅데이터분석기사
toc: true
toc_sticky: true
toc_label: "02. 분석기법 적용 (11) 앙상블 분석"
toc_icon: "sticky-note"
---

**키워드🔑**<br>
앙상블분석, 배깅, 부트스트랩, 부스팅, AdaBoost, 
<br>랜덤포레스트, 배깅을 이용한 포레스트 구성, 임의노드 최적화, 
<br>중요 매개변수, 포레스트 크기, 최대 허용 깊이, 임의성 정도
{: .notice--warning}

# 02. 분석기법 적용

## 2. 고급 분석기법

### 7) 앙상블 분석

<br>
(1) 앙상블(Ensemble)<br>
- 앙상블 기법
    - 여러 개의 동일 or 상이한 모형들의 예측 or 분류 결과를 종합하여 최종적인 의사결정에 활용하는 기법
- 앙상블 특징: 신뢰성 ↑ 정확도 ↑ 원인분석 X
    - 높은 신뢰성: 다양한 모형의 결과를 결합 → 단일 모형보다 높은 신뢰성 확보
    - 높은 정확도: 이상값 대응력 상승 / 전체 분산 감소 → 높은 모델 정확도(Accuracy)
    - 원인분석에 부적합: 모형 투명성 감사 → 현상에 대한 정확한 원인분석에는 부적합

<br>
(2) 앙상블 알고리즘<br>
- 여러 개의 예측 모형 생성 → 조합 → 하나의 최종 예측 모형
    - 조합: 다중 모델 조합, 분류기 조합
- 여러 개의 학습 모델 훈련 → 투표 → 최적화된 예측 수행 및 결정
    - 투표: 다수결로 최종 결과를 선정
- 학습 데이터의 작은 변화 때문에, 예측 모형이 크게 변한다면 → 불안정한 학습방법
- 가장 안정적인 학습방법: 1-nearest neighbor, linear regression, ...
    - 1-nearest neighbor: 가장 가까운 자료만 변하지 않으면, 예측 모형이 변하지 않음
    - 선형 회귀 모형: 최소제곱법으로 추정하여 모형을 결정함

<br>
(3) 앙상블 학습절차<br>
📌 도출 및 생성 → 집합별 모델학습 → 결과 조합 → 최적 의견 도출

- 도출 및 생성: 학습 데이터에서 학습집합을 여러 개 도출
- 집합별 모델학습: 각각의 집합마다 모델학습

---

<br>
(4) 앙상블 기법 종류<br>
📌 배깅 / 부스팅 / 랜덤포레스트

- 배깅(Bagging)
    - 학습 데이터에서 다수의 부트스트랩 자료 생성
    - 각 자료마다 모델 생성
    - 각 모델마다 도출된 결과 결합
    - 최종 예측 모형
- 부스팅(Boosting)
    - 학습 데이터에 동일한 가중치 적용하여 분류기 생성
    - 가중치 변경하여 분류기 다시 생성
    - 반복 및 결합
    - 최종 분류 모형
- 랜덤 포레스트(Random Forest)
    - 학습 데이터에서 N개의 부트스트랩 자료 생성
    - 분류기 훈련 후 대표 변수 샘플 도출
    - 의사결정나무의 Leaf Node로 분류
    - Leaf Node들의 선형결합

|기법|배깅(Bagging)|부스팅(Boosting)|
| :----------: | :----------: | :----------: |
|목표|-분산 감소<br>-전반적으로 분류를 잘 하도록 유도함|-예측력 강화<br>-분류하기 힘든 관측값들에 대해서, 정확하게 분류하도록 유도함|
|최적 모델 결정 방법|-독립수행 후, 다수결 결정<br>-연속형 변수: 평균<br>-범주형 변수: 다중투표|-순차 수행에 따른 가중치 재조정<br>-정분류 데이터: 낮은 가중치 부여<br>-오분류 데이터: 높은 가중치 부여<br> ⇒ 오답을 정답으로 맞추기 위해서|
|장점|-성능 향상에 효과적<br>-결측값에 강함|-특정 케이스에서 상당히 높은 성능<br>-과대적합 없음(일반적으로)|
|단점|계산 복잡도 다소 높음|계산 복잡도 다소 높음|
|적용 방안|소량 데이터 유리함<br>변수 크기가 작아, 단순할수록 유리함|대량 데이터 유리함<br>데이터, 속성이 복잡할수록 유리함|
|주요 알고리즘|MetaCost Algorithm|AdaBoost Algorithm|

---

<br>
(5) 배깅<br>
📌 부트스트랩 데이터 여러 개 → 데이터마다 모델링 → 다수결로 최종 결정

- 배경(Bagging; Bootstrap Aggregating)
    - 학습 데이터에서 다수의 부트스트랩 자료를 생성하고, 각 자료에 대해 모델을 생성한 후 결합하여 최종예측모델을 만드는 알고리즘
    - 부트스트랩(Bootstrap): 단순 랜덤 복원추출 → 동일 크기 표본을 여러 개 생성하는 샘플링 방법
    - 절차: 부트스트랩 데이터 추출 → 단일 분류자 생성 → 최종 모델결정
        - 부트스트랩 데이터 추출: 동일 크기 부트스트랩 데이터 n개 추출
        - 단일 분류자 생성(모델링): 각 데이터마다 단일 분류자 모델 n개 생성
        - 최종 모델 결정: 다수결(Majority Voting) or 평균으로 n 개의 모델 결과를 결합
            - 보팅: 여러 모델을 학습시켜서 나온 결과 → 다수결 투표로 최종 결과 선정
    - 특징
        - 배깅에선 가지치기 하지 않음 → 최대한 성장한 의사결정나무들을 활용
        - 훈련자료를 모집단으로 간주 → 평균 예측 모형을 구함 → 분산을 줄이고, 예측력을 향상시킴

---

<br>
(6) 부스팅<br>
📌 예측력 약한 모델들을 결합 → 강한 예측 모형 생성

- 부스팅(Boosting)
    - 잘못 분류된 개체들에 가중치 적용 후, 새로운 분류기 모델을 생성하는 과정을 반복하고, 약한 모델(Weak Learner)들을 결합하여 최종 모델을 만드는 알고리즘
- 순차적으로 진행: 분류기 1 생성 → 이 정보를 바탕으로 분류기 2 생성 → 이 정보를 바탕으로 분류기 3 생성 → 반복... → 생성된 분류기들을 결합 → 최종 분류기 모델
- 절차: 동일 가중치 분류기 생성 → 가중치 변경하여 분류기 생성 → 최종 분류기 생성
    - 가중치 변경: 이전 분류기의 결과에 따라서 가중치를 변경
    - 최종 분류기: 목표 정확도가 나올 때까지 n회 반복 후, 최종 분류기 결정
- AdaBoost(Adaptive Boost Algorithm): 배깅보다 성능 높은 경우 많음(예측 오차 향상)
    - 이진 분류 문제
    - 랜덤 분류기보다 좀 더 좋은 분류기 n개 생성
    - n개 분류기마다 가중치 설정(가중치 합 = 1)
    - n개 분류기를 결합하여 최종 분류기 생성

---

<br>
(7) 랜덤 포레스트<br>
📌 랜덤 입력에 따른 여러 트리의 집합인 포레스트를 이용한 분류 방법

- 랜덤 포레스트(Random Forest)
    - 배깅, 부스팅보다 더 많은 무작위성을 주어서 약한 학습기들을 생성한 후, 이것들을 선형결합하여 최종 학습기를 만드는 알고리즘
- 의사결정트리의 분산이 크다는 단점 개선 → 약한 학습기들을 선형결합
- 무작위성(랜덤성): 사건에 패턴, 예측가능성, 인위적요소, 규칙성이 없음
    - 트리들이 서로 조금씩 다른 특징을 가짐 → 예측 결과들의 비상관화&일반화
    - 노이즈에 강인하게 해줌
- 데이터로부터 임의복원추출을 통해 여러 개의 학습데이터를 추출하고, 학습데이터마다 개별 학습을 시켜서 트리를 생성하여, 투표 or 확률 등을 이용하여 최종목표변수를 예측한다
- 절차: 데이터 추출 → 대표변수 샘플 도출 → Leaf Node 분류 → 최종모델 결정
    - 부트스트랩 데이터 추출: 부트스트랩 데이터 n개 추출
    - 대표변수 샘플 도출: n개의 분류기를 훈련시켜 대표변수 샘플을 도출
    - Leaf Node 분류: 대표변수 샘플들을 트리의 Leaf Node로 분류
    - 최종모델 결정: Leaf Node들을 선형결합
- 특징
    - 변수 제거 없이 진행 → 입력변수가 많은 경우 정확도/예측력 높음
    - 이론적 설명과 결과 해석이 어려움
    - 배깅/부스팅과 비슷 or 더 좋은 예츠력
    - 배깅과의 가장 큰 차이점: 전체 변수 집합에서 부분 변수 집합을 선택한다는 점

---

<br>
(8) 랜덤 포레스트의 주요기법<br>
📌 배깅을 이용한 포레스트 구성 / 임의노드 최적화 / 중요 매개변수(크기/깊이/임의성)

- 배깅을 이용한 포레스트 구성(Bagging = Bootstrap Aggregating)
    - 부트스트랩 → 조금씩 다른 훈련 데이터 생성 → 훈련시킨 기초 분류기들을 결합시킴
- 임의노드 최적화: 노드 분할 함수 / 훈련목적함수 / 임의성 정도
    - 훈련목적함수를 최대화시키는 노드 분할 함수의 매개변수θ 최적값을 구하는 과정
    - 노드 분할 함수
        - 각 트리노드마다 좌우 자식노드로 분할하기 위한 함수
        - h(v, θj) ∈ {0, 1}
        - 노드에 도달한 데이터는 함수결과에 따라서 자식노드로 보내짐
        - 매개변수 θ = Φ, Ψ, τ 에 따라서 분할 함수 결정됨
    - 훈련목적함수
        - τ = 노드 분할 함수의 매개변수 θ의 가능한 모든 경우를 포함하는 집합
        - τj = j번째 노드의 훈련단계에서 τ의 부분집합 τj을 만듦
        - 매개변수의 최적값 θj*는 임곗값들 τj안에서, 목적함수 = 정보 획득량을 최대로 만드는 값으로 계산됨
    - 임의성 정도
        - 비상관화 수준의 결정요소
        - 임의성 정도는 │τj│/│τ│ 로 결정됨
        - ρ = │τj│ 가 임의성 정도 결정 ρ 값은 모든 트리 노드에서 같은 값
        - ρ = │τ│ → 모든 트리 서로 동일
        - ρ = 1 → 최대 임의성 비상관화된 트리
- 노드 분할 함수의 매개변수
    - Φ = 필터함수/ 특징 몇개만 선택 (특징 배깅)
    - Ψ = 분할함수의 기하학적 특성을 이용하여 데이터를 분리할지 나타냄
    - τ = 이진 테스트의 부등식에서 임곗값들을 가지고 있음
- 중요 매개변수: 포레스트 크기 / 최대 허용 깊이 / 임의성 정도

|포레스트 크기 T|최대 허용 깊이 D|임의성 정도|
| :----------: | :----------: | :----------: |
|-포레스트의 트리 개수<br>-포레스트를 몇개의 트리로 구성할지 결정하는 매개변수<br>-크기가 작으면 → 시간 ↓ 일반화능력 ↓<br>-크기 크면 → 시간 ↑ 정확성 ↑ 일반화능력 ↑|-한 트리의 최대 깊이<br>-한 트리에서 루트~종단 노드까지 최대 몇 개의 노드(테스트)를 거칠지 결정하는 매개변수<br>-깊이 작으면 → 과소 적합<br>-깊이 크면 → 과대 적합|임의성 정도에 따라서<br>비상관화 수준이 결정됨|

---