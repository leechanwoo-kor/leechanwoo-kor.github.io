---
title: "[빅데이터분석기사 필기] Ⅰ.빅데이터 분석 기획 - 03. 데이터 수집 및 저장 계획(1)"
categories:
    - 빅데이터분석기사
tags:
    - 빅데이터분석기사
toc: true
toc_sticky: true
toc_label: "03. 데이터 수집 및 저장 계획(1)"
toc_icon: "sticky-note"
---

**키워드🔑**<br>
빅데이터 처리, 데이터 수집, ETL, FTP, Sqoop, Crawling, RSS, Scrpy, Apache Kafka, Flume, Scribe, Chukwa
{: .notice--warning}

# 03. 데이터 수집 및 저장 계획

## 1. 데이터 수집 및 전환

< 데이터 처리 기술 >
- 필터링: 목적에 맞지 않는 정보를 필터링 하여 분석시간 및 저장공간을 효율적으로 사용
    - 정형 데이터: 오류발견, 보정, 삭제, 중복성 검사 등
    - 비정형 데이터: 자연어 처리, 기계학습과 같은 추가 기술 적용 → 오류 및 중복과 같은 저품질 데이터 필터링
- 변환: 분석하기 쉽도록 일관성 있는 형식으로 변환
    - 평활화, 집계, 일반화, 정규화, 속성 생성 기술을 사용
- 정제: 데이터의 불일치성을 교정하기 위함
    - 결측값 처리, 잡음(Noise) 처리 기술을 활용
- 통합: 출저는 다르지만 상호연관성이 있는 데이터들 → 하나로 결합
    - 연관 관계 분석 등을 통해 중복 데이터 검출 필요
- 축소: 분석에 불필요한 데이터 축소
    - 데이터의 고유한 특성은 손상되지 않도록 → 분석 효율성 ↑

### 1) 데이터 수집

<br>
(1) 데이터 수집 프로세스<br>
📌 수집 데이터 도출 → 목록 작성 → 데이터 소유기관 파악 및 협의 → 데이터 유형 분류 및 확인(포맷 등) → 수집 기술 선정(포맷에 맞게) → 수집 계획서 작성 → 수집 주기 결정 → 데이터 수집 실행
- 수집 데이터 도출: 빅데이터 서비스 제공 시 서비스의 품질을 결정하는 중요한 핵심 업무
- 목록 작성: 수집 가능성, 보안, 정확성, 수집 비용 등을 검토
- 수집 기술 선정: 다양한 유형의 데이터 수집을 위해 확장성, 안정성, 실시간성, 유연성 확보 필요
- 수집 주기 결정: 데이터 유형에 따라 배치 or 실시간 방식을 적용

<br>
(2) 수집 데이터의 대상<br>
📌 데이터 위치에 따라 외부 or 내부
- 내부 데이터: 조직(인프라 내부), 주로 수집하기 쉬운 정형 데이터, 서비스 수명 주기 관리 용이
    - 서비스: SCM, ERP, CRM, 포털, 인증 시스템, 거래 시스템 등
    - 네트워크: 백본, 방화벽, 스위치, IPS 등
    - 마케팅: VOC 접수, 고객 포털 시스템 등


**💡**<br>
**SCM(Supply Chain Management, 공급사슬관리) 이란,**<br>
**부품 제공업자로부터 생산자, 배포자, 고객에 이르는 물류 흐름을 하나의 가치사슬 관점에서 파악하고,**<br>
**필요한 정보가 원활히 흐르도록 지원하는 시스템을 말한다.**<br>
**기업이 외부 공급업체 또는 제휴업체와 통합된 정보시스템으로 연계하여**<br>
**시간과 비용을 최적화시키기 위한 것으로, 자재구매, 생산/재고, 유통/판매, 고객데이터로 구성된다.**<br>
{: .notice--primary}


**💡**<br>
**ERP(Enterprise Resource Planning, 전사적 자원 관리) 란,**<br>
**회사의 모든 정보, 공급사슬관리, 고객 주문정보까지 포함하여 통합적으로 관리하는 시스템을 말한다.**<br>
{: .notice--primary}


**💡**<br>
**CRM(Customer Relationship Management, 고객 관계관리) 이란,**<br>
**소비자들을 자신의 고객으로 만들고 장기간 유지하고자 하는 경영방식이며,**<br>
**고객에 대한 정보를 분석/저장하는 데에 사용하는 넓은 분야를 아우른다.**<br>
{: .notice--primary}

- 외부 데이터: 주직(인프라) 외부, 주로 수집하기 어려운 비정형 데이터
    - 소셜: SNS, 커뮤니티, 게시판
    - 네트워크: 센서 데이터, 장비 간 발생로그(M2M; Machine 2 Machine)
    - 공공: 정부에서 공개한 공공 데이터(LOD; Linkded Open Data)

---

<br>
(3) 데이터 수집 방식 및 기술<br>
📌 정형, 비정형, 반정형 → 데이터 유형에 따라 적합한 방식이 다르다

|정형 데이터 수집|비정형 데이터 수집|반정형 데이터 수집|
| ------------ | ------------ | ------------ |
|**-ETL:** 추출, 변환, 적재<br>**-FTP:** 파일 송수신 프로토콜<br>**-API:** 실시간 데이터 수신 인터페이스<br>**-DBToDB:** 데이터베이스간 동기화<br>**-Rsync:** 일대일 동기화<br>**-Sqoop:** RDBMS와 하둡간 데이터 전송|**-크롤링:** 웹사이트에서 데이터수집<br>**RSS:** XML기반 프로토콜 활용<br>**-Open API:** 실시간 데이터 수신<br>**-스크래파이:** 파이썬 기반 크롤링<br>**-아파치 카프카:** 데용량 실시간 로그 처리|**-센싱:** 센서 데이터<br>**-스트리밍:** 미디어 실시간 수집<br>**-플럼:** 분산형 대량 로그 수집 기술<br>**-스크라이브:** 대량 실시간 로그 수집 기술<br>**-척와:** 대규모 분산 시스템 모니터링|

- 정형 데이터 수집 방식 및 기술: ETL, FTP, API, DBToDB, Rsync, Sqoop
    - ETL: 추출(Extract), 변환(Transform), 적재(Load)
        - 데이터를 추출, 변환하여 데이터 웨어하우스 DW() 및 데이터 마트(DM)에 저장하는 기술
        - 데이터 조회 or 분석을 목적으로 적절한 포맷 or 구조로 데이터를 변환한다

        **💡**<br>
        **데이터 웨어하우스(Data Warehouse)란,**<br>
        **DB 축적된 데이터를 공통 형식으로 변환해서 관리하는 저장소**<br>
        **여기서 관리하는 데이터들은 시간의 흐름에 따라 변화하는 값을 유지한다**<br>
        {: .notice--primary}

        **💡**<br>
        **데이터 마트(Data Mart)란, DW에서 데이터를 꺼내 사용자에게 제공하는 역할이며,**<br>
        **특정 사용자가 관심을 가지고 있는 데이터를 담은 비교적 작은 규모의 DW이다**<br>
        **따라서, 재무/ 생산/ 운영 등과 같이 특정 조직의 특정 업무 분야에 초점을 맞추어 구축된다**<br>
        {: .notice--primary}

    - FTP: File Transfer Protocol, 파일 송수신 응용계층 통신 프로토콜
        - 원격지 시스템 간 파일 공유를 위한 서버-클라이언트 모델
        - TCP/IP 프로토콜을 기반으로 서버, 클라이언트 사이에서 파일 송수신
        - Active FTP: 클라이언트가 포트를 알려주면 데이터 전송해주는 방식
        - Passive FTP: 서버가 포트를 알려주면 데이터 가져가는 방식
    - API: Application Programming Interface, 실시간 데이터 수신 기능을 제공하는 인터페이스 기술
        - 솔루션 제조사 및 3rd party 소프트웨어로 제공되는 도구
    - DBToDB: 데이터베이스 시스템 간 데이터 동기화 및 전송 기능
    - Rsync: Remote Sync, 서버-클라이언트 방식, 수집 대상 시스템과 일대일로 파일 및 디렉터리를 동기화
    - 스쿱(Sqoop): 커넥터를 사용하여 RDBMS와 하둡 간 데이터 전송
        - 전송, 수집 등 모든 적재 과정이 자동화, 병렬 처리 방식
        - 스쿱 특징: 벌크 임포트 지원(한번에 전송), 데이터 전송 병렬화, 직접입력 제공, 프로그래밍 방식의 데이터 인터렉션
        - 툴: Import(가져오기), Export(내보내기), Job(생성 및 실행) Metastore, Merge(데이터셋 병합)

---

- 비정형 데이터 수집 방식 및 기술: 크롤링, RSS, Open API, 스크래파이, 아파치 카프카
    - 크롤링(Crawling): 웹 사이트로부터 컨텐츠를 수집
    - RSS(Rich Site Summary): XML 기반으로 정보를 배포하는 프로토콜을 활용한 데이터 수집 기술
    - Open API: 응용 프로그램을 통해 실시간 데이터 수신
    - 스크래파이(Scrapy): Python 기반 비정형 데이터 수집 기술
        - 웹 사이트를 크롤링하여 구조화된 데이터 수집
        - 주요기능: Spider, Selector, Items, Pipelines, Settings
    - 아파치 카프카(Apache Kafka): 대용량 실시간 로그 처리를 위한 분산 스트리밍 플랫폼
        - 레코드 스트림을 발행, 구독하는 방식(기존 메시징 시스템과 유사)
        - 특징: 신뢰성, 확장성 제공(수평 확장 및 분산 처리 가능)
        - 주요 기능: 소스(수집영역), 채널(소스와 싱크간 버퍼구간), 싱크(전달 및 저장), 인터프리터(가공)

---

- 반정형 데이터 수집 방식 및 기술: 센싱, 스트리밍, 플럼, 스크라이브, 척와
    - 센싱(Sensing): 센서 데이터 → 네트워크로 수집 및 활용
    - 스트리밍(Streaming): 네트워크로 미디어 데이터를 실시간 수집(센서, 오디오, 비디오 등)
    - **플럼/스크라이브/척와의 활용은 점차 증가하는중**
    - 플럼(Flume): 분산형 대용량 로그 수집 기술, 이벤트, 에이전트 활용
        - 발행/구독 모델: 풀(Pull) 방식으로 부하 감소, 고성능 기능 제공
        - 고가용성(High Availiability) 제공: 클러스터 구성, 분산 처리를 통해 고가용성 서비스 제공 가능
        - 파일 기반 저장방식: 데이터를 디스크에 순차 저장
        - 주요 기능: 소스(이벤트를 전달하는 컨테이너), 채널(이벤트 전달 통로), 싱크(이벤트 저장 및 전달)
        
        **💡**<br>
        **풀 방식이란, 사용자가 자신이 원하는 정보를 요청할 때, 서버에서 정보를 전송하는 기법을 말한다**<br>
        **푸시 방식은 반대로, 사용자가 요청하지 않아도 자동으로 원하는 정보를 제공하는 기법이다**<br>
        {: .notice--primary}

        - 스크라이브(Scribe): 대용량 실시간 로그 수집 기술
            - 다수의 서버로부터 실시간 스트리밍 로그 데이터를 수집하여 분산 시스템에 저장한다
            - 데이터 수집 다양성: 클라이언트 서버 타입에 상관없이 로그 수집 가능
            - 고가용성: 단일 중앙 서버 장애 → 다중 로컬 서버에서 저장
        - 척와(Chukwa): 대규모 분산 시스템 모니터링을 위한 에이전트-컬렉터 구성의 데이터 수집 기술
            - 분산된 서버에서 에이전트 실행 → 컬렉터가 데이터 수집 → HDFS에 저장 및 실시간 분석 기능 제공
            - 특징: HDFS 연동, 실시간 분석, 청크 단위 처리
            - 구성: 에이전트(데이터 수집), 컬렉터(데이터를 주기적으로 HDFS에 저장)
            - 데이터 처리: 아카이빙(Archiving, 시간 순서로 그룹핑), 디먹스(Demux, 키-값 쌍으로 척와레코드 생성 및 저장)
            - 척와는 비정형, 반정형 데이터 수집 둘 다 사용된다고 한다

---