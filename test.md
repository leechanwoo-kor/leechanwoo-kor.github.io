# 1. 다음 글을 읽고 글이 참(True)인 경우 "T", 거짓(False)인 경우 "F"를 적으세요. 다른 문자는 인정되지 않습니다.

## 1-1 The instructor name of this class is JinYeong Bak.
- Y

## 1-2 NLP is hard because it is dense.
- F

## 1-3 Distributed prediction-based embeddings is created by training a classifier to distinguish near by and far-away words.
- T

## 1-4 One word can have several meanings/roles depending on the context.
- T

## 1-5 Human pay money to correlate words in one sentence or different regions of an image.
-

## 1-6 Byte pair encoding algorithm is orginally compression algorithm, but it can be used in NLP too to encode subwords
- T

## 1-7 The prior gives us a way of discriminating between possible alternative parameters in classifier models.
-

## 1-8 To measure the performance of classifiers, we use accuracy, precision, re-call, and F score.
- T

## 1-9 The task of named entitiy recognition is finding entities in text only.
- 

## 1-10 To train the machine translation models, we need parallel corpora which are the same meaning but different language sentence pairs.
- T

# 2. 다음 질문을 읽고 답을 한 두 문장 내외로 작성해주세요

## 2-1. NLP가 어려운 이유 중 하나 얘기해주세요

## 2-2. 단어 임베딩 만드는 시점에서 BERT는 Word2Vec과 어떤 차이가 있나요?
- Word2Vec과 같은 모델에 비해 문맥을 고려한 임베딩이 된다.

## 2-3. 기계번역에서 왜 encode(부호화), decode(해독화)라는 표현을 쓰는가요?

## 2-4. Reading Comprehension 문제는 인공지능에 의해 완전 해결되었나요? 이유는 무엇인가요?

## 2-5. Dirichlet-Multinomial Mixture Model과 Latent Dirichlet Allocation의 차이는 무엇인가요?

## 2-6. BERT를 학습 시키는 방법은 무엇인가요?


# 3. 이번 문제는 ROUGE 코딩 숙제와 관련 있습니다. 문제를 읽고 알맞은 답을 적어주세요.

## 3-1. ROUGE를 완성하기 위해 3-1 박스에 들어갈 코드를 작성해주세요.

## 3-2. 위 코드에는 버그가 있습니다. 버그에 대해 설명해주세요.

## 3-3. 버그를 고쳐봅시다. 해당하는 코드의 라인 숫자와 함께 어떻게 코드를 바꿔야하는지 설명해주세요.


# 4. NLP 알고리즘/모델을 사용하여 여러분이 풀고자하는 문제가 무엇인지 설명해주세요. 이 문제를 풀기 위해 NLP 중 어떤 모델을 쓰면 좋을지, 그 모델을 학습시키기 위해 혹은 문제를 풀기 위해 어떤 데이터가 존재하는지 혹은 필요한지, 어떤 실험을 할 것인지, 마지막으로 예상되는 결과는 무엇인지를 설명해주세요.
















